[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory distance sampling training materials",
    "section": "",
    "text": "This is the landing page for the distance sampling training materials website.\n\nDescribe philosophy of lecture/exercise pairing.\ndifferentiate the two exercise pathways\ndiscuss role of video introduction/summary\nif there are self-assessed tutorials, describe them\ngiscus if it exists https://giscus.app/\n\n\n\n\nOther sundries\n\nmention distancesampling.org\nmention “other workshops” if they exist"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#and-everything-else",
    "href": "index.html#and-everything-else",
    "title": "async2024-2",
    "section": "And everything else",
    "text": "And everything else"
  },
  {
    "objectID": "02-detfns/detfnlanding.html",
    "href": "02-detfns/detfnlanding.html",
    "title": "Fitting detection functions to line transect data",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described in (Anderson & Pospahala, 1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit."
  },
  {
    "objectID": "02-detfns/detfnlanding.html#estimating-density-of-duck-nests",
    "href": "02-detfns/detfnlanding.html#estimating-density-of-duck-nests",
    "title": "Fitting detection functions to line transect data",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described in (Anderson & Pospahala, 1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit."
  },
  {
    "objectID": "02-detfns/truncation-decisions.html",
    "href": "02-detfns/truncation-decisions.html",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "This question arose in our discussion of truncation. The truncation decision is one of the first decisions an analyst will make when performing a distance sampling analysis. Is the truncation decision a decision that has profound consequences and therefore deserve lots of thought (answer: probably not). Let’s see\nI have written a function that simply repeatedly calls ds() to fit a detection function to a data set. Within the function is a for() loop containing the call to ds(). After each call to ds() the results of the fitted model object are stored within a data frame for subsequent plotting. The function retains the point estimate of density and the confidence interval bounds of the estimate.\n\n\nIs presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#the-function",
    "href": "02-detfns/truncation-decisions.html#the-function",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "Is presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#duck-nest-result",
    "href": "02-detfns/truncation-decisions.html#duck-nest-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Duck nest result",
    "text": "Duck nest result\nThe figure below shows there is very little change in point estimates of duck nest density until ~20% (1/5th) of the data have been truncated. The magnitude of the point estimate changes are minute in comparison to the width of the confidence intervals.\n\ntrange &lt;- 1:26\nplot(trange-1, duck.trunc$est, type=\"p\", \n     ylim=range(c(duck.trunc[,2], duck.trunc[,3])), pch=20,\n     main=\"Duck nest data\\ntruncation experiment\", ylab=\"Nest density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, duck.trunc[trange,2],\n         trange[trange]-1, duck.trunc[trange,3])\n\n\n\n\nEffects of truncation upon density estimates for duck nest data."
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#simulated-line-transect-result",
    "href": "02-detfns/truncation-decisions.html#simulated-line-transect-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Simulated line transect result",
    "text": "Simulated line transect result\nThe value of writing functions is they can be re-used if written with sufficient generality. I re-use the truncation function upon a different data set; the simulated data from Practical 3 where the true density is known to be 79.8 animals per hectare (depicted with horizontal dashed line).\n\ndata(\"LTExercise\")\nsim.trunc &lt;- trunc.experiment(mydata=LTExercise, trunc.range=0:25, cu=cu, type=\"line\")\n\n\nplot(trange-1, sim.trunc$est, type=\"p\", \n     ylim=range(c(sim.trunc[,2], sim.trunc[,3])), pch=20,\n     main=\"Simulated data\\ntruncation experiment\", ylab=\"Density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, sim.trunc[trange,2],\n         trange[trange]-1, sim.trunc[trange,3])\nabline(h=79.8, lwd=2, lty=3)\n\n\n\n\nEffects of truncation upon density estimates for simulated line transect data."
  },
  {
    "objectID": "02-detfns/Pr2-instructions.html",
    "href": "02-detfns/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "In this exercise, we use R (R Core Team, 2019) and the Distance package (Miller et al., 2019) to fit different detection function models to the duck nest data (introduced in Exercise 1) and estimate duck nest density and abundance.\n\nObjectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it from CRAN:\n\ninstall.packages(Distance)\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn &lt;- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "02-detfns/Prac2_solution.html",
    "href": "02-detfns/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks &lt;- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn &lt;- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel       : Half-normal key function \nAIC         :  928.1338 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "02-detfns/detfnlanding.html#lecture-heading",
    "href": "02-detfns/detfnlanding.html#lecture-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Lecture heading",
    "text": "Lecture heading\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#exercise-heading",
    "href": "02-detfns/detfnlanding.html#exercise-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Exercise heading",
    "text": "Exercise heading\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#supplement-heading",
    "href": "02-detfns/detfnlanding.html#supplement-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Supplement heading",
    "text": "Supplement heading\n\nEffect of truncation upon density estimates\n\nDemonstration of changing truncation distance"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#lecture-materials",
    "href": "02-detfns/detfnlanding.html#lecture-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#exercise-materials",
    "href": "02-detfns/detfnlanding.html#exercise-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#supplemental-materials",
    "href": "02-detfns/detfnlanding.html#supplemental-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nEffect of truncation upon density estimates\n\nDemonstration of changing truncation distance"
  },
  {
    "objectID": "03-criticism/criticismlanding.html",
    "href": "03-criticism/criticismlanding.html",
    "title": "Model criticism",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the R code inside the Quarto (.qmd) instructions.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird."
  },
  {
    "objectID": "03-criticism/criticismlanding.html#absolute-and-relative-fit-of-detection-function-models",
    "href": "03-criticism/criticismlanding.html#absolute-and-relative-fit-of-detection-function-models",
    "title": "Model criticism",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the R code inside the Quarto (.qmd) instructions.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird."
  },
  {
    "objectID": "03-criticism/criticismlanding.html#lecture-materials",
    "href": "03-criticism/criticismlanding.html#lecture-materials",
    "title": "Model criticism",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "03-criticism/criticismlanding.html#exercise-materials",
    "href": "03-criticism/criticismlanding.html#exercise-materials",
    "title": "Model criticism",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "03-criticism/criticismlanding.html#supplemental-materials",
    "href": "03-criticism/criticismlanding.html#supplemental-materials",
    "title": "Model criticism",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nA detailed look at model selection\n\nThe two stages of model selection"
  },
  {
    "objectID": "01-intro/introlanding.html#estimating-widehatp_a-with-a-pencil",
    "href": "01-intro/introlanding.html#estimating-widehatp_a-with-a-pencil",
    "title": "Fundamentals of distance sampling",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region."
  },
  {
    "objectID": "01-intro/introlanding.html#lecture-materials",
    "href": "01-intro/introlanding.html#lecture-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "01-intro/introlanding.html#exercise-materials",
    "href": "01-intro/introlanding.html#exercise-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Exercise materials",
    "text": "Exercise materials\n\nExercise\nThis exercise does not involve use of the computer. Instead, you will be graphing the distribution of perpendicular detection distances. You will fit (using nothing but your eye), a detection function curve to the histogram of detection distances.\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "01-intro/introlanding.html#supplemental-materials",
    "href": "01-intro/introlanding.html#supplemental-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNothing to add to first module\n\nNo supplement"
  },
  {
    "objectID": "01-intro/introlanding.html",
    "href": "01-intro/introlanding.html",
    "title": "Fundamentals of distance sampling",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region."
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html",
    "href": "03-criticism/R-prac/modelsel-demo.html",
    "title": "Demonstration two stages of model selection",
    "section": "",
    "text": "Demonstration\n\n\n\nEastern tropical Pacific dolphin data\nAfter my improvised description of selection of adjustment terms, I thought I should provide a more thorough description via an example. The purpose of the demonstration is to fit models with adjustments to a data set and expose, in detail, all models fitted during the process.\nFor this demonstration, I require a data set with an interesting shape to the histogram. I will not describe the data set, other than to note it contains roughly 1000 detections. We will see this data set again next week. More complete details of the data set, as well as a detailed analysis are in Marques & Buckland (2003)."
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#half-normal-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#half-normal-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Half-normal cosine",
    "text": "Half-normal cosine\n\nhncos &lt;- ds(bino, key=\"hn\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting half-normal key function\n\n\nAIC= 2816.871\n\n\nFitting half-normal key function with cosine(2) adjustments\n\n\nAIC= 2806.01\n\n\nFitting half-normal key function with cosine(2,3) adjustments\n\n\nAIC= 2807.589\n\n\n\nHalf-normal key function with cosine(2) adjustments selected.\n\n\nThree models with the half-normal key are fitted, with the preferred model being the second fitted, namely the model with a single adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hn', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   D --&gt; ID2\n   ID0(hn0&lt;br&gt;AIC=2817)\n   ID1(hn1&lt;br&gt;AIC=2806)\n   ID2(hn2&lt;br&gt;AIC=2808)\n   FIN(hn1)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN"
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#uniform-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#uniform-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Uniform cosine",
    "text": "Uniform cosine\n\nunicos &lt;- ds(bino, key=\"unif\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting uniform key function\n\n\nAIC= 2971.022\n\n\nFitting uniform key function with cosine(1) adjustments\n\n\nAIC= 2811.177\n\n\nFitting uniform key function with cosine(1,2) adjustments\n\n\nAIC= 2808.378\n\n\nFitting uniform key function with cosine(1,2,3) adjustments\n\n\nAIC= 2806.685\n\n\nFitting uniform key function with cosine(1,2,3,4) adjustments\n\n\nAIC= 2808.105\n\n\n\nUniform key function with cosine(1,2,3) adjustments selected.\n\n\nThe same pattern as with the half-normal key, with a small exception. Four models with the uniform key are fitted, with the preferred model being the third fitted, namely the model with a three adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='unif', adj='cos')\n   D --&gt; ID1\n   D --&gt; ID2\n   D --&gt; ID3\n   D --&gt; ID4\n   ID1(unif1&lt;br&gt;AIC=2811)\n   ID2(unif2&lt;br&gt;AIC=2808)\n   ID3(unif3&lt;br&gt;AIC=2807)\n   ID4(unif4&lt;br&gt;AIC=2808)\n   FIN(unif3)\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN\n   ID3 --&gt; FIN\n   ID4 --&gt; FIN"
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#hazard-rate-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#hazard-rate-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Hazard rate cosine",
    "text": "Hazard rate cosine\n\nhrcos &lt;- ds(bino, key=\"hr\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting hazard-rate key function\n\n\nAIC= 2805.467\n\n\nFitting hazard-rate key function with cosine(2) adjustments\n\n\nAIC= 2807.47\n\n\n\nHazard-rate key function selected.\n\n\nTwo models are fitted with the hazard rate key function. The addition of a single adjustment term does not improve the AIC score, so there is no point in fitting a more complex model with additional adjustment terms.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hr', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   ID0(hr0&lt;br&gt;AIC=2805)\n   ID1(hr1&lt;br&gt;AIC=2807)\n   FIN(hr0)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n\n\n\n\n\nThe contestants that emerge from the first round of model competition are:\n\nhalf-normal with 1 adjustment term\nuniform with 3 adjustment terms\nhazard rate with no adjustment terms"
  },
  {
    "objectID": "05-points/pointslanding.html",
    "href": "05-points/pointslanding.html",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "05-points/pointslanding.html#analysis-of-point-transect-data",
    "href": "05-points/pointslanding.html#analysis-of-point-transect-data",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "05-points/pointslanding.html#lecture-materials",
    "href": "05-points/pointslanding.html#lecture-materials",
    "title": "Analysis of point transect data",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "05-points/pointslanding.html#exercise-materials",
    "href": "05-points/pointslanding.html#exercise-materials",
    "title": "Analysis of point transect data",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "05-points/pointslanding.html#supplemental-materials",
    "href": "05-points/pointslanding.html#supplemental-materials",
    "title": "Analysis of point transect data",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html",
    "href": "05-points/R-prac/Prac5_solution.html",
    "title": "Point transect sampling solution",
    "section": "",
    "text": "Solution\n\n\n\nPoint transect analysis exercise"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#truncation-of-20m",
    "href": "05-points/R-prac/Prac5_solution.html#truncation-of-20m",
    "title": "Point transect sampling solution",
    "section": "Truncation of 20m",
    "text": "Truncation of 20m\n\nPTExercise.hn.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hn\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.hr.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hr\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.uf.cos.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"unif\", \n                        adjustment=\"cos\", truncation=20,convert_units=conversion.factor)\n\n\n\n\nResults from simulated point transect data.\n\n\n\n\n\n\n\n\n\n\n\n\nDetectionFunction\nAdjustments\nTruncation\nAIC\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\nNone\n34.2\n919.140\n79.630\n0.126\n62.121\n102.074\n\n\nHalf-normal\nNone\n20.0\n764.305\n70.826\n0.157\n51.978\n96.507\n\n\nHazard rate\nNone\n20.0\n767.211\n62.364\n0.187\n43.207\n90.015\n\n\nUniform\nCosine\n20.0\n765.503\n75.044\n0.144\n56.515\n99.648"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "href": "05-points/R-prac/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "title": "Point transect sampling solution",
    "section": "Plots of probability density functions to inspect fit",
    "text": "Plots of probability density functions to inspect fit\n\nplot(PTExercise.hn, main=\"Half normal, no truncation\", pdf=TRUE)\nplot(PTExercise.hn.t20m, main=\"Half normal, truncation 20m\", pdf=TRUE)\nplot(PTExercise.hr.t20m, main=\"Hazard rate, truncation 20m\", pdf=TRUE)\nplot(PTExercise.uf.cos.t20m, main=\"Uniform with cosine, truncation 20m\", pdf=TRUE)\n\n\n\n\n\n\nHalf normal without truncation\n\n\n\n\n\n\n\nHalf normal 20m truncation\n\n\n\n\n\n\n\n\n\nHazard rate 20m truncation\n\n\n\n\n\n\n\nUniform cosine 20m truncation\n\n\n\n\n\n\nWe see a fair degree of variability between analyses - reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates."
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "href": "05-points/R-prac/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "title": "Point transect sampling solution",
    "section": "Probability density functions for Buckland’s winter wren point transects",
    "text": "Probability density functions for Buckland’s winter wren point transects\n\nplot(wren5min.uf.cos.t110, main=\"5 minute count\")\nplot(wrensnap.hr.cos.t110, main=\"Snapshot moment\")\n\n\n\n\n\n\nDetection function 5 minute count\n\n\n\n\n\n\n\nDetection function snapshot\n\n\n\n\n\n\nAs the detection distance histograms indicate, winter wren showed evidence of observer avoidance, more than other species in the Montrave study. We show the detection function graph rather than the PDF to emphasise the evasive movement aspect of the data. If you conduct the goodness of fit test, using gof_ds(), you will find that the models still suitably fit the data."
  },
  {
    "objectID": "04-precision/precisionlanding.html",
    "href": "04-precision/precisionlanding.html",
    "title": "Precision of density estimates",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set."
  },
  {
    "objectID": "04-precision/precisionlanding.html#precision-of-density-estimates",
    "href": "04-precision/precisionlanding.html#precision-of-density-estimates",
    "title": "Precision of density estimates",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set."
  },
  {
    "objectID": "04-precision/precisionlanding.html#lecture-materials",
    "href": "04-precision/precisionlanding.html#lecture-materials",
    "title": "Precision of density estimates",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "04-precision/precisionlanding.html#exercise-materials",
    "href": "04-precision/precisionlanding.html#exercise-materials",
    "title": "Precision of density estimates",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "04-precision/precisionlanding.html#supplemental-materials",
    "href": "04-precision/precisionlanding.html#supplemental-materials",
    "title": "Precision of density estimates",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nCould use left-truncation\n\nThere is no supplemental material"
  },
  {
    "objectID": "03-criticism/DistWin-prac/temp.html",
    "href": "03-criticism/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "02-detfns/DistWin-prac/temp.html",
    "href": "02-detfns/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "04-precision/DistWin-prac/temp.html",
    "href": "04-precision/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "05-points/DistWin-prac/temp.html",
    "href": "05-points/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html",
    "href": "05-points/R-prac/Pr5-instructions.html",
    "title": "Point transect sampling",
    "section": "",
    "text": "The purpose of this exercise is to analyse point transect survey data: it can sometimes be more difficult than line transect data. In the first problem, the data are simulated and so the true density is known. In the second problem, two different data collection methods were used to survey song birds."
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html#probability-density-function",
    "href": "05-points/R-prac/Pr5-instructions.html#probability-density-function",
    "title": "Point transect sampling",
    "section": "Probability density function",
    "text": "Probability density function\nTo plot the more informative probability density function (pdf), an additional argument is required in the plot() function:\n\nplot(ptdat.hn, pdf=TRUE)"
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "href": "05-points/R-prac/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "title": "Point transect sampling",
    "section": "Analyse both conventional and snapshot data sets",
    "text": "Analyse both conventional and snapshot data sets\nWhat to do:\n\nSelect a simple model for exploratory data analysis. Experiment with different truncation distances, \\(w\\), and select a suitable value for each method. Are there any potential problems with any of the data sets?\nTry other models and model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density and corresponding confidence interval for each method."
  },
  {
    "objectID": "02-detfns/R-prac/Pr2-instructions.html",
    "href": "02-detfns/R-prac/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "In this exercise, we use R (R Core Team, 2019) and the Distance package (Miller et al., 2019) to fit different detection function models to the duck nest data (introduced in Exercise 1) and estimate duck nest density and abundance.\n\nObjectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it from CRAN:\n\ninstall.packages(Distance)\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn &lt;- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "02-detfns/R-prac/Prac2_solution.html",
    "href": "02-detfns/R-prac/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks &lt;- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn &lt;- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel       : Half-normal key function \nAIC         :  928.1338 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html",
    "href": "03-criticism/R-prac/Pr3-instructions.html",
    "title": "Assessing line transect detection functions",
    "section": "",
    "text": "In Exercise 2, we fitted different detection functions and compared them in terms of goodness of fit and AIC. Here, we continue to fit and assess different models and look at additional arguments in the ds package."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#accessing-the-data",
    "href": "03-criticism/R-prac/Pr3-instructions.html#accessing-the-data",
    "title": "Assessing line transect detection functions",
    "section": "Accessing the data",
    "text": "Accessing the data\nLoad the Distance package to access both the data and analytical functions.\n\n# Load library (if not already loaded)\nlibrary(Distance)\n# Access data\ndata(\"LTExercise\")\n# Check that it has been imported correctly\nhead(LTExercise, n=3)\n\n\nData format: no detections on a transect\nBefore fitting models, it is worth investigating the data a bit further: let’s start by summarising the perpendicular distances:\n\n# Summary of perpendicular distances\nsummary(LTExercise$distance)\n\nThe summary indicates that the minimum distance is min(hndat$distance) and the maximum is max(hndat$distance) metres and there is one missing value (indicated by the NA). If we print a few rows of the data, we can see that this missing value occurred on transect ‘Line 11’.\n\n# Print out a few lines of data\nLTExercise[100:102, ]\n\nThe NA indicates that there were no detections on this transect, but the transect information needs to be included otherwise the number of transects and the total line length will be incorrect."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#truncation",
    "href": "03-criticism/R-prac/Pr3-instructions.html#truncation",
    "title": "Assessing line transect detection functions",
    "section": "Truncation",
    "text": "Truncation\nLet’s start by fitting a basic model, i.e. no adjustment terms (by default a half normal model is fitted).\nFor this project, perpendicular distances are in metres and the transect lines are recorded in kilometres.\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n# Fit half normal, no adjustments\nlt.hn &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL,\n            convert_units=conversion.factor)\n\nLooking at a summary of the model object, how many objects are there in total? What is the maximum observed perpendicular distance?\n\n# Print a summary of the fitted detection function\nsummary(lt.hn)\n\nPlot the detection function and specify many histogram bins:\n\nplot(lt.hn, nc=30)\n\nThe histogram indicates that there is a large gap in detections therefore, to avoid a long right hand tail in the detection function, truncation is necessary. There are several ways to truncate: excluding distances beyond some specified distance or excluding a specified percentage of the largest distances. Note that here we only consider excluding large perpendicular distances, which is frequently referred to as right truncation.\n\nTruncation at a fixed distance\nThe following command truncates the perpendicular distances at 20 metres i.e. objects detected beyond 20m are excluded.\n\n# Truncate at 20metres\nlt.hn.t20m &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=20,\n                 convert_units=conversion.factor)\n\nGenerate a summary and plot of the detection function to see what effect this truncation has had on the number of objects.\n\n\nTruncating a percentage of distances\nAn alternative way to truncate distances is to specify a percentage of detected objects that should be excluded. In the command below, 10% of the largest distances are excluded.\n\n# Truncate largest 10% of distances\nlt.hn.t10per &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=\"10%\",\n                   convert_units=conversion.factor)\n\nGenerate a summary and plot to see what effect truncation has had.\n\nsummary(lt.hn.t10per)\nplot(lt.hn.t10per)"
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#exploring-different-models",
    "href": "03-criticism/R-prac/Pr3-instructions.html#exploring-different-models",
    "title": "Assessing line transect detection functions",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don’t spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‘wrong’ model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\n# Half normal detection, cosine adjustments, no truncation\nlt.hn.cos &lt;- ds(data=LTExercise, key=\"hn\", adjustment=\"cos\",\n                convert_units=conversion.factor)\n\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\)."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "href": "03-criticism/R-prac/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, …, 80 are specified.\n\ndata(capercaillie)\n# Specify cutpoint for bins\nbins &lt;- seq(from=0, to=80, by=10)\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"hectare\")\n# Fit model with binned distances\ncaper.bin &lt;- ds(data=capercaillie, key=\"hn\", cutpoints=bins, \n                convert_units=conversion.factor)\nplot(caper.bin)"
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html",
    "href": "03-criticism/R-prac/Prac3_solution.html",
    "title": "Assessing line transect detection functions solution",
    "section": "",
    "text": "Solution\n\n\n\nAssessing line transect detection functions"
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "href": "03-criticism/R-prac/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "title": "Assessing line transect detection functions solution",
    "section": "Fitting multiple models to exact distance data",
    "text": "Fitting multiple models to exact distance data\n\n# Half normal model \ncaper.hn.cos &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Hazard rate model  \ncaper.hr.cos &lt;- ds(data=capercaillie, key=\"hr\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Uniform model  \ncaper.uf.cos &lt;- ds(data=capercaillie, key=\"unif\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n\nThe detection functions and QQ plots are shown below:\n\nplot(caper.hn.cos, main=\"Half normal\")\nx &lt;- gof_ds(caper.hn.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.hr.cos, main=\"Hazard rate\")\nx &lt;- gof_ds(caper.hr.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.uf.cos, main=\"Uniform\")\nx &lt;- gof_ds(caper.uf.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nQQ plot half normal\n\n\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\n\nQQ plot hazard rate\n\n\n\n\n\n\n\n\n\nUniform with adjustment\n\n\n\n\n\n\n\nQQ plot uniform adj\n\n\n\n\n\n\nSummarise the goodness of fit statistics (in a pretty format). This table indicates that the hazard rate detection function had the lowest AIC but the difference in AIC between all three models was small.\n\nknitr::kable(summarize_ds_models(caper.hn.cos, caper.hr.cos, caper.uf.cos, output=\"plain\"),\n               caption=\"Summary of results of Capercaillie analysis.\", digits = 3)\n\n\nSummary of results of Capercaillie analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\n2\ncaper.hr.cos\nHazard-rate\n~1\n0.663\n0.703\n0.052\n0.000\n\n\n1\ncaper.hn.cos\nHalf-normal\n~1\n0.332\n0.613\n0.053\n0.031\n\n\n3\ncaper.uf.cos\nUniform with cosine adjustment terms of order 1,2\nNA\n0.613\n0.682\n0.098\n0.280\n\n\n\n\n\nThe results for the three different models are shown below: density is in birds per ha.\n\n\n\nCapercaillie point estimates of density and associated measures of precision.\n\n\nDetectionFunction\nAIC\nPa\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\n957.905\n0.613\n0.048\n0.148\n0.027\n0.083\n\n\nHazard rate\n957.874\n0.703\n0.042\n0.148\n0.020\n0.084\n\n\nUniform\n958.154\n0.682\n0.043\n0.191\n0.026\n0.069\n\n\n\n\n\nThese capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results."
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "href": "03-criticism/R-prac/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions solution",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nTo deal with rounding in the distance data, the exact distances can be converted into binned distances. The cutpoints need to be chosen with care so that the distance bins are sufficiently wide enough to ensure that the ‘correct’ perpendicular distance is in the band containing the rounded recorded value. The bin widths do not have to be equal, as shown in example here: the cutpoints are 0, 7.5, 17.5, 27.5, …, 67.5, 80.0 m. Note, that any distances beyond the largest bin will be excluded.\n\n# Specify (uneven) cutpoint for bins\nbins &lt;- c(0, seq(from=7.5, to=67.5, by=10), 80)\nprint(bins)\n\n[1]  0.0  7.5 17.5 27.5 37.5 47.5 57.5 67.5 80.0\n\ncaper.hn.bin &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\", cutpoints=bins,\n                   convert_units=conversion.factor)\nplot(caper.hn.bin, main=\"Capercaillie, binned distances\")\n\n\n\n# See a portion of the results\nknitr::kable(caper.hn.bin$dht$individuals$summary, row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\nmean.size\nse.mean\n\n\n\n\nMonaughty Forest\n1472\n3840\n240\n112\n1\n0.4666667\n0\n0\n1\n0\n\n\n\n\nknitr::kable(caper.hn.bin$dht$individuals$D[1:6], row.names = FALSE, digits=3)\n\n\n\n\nLabel\nEstimate\nse\ncv\nlcl\nucl\n\n\n\n\nTotal\n0.045\n0.007\n0.152\n0.026\n0.079\n\n\n\n\n\nNote that the binning of the data results in virtually identical estimates of density (0.045 birds per ha) and essentially no change in the precision of the density estimate compared with the estimates with analysis of exact distance data."
  },
  {
    "objectID": "04-precision/R-prac/Pr4-instructions.html",
    "href": "04-precision/R-prac/Pr4-instructions.html",
    "title": "Variance estimation for systematic survey designs",
    "section": "",
    "text": "In the lecture describing measures of precision, we explained that systematic survey designs usually have the best variance properties, but obtaining good estimates of the variance is a difficult problem for statisticians. In this exercise, we give an example of a situation where the systematic design gives a density estimate with much better precision than a random design. This means that the usual variance estimators used in the ds function, which are based on random transect placement, are far too high. The true variance is low, but the estimated variance is high.\nWe will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance. In Section 6, we examine another case to see that the unstratified variance estimates provided by ds are usually fine for a systematic design: things only go wrong when there are strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g. the highest densities always occur on the shortest lines, or vice versa).\nWe begin with a population and survey shown below. The data used for this exercise were simulated on a computer: they are not real data. Note the characteristics for the data in Figure 1: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design has covered a fairly large proportion of the survey area (the covered region is shaded). These are danger signals that the usual ds variance estimators might not work well and a post-stratification scheme should be considered.\n\n\n\n\n\nAn example of survey data where there is a strong trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\n\nObjectives\nThe aims of this exercise are to illustrate:\n\nDefault variance estimation,\nVariance estimation with bootstrapping,\nPost-stratification to improve variance estimation,\nWhen post-stratification is not needed (optional).\n\n\n\nGetting started\nDon’t forget to load the Distance package for your session.\n\nlibrary(Distance)\n\n\n\nBasic (default) variance estimation\nIn the code below, the necessary data file is imported and a simple model is fitted and a summary produced. Make a note of the CV of the density estimate - this is obtained using the default (analytical) estimator in the ds function and is based on the assumption that the lines were placed at random. This CV can then be compared with the CV estimates obtained from alternative methods.\n\n# Import data\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\n# Fit a simple model\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=NULL,\n                 convert_units=conversion.factor)\n# Summary\nprint(sysvar2.hn$dht$individuals$D)\nprint(sysvar2.hn$dht$individuals$N)\n\nThe true density and abundance are known (because the data were simulated): the true abundance in the survey region was \\(N=1000\\) and \\(D=2000 \\textrm{ animals per km}^2\\) (i.e. 1000 animals in an area of size \\(A=0.5 \\textrm{km}^2\\)). How do the point estimates compare with truth? What do you think about the precision of the estimates?\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\n# Bootstrap estimate of uncertainty\n# Run the bootstrap (this can take a while if nboot is large!) \nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize,\n                    convert_units=conversion.factor, nboot=199)\n\nThe arguments for this command are:\n\nmodel - fitted detection function model object\nflatfile - data frame of the survey data\nsummary_fun - function used to obtain the summary statistics from each bootstrap\nconvert_units - conversion units for abundance estimation\nnboot - number of bootstrap samples to generate. Note, it can take a long time to produce a large number of bootstraps and so perhaps try a small number at first.\n\n\n# See the results\nsummary(est.boot)\n\nThe summary includes:\n\nEstimate - the median value of the bootstrap estimates\nse is the standard deviation of the bootstrap estimates\nlcl and ucl are the limits for a 95% confidence interval.\ncv is the coefficient of variation (\\(CV = SE/Estimate\\))\n\nAre the bootstrapped confidence intervals for abundance and density similar to the analytical confidence intervals produced previously?\nRecall that we have a particular situation in which we have systematically placed transects which are unequal in length. Furthermore, there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study region with the highest density. In the next section, we examine a process by which we can use post-stratification to produce a better estimate of the variance in estimated abundance.\n\n\nPost-stratification to improve variance estimation\nThe estimation of encounter rate variance in the previous section used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic and, in some circumstances, taking this in account can substantially reduce the encounter rate variance. The data we are working with is an example of this, where there are very high densities on the very shortest lines. In samples of lines, collected using a completely random design, the sample, by chance, might not contain any very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance. Although there is no way of getting a variance estimate that is exactly unbiased for a systematic sample because it is effectively a sample of size 1- only the first line position was randomly chosen and the rest followed on deterministically from there. We can greatly improve on the random-based estimate by using a post-stratification scheme.\nThe post-stratification scheme works by grouping together pairs of adjacent lines from the systematic sample and each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample. This encounter rate estimator is called ‘O2’ (Fewster et al., 2009) and is implemented in the dht2 function.\n\n# Post-stratification - stratified variance estimation by grouping adjacent transects\n\n# Ensure that Sample.Labels are numeric, this is required for O2 ordering\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\n\n# Use the Fewster et al 2009, \"O2\" estimator \nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, strat_formula=~1, \n               convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nNote that this estimator assumes that the numbering of the transects (in this example Sample.Label takes values 1 to 20) has some geographical meaning (i.e. transect 1 is next to 2 and 2 is next to 3 etc.). If this is not the case, then the user can manually define some sensible grouping of transects and create a column called grouping in the data object.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThe simulated population shown in Figure 2 does not exhibit strong trends across the survey region, otherwise, the strip dimensions and systematic design are the same as for the previous example. These data are stored in the data set Systematic_variance_1.\n\n\n\n\n\nAn example of survey data that does not exhibit a trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\nIn the code below, these data are imported into R and a simple detection function model is fitted. The default estimate of variance is then compared to that obtained using the ‘O2’ estimator (Fewster et al., 2009).\n\n# When post-stratification is not needed\n# Import the data\ndata(\"Systematic_variance_1\")\n# Ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\n# First fit a simple model\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\n# Obtain default estimates for comparison\nprint(sysvar1.hn$dht$individuals$D)\nprint(sysvar1.hn$dht$individuals$N)\n# Now use Fewster et al 2009, \"O2\" estimator \nest1.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1, \n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est1.O2, report=\"both\")\n\nDid you see a difference in the CV and 95% confidence interval between the two estimators?\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "04-precision/R-prac/Prac4_solution.html",
    "href": "04-precision/R-prac/Prac4_solution.html",
    "title": "Variance estimation for systematic survey designs solution",
    "section": "",
    "text": "Solution\n\n\n\nVariance estimation for systematic designs\n\n\n\nBasic (default) variance estimation\nRecall the data for this example, in which we have a strong gradient in animal density across our study region and at the same time we have a difference in the lengths of the transects, such that short transects are in regions of high animal density and long transects are in regions of low animal density.\n\nlibrary(Distance)\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=\"cos\",\n                 convert_units=conversion.factor)\nprint(sysvar2.hn$dht$individuals$D)\n\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 2044.592 566.3958 0.2770214 1161.012 3600.614 20.74468\n\nprint(sysvar2.hn$dht$individuals$N)\n\n  Label Estimate       se        cv     lcl      ucl       df\n1 Total 1022.296 283.1979 0.2770214 580.506 1800.307 20.74468\n\n\nThe point estimates are good (\\(\\hat D = 2,044\\) animals per unit area and \\(\\hat N=1,022\\) - note the size of the area) but the precision obtained with the default estimator is poor: estimated abundance ranges from about 580 to 1,800 - a three-fold difference over which we are uncertain. Given that our survey covered 40% of the triangular region and had a good sample size (254 animals on 20 transects), this would be a disappointing result in practice.\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize, \n                    convert_units=conversion.factor, nboot=100)\n\n\nsummary(est.boot)\n\nBootstrap results\n\nBoostraps          : 100 \nSuccesses          : 100 \nFailures           : 0 \n\n     median   mean     se    lcl    ucl   cv\nNhat 985.58 981.06 254.52 550.31 1508.1 0.26\n\n\nThe bootstrap results are very similar to the analytical results, as we would expect, because again this process assumed the transects were placed at random.\n\n\nPost-stratification to improve variance estimation\n\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, \n               strat_formula=~1, convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k     ER se.ER cv.ER\n  Total  0.5      0.1922   9.61 254 20 26.431 1.459 0.055\n\nDensity estimates:\n .Label Estimate      se   cv      LCI      UCI     df\n  Total 2044.592 162.914 0.08 1744.988 2395.636 75.871\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     52.03 47.97\n\n\nThe precision of the estimated abundance has greatly improved in the post-stratified analysis (Fewster et al., 2009).\nIt must be remembered that we have not made any change to our data by the post-stratification; we are using getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be as extreme as they are in this example and post-stratification will not make a great difference. Such an situation is illustrated in the next problem.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThese data did not exhibit strong trends across the survey region and, hence, there are no great differences between the CVs and 95% confidence intervals using the two methods.\n\n# Access the data\ndata(\"Systematic_variance_1\")\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\nprint(sysvar1.hn$dht$individuals$D)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 1954.016 160.5554 0.08216688 1657.276 2303.888 50.59541\n\nprint(sysvar1.hn$dht$individuals$N)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 977.0078 80.27768 0.08216688 828.6378 1151.944 50.59541\n\nest2.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1,\n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est2.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total  0.5      0.2058  10.29 252 20 24.49 1.594 0.065\n\nDensity estimates:\n .Label Estimate      se    cv      LCI      UCI     df\n  Total 1954.015 162.491 0.083 1653.804 2308.723 49.172\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     38.76 61.24\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "01-intro/Pr1-instructions.html",
    "href": "01-intro/Pr1-instructions.html",
    "title": "Line transect detection function fitting",
    "section": "",
    "text": "In this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA (Anderson & Pospahala, 1970). Twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nTable 1. Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nObjectives\nThe aim of this exercise is to plot a histogram of the perpendicular distances to the detected duck nests and estimate (by eye) a detection function and hence estimate density of duck nests, i.e. the number of nests per square metre or per square kilometre (be careful of units).\n\n\nAnswer these questions in sequence\nThese steps will produce an estimate of duck nest density.\n\nWith the graph paper PDF found at this link, print the PDF and plot a histogram of the data in Table 1 and fit a detection function by eye.\nEstimate the areas under the rectangle and the fitted detection function curve and hence estimate the proportion of nests that are detected in the covered region, i.e. the region within 2.4m of the transect centre line.\n\n\\[ Area_{rectangle} = \\] \\[ Area_{curve} = \\]\n\\[ \\hat{P}_a = \\frac{Area_{curve}}{Area_{rectangle}} = \\]\n\nObtain an estimate of the number of nests in the covered region (Note \\(n=534\\)):\n\n\\[ \\hat{N}_a = \\frac{n}{\\hat{P}_a} = \\]\n\nEstimate density (Note \\(L = 20 \\times 128.75 = 2575\\) km):\n\n\\[\\hat{D} = \\frac{\\hat{N}_a}{a} = \\frac{\\hat{N}_a}{2wL} = \\]\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501"
  },
  {
    "objectID": "01-intro/Prac1_solution.html",
    "href": "01-intro/Prac1_solution.html",
    "title": "Line transect detection function fitting solution",
    "section": "",
    "text": "Solution\n\n\n\nEstimation of duck nest density by hand\n\n\nIn this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA: twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nTable 1. Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nHistogram of detected nests (black) overlaid with the estimated detection function (red) is shown below.\n\n\n\n\n\n\n\nTo estimate the area under the curve, I read off the heights of the mid points of my fitted curve (red) as follows: 75, 74, 72, 70, 66, 62, 58, 53. Therefore, my estimate of area under the curve is:\n\n\\[ Area_{curve} = (75+74+72+70+66+62+58+53) \\times 0.3 = 530 \\times 0.3 = 159 \\] There are lots of other ways to work out the area under a curve, e.g. counting the number of grid squares under the curve on your graph paper or using the trapezoidal rule.\n\\[Area_{rectangle} = height \\times width = 75 \\times 2.4 = 180\\]\nHence, my estimate of the proportion of nests detected in the covered region is:\n\\[\\hat P_a = \\frac{159}{180} = 0.883\\]\n\nHow many actual nests were there in the covered area? I saw 534 nests, and I estimate the proportion seen is 0.883, so my estimate of nests in the covered region is:\n\n\\[ \\hat N_a = \\frac{n}{\\hat P_a} =\\frac{534}{0.883} = 604.7 \\textrm{ nests in the covered area}\\] This estimate is for a covered area of \\(a = 2wL = 2 \\times (\\frac{2.4}{1000}) \\times 2575 = 12.36\\) km\\(^2\\).\n\nI therefore estimate nest density as:\n\n\\[\\hat D = \\frac{\\hat N_a}{2wL} = \\frac{604.7}{12.36} = 48.9 \\textrm{ nests per km}^2\\]"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "",
    "text": "Introduction to Distance Sampling with duck nest data"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#working-on-the-computer",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#working-on-the-computer",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Working on the computer",
    "text": "Working on the computer\n\nInstall the Distance software:\n\nDownload and install the Distance software from http://distancesampling.org/Distance/distance75download.html, if you have not already.\n\nAcquire the Distance project for this exercise:\n\nClick on this link to access data to be used in this exercise. You will be asked to save this file to your local computer. Remember the location where you saved the file, as you will be opening it in the next step.\n\nWe are going to work with a project called “Ducknest exercise”. It contains the same data you previously analysed by hand. Open the Distance software and select File followed by Open project. Under “Files of type” choose Zip archive files (*.zip).\n\n\n\n\n\n\n\n\nOther Distance projects\n\n\n\n\n\nThe Sample Projects folder created when you installed the Distance software, contains a number of other data sets, not related to this training workshop. You may want to look at those projects at a later date.\n\n\n\n\nDouble click on Ducknest exercise.zip. Click OK to unpack the project into the current directory and open it. Next time you open the project, you can open the file Ducknest exercise.dst directly."
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#examining-the-data",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#examining-the-data",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Examining the data",
    "text": "Examining the data\n\nClick on the Data tab of the Project Browser to show the Data Explorer. Look at the data structure, and in particular how the distance data have been entered. (You will need to click on Observation in the left hand pane of the Data Explorer to see this.)"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#studying-the-first-analysis",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#studying-the-first-analysis",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Studying the first analysis",
    "text": "Studying the first analysis\n\nClick on the Analysis tab of the Project Browser to show the Analysis Browser. You should see one analysis listed, called “Half-normal no adjustments.” Double-click on the grey status button for this analysis to open the Analysis Inputs tab for this analysis (you can do the same thing by clicking the 3rd button after “Analysis:” on the Analysis Browser menu bar, or by choosing Analyses then Analysis Details… from the menu bar at the top).\n\n\n\nA grey status icon indicates that this analysis has yet to be run. Click on the Run button in order to run the analysis. The Results tab should turn green.\nClick on the Results tab to see the results, and use the Next &gt; button to move through the pages of results, looking at each page and trying to relate the analysis given here to the one you did by hand. (Note: These are the analysis details (Inputs/Log/Results) for one analysis – you can resize this window so that you can view details from multiple analyses when you have more than one analysis to examine)."
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-solution-DistWin.html",
    "href": "02-detfns/DistWin-prac/Pr2-solution-DistWin.html",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "",
    "text": "Introduction to Distance Sampling with duck nest data\n\n\n\n\n\nDistance for Windows Solution\n\n\n\n\nWatch the solutions video, where we work through the exercise step by step.\nWhen you have the Ducknest project open in Distance and click on the Data tab, your screen should look like this:\n\n\n\nOnce you’ve run the analysis, and open the Analysis Details window for that analysis, your screen should look something like this (see below). Distance gets a density estimate of 49.697 nests km-2, similar to the value we obtained by hand of 48.9 nests km-2."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html",
    "title": "Analysis of point transect survey data",
    "section": "",
    "text": "Analysis of point transect survey data"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html",
    "title": "Variance estimation for systematic designs",
    "section": "",
    "text": "Variance estimation for systematic designs\nRecall the situation in which we have a strong gradient in animal density across our study region, and at the same time we also have a difference in the lengths of our transects; such that short transects are in areas of high animal density, and long transects are in areas of low animal density."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "title": "Variance estimation for systematic designs",
    "section": "Basic variance estimation, with bootstrapping",
    "text": "Basic variance estimation, with bootstrapping\n\nThe precision is very poor: estimated density ranges from about 1000 to 3600: a three-and-a half-fold difference over which we are uncertain. Given that our survey covered 40% of the triangle region, and had a good sample size (254 on 20 transects), this would be a very disappointing result in practice.\nBootstrap output [your results may differ slightly as these are created from a random process]:\n\nEstimate %CV \\# df 95% Confidence Interval\n--------------------------------------------------------\nHalf-normal/Cosine\nD 2129.2 27.40 999 20.74 1216.2 3727.5\n                         1164.0 3427.2\n\nHalf-normal/Cosine\nN 1064.6 27.40 999 20.74 608.00 1864.0\n                         582.00 1714.0\n\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals.\nInterval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\nThe bootstrap results are very similar to the analytic results, as we would expect. In fact, this did not used to be the case in previous versions of Distance, as the old analytic variance estimator did not perform well when there were extreme trends in both density and line length. You can access the previous default estimator under the Advanced… tab on the Variance page of the Model Definition Properties (its estimator R3), and more details are given in (Fewster et al., 2009).\nThe component percentages of variance are as follows:\n\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 4.3\nEncounter rate :       95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#post-stratification-to-improve-variance-estimation",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#post-stratification-to-improve-variance-estimation",
    "title": "Variance estimation for systematic designs",
    "section": "Post-stratification to improve variance estimation",
    "text": "Post-stratification to improve variance estimation\n\nThe precision is now greatly improved:\n\nEstimate %CV df 95% Confidence Interval\n-----------------------------------------------------\nHalf-normal/Cosine\n\nD 2044.6 8.64 31.41 1715.0 2437.5\nN 1022.0 8.64 31.41 858.00 1219.0\nand a much smaller and more reasonable (considering the sample size and survey coverage) proportion of the variation comes from estimating encounter rate:\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 44.3\nEncounter rate :        55.7\n\nThe CV is now even smaller, although it could have gone either way because this is an estimator of the same quantity as the last question–just a more precise estimator.\n\nEstimate %CV df 95% Confidence Interval\n-----------------------------------------------------\n\nHalf-normal/Cosine\n\nD 2044.6 7.97 75.87 1745.0 2395.6\nN 1022.0 7.97 75.87 872.00 1198.0\nThe encounter rate degrees of freedom are now 19 (number of lines – 1) rather than 10 (number of post-strata) for the previous question–which is why this is a more precise estimator of the variance.\nRemember we have not made any change to our data by the post-stratification; we are just getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be extreme as they are in this example, and post-stratification will not make a great difference. Such an example is shown below."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "title": "Variance estimation for systematic designs",
    "section": "Systematic designs where post-stratification is not needed",
    "text": "Systematic designs where post-stratification is not needed\n\nThe following simulated population does not exhibit strong trends across the survey region. Otherwise, the strip dimensions and systematic design are the same as for the previous example.\n\nWithout post-stratification: analytic output\nEstimate %CV df 95% Confidence Interval\n------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1954.0 8.22 50.60 1657.3 2303.9\nN 977.00 8.22 50.60 829.00 1152.0\nNote: Your bootstrap results will differ slightly, as bootstrapping is a random procedure.\nEstimate %CV \\# df 95% Confidence Interval\n--------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1947.4 10.03 999 50.60 1592.8 2380.8\n                         1565.0 2350.3\n\nHalf-normal/Cosine\n\nN 973.69 10.03 999 50.60 796.00 1190.0\n                         782.00 1175.0\n\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals.\nInterval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\n\nWith post-stratification (non-overlapping): analytic output\nEstimate %CV df 95% Confidence Interval\n------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1954.0 8.38 25.80 1645.4 2320.6\nN 977.00 8.38 25.80 823.00 1160.0"
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html",
    "href": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "",
    "text": "More detection function modelling and model criticism\nResults of estimating density from simulated data in which true density was 79.8 per km2. Findings from some candidate models:\nNot surprisingly for these data (simulated from a half normal detection function with a broad shoulder), the negative exponential model gives a higher estimate than the others, although the confidence interval still includes the true density. The other models provide very similar estimates, though precision is slightly worse for the hazard-rate model (because more parameters fitted). Agreement between the estimate and the known true density is less good if you do not truncate the data, or do not truncate them sufficiently."
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html#additional-question",
    "href": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html#additional-question",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "Additional question",
    "text": "Additional question\nThe capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results. Note the rounding in the distance data. This means that interval cutpoints for histograms and goodness-of-fit testing, and for the analysis of grouped data if this is required, should be chosen with care (i.e., distance bands ought to be sufficiently broad such that the ‘correct’ perpendicular distance is in the bands containing the rounded recorded value. e.g. 0, 7.5, 17.5, 27.5, … )\n\n\n\n\n\n\n\n\n\n\nFitted model\n\\(\\hat{D}\\)\n\\(\\hat{D}\\) LCB\n\\(\\hat{D}\\) UCB\n\\(\\hat{D}\\) CV\n\n\nHalf normal\n4.76\n4.01\n5.65\n0.09\n\n\nUniform cosine\n4.28\n3.22\n5.68\n0.14\n\n\nHazard rate\n4.20\n3.6\n4.9\n0.08\n\n\nHalf normal with grouped data\n4.52\n3.81\n5.36\n0.09\n\n\n\n\n\n\n\n\n\nNote regarding estimates in previous table\n\n\n\n\n\nI have reported the density estimates in the table above as numbers km-2, rather than numbers ha-1 to make them easier to read. There are 100 hectares in a square kilometer."
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html",
    "href": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "",
    "text": "More detection function modelling and model criticism\n1a). The line transect data below were generated from a half-normal detection function model. You may either enter the data by hand (if you want practice doing this) using instructions in the next bullet point, or skip to step 1b) to open a project containing the data to begin analysis.\nPerpendicular distances in metres generated from a half-normal line transect detection function model.\n1b) The full data set is in project Exercise3-2023.zip Download the file from this link and save the file in a location on your computer. Open Distance for Windows. Choose Open project and select zip file type to open the file and begin analysis."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html",
    "title": "Variance estimation for systematic designs",
    "section": "",
    "text": "Variance estimation for systematic designs\nIn the lecture describing measures of precision we explained systematic survey designs usually have the best variance properties, but obtaining good estimates of the variance is a difficult problem for statisticians. In this exercise we give an example of a situation where the systematic design gives a density estimate with much better precision than a random design.\nThis means that the usual variance estimators used in Distance, which are based on a random design, give variance estimates that are far too high. The true variance is low, but the estimated variance is high. We will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance.\nWe also look at another case to see that the unstratified variance estimates provided by Distance are usually fine for a systematic design: things only go wrong when there are very strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g., the highest densities always occur on the shortest lines, or vice versa).\nWe begin with the population and survey shown below. All the populations in this exercise are simulated on a computer: they are not real data. Note the characteristics: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design (search strips are shaded) covers a fairly large portion of the survey area. These are the danger signals that the usual Distance variance estimators might not work well, and a post-stratification scheme should be considered.\nThe survey region is a triangle, with dimensions 1km by 1km. The systematically placed search strips are shaded."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "title": "Variance estimation for systematic designs",
    "section": "Basic variance estimation, with bootstrapping",
    "text": "Basic variance estimation, with bootstrapping\n\nDownload and open the Distance project Systematic_variance_2.zip.\nOn the Analysis tab, click New Analysis. Rename it Without post-stratification.\nUnder Model definition, click Properties. Rename the new model: No_adjustments_plus_bootstrap.\nClick the tab for Detection function, and click Adjustment terms. Select Manual selection so that no adjustment terms are fitted. Select the Constraints button, and click No constraints. These options will reduce the work that has to be done during bootstrapping.\nClick the tab for Variance, and check the box for Bootstrap variance estimate: Select non-parametric bootstrap. The box Resample samples should be checked (this means resample transect lines). Leave the other settings at default, noting that there will be 999 bootstrap resamples conducted.\nClick OK and then run the model. You can see the progress of the bootstrap in the bar at the top. Wait a few moments until the bootstrapping is completed.\nYour analytic output should resemble this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\n%CV\ndf\nLower 95% Confidence bound\nUpper 95% Confidence bound\n\n\n\n\nD\n2044.6\n27.70\n20.74\n1161.0\n3600.6\n\n\nN\n1022.0\n27.70\n20.74\n581.0\n1800.0\n\n\n\n\nBecause we have simulated these data, we know the true values. The true number of animals in the survey region is N=1000, and the true density is D=2000 km-2 (1000 animals in an area of size A=0.5 km2). The point estimates are good, but what do you think about the precision in the output above?\nFind the bootstrapped confidence intervals for D and N, and check whether they are similar to the confidence intervals above.\nWhat percentage of the total density variance is attributed to encounter rate estimation and what percentage to the detection function estimation?\n\n\nVariance estimation for systematic designs using post-stratification\nRecall we have a particular situation in which we have systematically placed transects, unequal in length. Furthermore there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study area with the highest density. We examine a means by which we can use post-stratification to produce a better estimate of the variance in estimated density."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#post-stratification-to-improve-variance-estimation",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#post-stratification-to-improve-variance-estimation",
    "title": "Variance estimation for systematic designs",
    "section": "Post-stratification to improve variance estimation",
    "text": "Post-stratification to improve variance estimation\nThe estimation of encounter rate variance above used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic. In some circumstances, this can reduce the encounter rate variance a great deal.\nThe data we are working with is an example of this. There are very high densities on the very shortest lines. In samples of lines collected using a completely random design, the sample by chance might not contain any of these very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance.\nAlthough there is no way of getting a variance estimate that is exactly unbiased for a systematic sample1, we can greatly improve on the random-based estimate by using a post-stratification scheme. This works by grouping together pairs of adjacent lines from the systematic sample. Each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample.\nFollow the steps below.\n\nOpen the Distance project we used in the previous section (Systematic_variance_2.dst; is has the “.dst” extension because you uncompressed it minutes ago).\nClick the Analyses tab, and click the “New analysis” button to create a new analysis. Double click the grey ball and the Analysis Details Window should come up. Name the new analysis something like With post stratification.\nUnder Model Definition, click New. Change the name at the bottom of the dialogue box to Poststratified_no_adjustments_no_bootstrap. (We don’t want to conduct a bootstrap for our poststratified data, because it would involve some extra confusion and is not necessary.) In the Variance tab, click Advanced..., and select the option “Post-stratify, grouping adjacent pairs of samplers”. Un-tick the option “Select non-parametric bootstrap”.\nClick OK and then Run to run the analysis. How does the variance and confidence limits compare with those you obtained in the previous section? What are the implications? Note what percentage of the overall variance now comes from encounter rate and from estimating the detection function, and compare this with the earlier percentages.\nNow try the overlapping post-stratification option. A simulation study in (Fewster et al., 2009) concluded that its performance was very similar to, but marginally better than the regular post-stratification. When the sample size of lines is small, it gives more post-strata and so is to be preferred for that reason. Create a new analysis, called say With overlapping post stratification, and then a new Model Definition for that analysis, in which you choose the Advanced variance option “Post-stratify, with overlapping strata made up of adjacent samplers”. How does the variance compare with those you previous obtained? How do the degrees of freedom in the Estimation Summary – Encounter Rate page of output compare with that from the previous question?\n(Optional) If you wish, you can try manual post-stratification. This is good practice if you need to do post-stratification for point transect studies. In this case you will have to add a new field to the sample layer, and then set up a new model definition in which you tell Distance to use post-stratification. Here goes:\n\nClick the Data tab. Click the padlock button on the toolbar to unlock the data sheet for modification.\nOn the left-hand outline, click Line transect. The data sheet expands to 20 rows, each row corresponding to one line transect. This is the best format for the data sheet to be in when entering a new stratum number for each transect.\nClick on the cell corresponding to Line transect Label 1. Several buttons on the tool-bar should become live. Click on the button corresponding to Append field after current. (The button has an arrow pointing sideways then downwards.)\nYou are prompted for Field name: enter VarGroup to indicate that you are grouping lines together for the purpose of variance estimation. Click Field type: Integer, and click OK.\nYou can now enter the line groupings for post-stratified variance estimation. Enter label 1 for lines 1 and 2; label 2 for lines 3 and 4; label 3 for lines 5 and 6; and so on, to finish with label 10 for lines 19 and 20. You have now defined 10 strata, each containing two adjacent transect lines from the systematic sample of lines.\nAfter entering the column of VarGroup labels, click the padlock button again to lock the data sheet.\nNow we will analyse the post-stratified data. Click the Analyses tab. Create a new analysis with a suitable name - .e.g, Manual post stratification\nCreate a new Model definition, with a suitable name. In the Estimate tab, click the button for Poststratify. Select Layer type: sample, and Field name: VarGroup. This means that we want to poststratify at the sample (transect) level, using our newly defined groupings VarGroup to delimit the strata.\nFor the levels of resolution, select the following:\n\n\n\nDensity: Global and Stratum\nEncounter Rate: Stratum only\nDetection function: Global only\nCluster size (not required): Global only\n\n\nThese settings ensure that it is only encounter rate variance that is affected by the post-stratification scheme; the detection function is still pooled over all observations as before.\n\n\nIn the next field, enter Global density estimate is Mean of stratum estimates, and in the next field select Weighted by Total effort in stratum. Do not tick the box saying Strata are Replicates.\nClick OK and run the new model. The point estimates should be the same as the previous non-overlapping post stratification run.\n\n\nNote: The precision of D and N are greatly improved in the post-stratified analyses. Note that we are not getting something for nothing: the second analysis is giving us an answer much closer to the true answer, while the first analysis was simply giving us the wrong answer. We have not changed the true variance by our post-stratification scheme: we are just getting a better estimate of the true variance. Because the data above were generated by simulation, we can use repeated simulated surveys to check that the second answer is indeed close to the true density variance over the repeats."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "title": "Variance estimation for systematic designs",
    "section": "Systematic designs where post-stratification is not needed",
    "text": "Systematic designs where post-stratification is not needed\nThe following simulated population does not exhibit strong trends across the survey region. Otherwise, the strip dimensions and systematic design are the same as for the previous example.\n\nDownload and open the project Systematic_variance_1.zip. Add the new data column VarGroup before conducting any analyses this time. With the augmented data, repeat the analyses you performed on the Systematic_variance_2.zip project. Find the relevant outputs. Has the post-stratification scheme been necessary in this case?"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#footnotes",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#footnotes",
    "title": "Variance estimation for systematic designs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nbecause it is effectively a sample of size 1 – only the first line position was randomly chosen, and the rest followed on deterministically from there.↩︎"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html",
    "title": "Analysis of point transect survey data",
    "section": "",
    "text": "Analysis of point transect survey data"
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html#additional-question-with-another-line-transect-data-set",
    "href": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html#additional-question-with-another-line-transect-data-set",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "Additional question with another line transect data set",
    "text": "Additional question with another line transect data set\n\n2. Below are perpendicular distance data (m) from line transect surveys of capercaillie (a large grouse) in Scotland. Total line length was 240km. The data are also in a text file capercaillie.txt you may download from this link and save on your computer. Description of a line transect survey of this species is presented in (Catt et al., 1998)\nIn the text file, column 1 is the transect number, column 2 is the transect length and column 3 is perpendicular distance as shown.\n\n\n  V1  V2 V3\n1  1 240 28\n2  1 240 17\n3  1 240 15\n4  1 240 14\n5  1 240 18\n6  1 240  0\n\n\nColumns are separated by tab characters. Create a new Distance project and either enter the data by hand or use the Data Import Wizard (Tools &gt; Import Data Wizard) to import the data from the text file. Then decide on a suitable model for the detection function and estimate bird density.\nCapercaillie, Monaughty Forest 112 detections\n28.0 17.0 15.0 14.0 18.0 0.0 38.0 6.0 50.0 65.0\n75.0 1.0 70.0 28.0 40.0 40.0 40.0 15.0 40.0 30.0\n5.0 55.0 60.0 40.0 24.0 30.0 0.0 50.0 55.0 10.0\n40.0 10.0 30.0 34.0 24.0 30.0 15.0 20.0 14.0 48.0\n0.0 30.0 2.0 52.0 11.0 48.0 28.0 38.0 25.0 35.0\n45.0 0.0 16.0 12.0 2.0 14.0 12.0 24.0 70.0 50.0\n49.0 40.0 80.0 18.0 27.0 30.0 30.0 60.0 58.0 14.0\n0.0 56.0 40.0 19.0 21.0 0.0 38.0 20.0 28.0 30.0\n20.0 16.0 0.0 69.0 40.0 46.0 50.0 40.0 70.0 67.0\n28.0 12.0 12.0 22.0 40.0 48.0 48.0 15.0 12.0 0.0\n15.0 20.0 17.0 30.0 30.0 32.0 48.0 20.0 10.0 20.0\n42.0 30.0"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard",
    "title": "Variance estimation for systematic designs",
    "section": "Component Percentages of Var(D)",
    "text": "Component Percentages of Var(D)\nDetection probability : 4.3 Encounter rate : 95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard-1",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard-1",
    "title": "Variance estimation for systematic designs",
    "section": "Component Percentages of Var(D)",
    "text": "Component Percentages of Var(D)\nDetection probability : 44.3 Encounter rate : 55.7\n\nThe CV is now even smaller, although it could have gone either way because this is an estimator of the same quantity as the last question–just a more precise estimator.\n\n\nEstimate %CV df 95% Confidence Interval\n\nHalf-normal/Cosine\nD 2044.6 7.97 75.87 1745.0 2395.6\nN 1022.0 7.97 75.87 872.00 1198.0\nThe encounter rate degrees of freedom are now 19 (number of lines – 1) rather than 10 (number of post-strata) for the previous question–which is why this is a more precise estimator of the variance.\n\nRemember we have not made any change to our data by the post-stratification; we are just getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be extreme as they are in this example, and post-stratification will not make a great difference. Such an example is shown below."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#estimate-cv-df-95-confidence-interval",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#estimate-cv-df-95-confidence-interval",
    "title": "Variance estimation for systematic designs",
    "section": "Estimate %CV # df 95% Confidence Interval",
    "text": "Estimate %CV # df 95% Confidence Interval\nHalf-normal/Cosine D 2129.2 27.40 999 20.74 1216.2 3727.5 1164.0 3427.2\nHalf-normal/Cosine N 1064.6 27.40 999 20.74 608.00 1864.0 582.00 1714.0\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals. Interval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\nThe bootstrap results are very similar to the analytic results, as we would expect. In fact, this did not used to be the case in previous versions of Distance, as the old analytic variance estimator did not perform well when there were extreme trends in both density and line length. You can access the previous default estimator under the Advanced… tab on the Variance page of the Model Definition Properties (its estimator R3), and more details are given in (Fewster et al., 2009).\nThe component percentages of variance are as follows:\n\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 4.3\nEncounter rate :       95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#simulated-point-transect-data",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#simulated-point-transect-data",
    "title": "Analysis of point transect survey data",
    "section": "Simulated point transect data",
    "text": "Simulated point transect data\n\nSimulated point transect data from 30 points are given in project PTExercise1.zip. These data were generated from a half-normal detection function, and true density was 79.6 animals ha-1.\n\nDownload and analyse this data set. Experiment with keys other than the half-normal (uniform, hazard-rate and negative exponential), to assess whether these data can be satisfactorily analysed using the wrong model. For each key, determine a suitable truncation point, and decide on whether, and which, adjustments are needed. (Truncation points come under the data filter.) How do bias and precision compare between models?"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#winter-wrens-in-scotland",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#winter-wrens-in-scotland",
    "title": "Analysis of point transect survey data",
    "section": "Winter wrens in Scotland",
    "text": "Winter wrens in Scotland\n\nThe following projects are available for download and analysis:\n\n\nWren1.zip,\nWren2.zip,\nWren3.zip and\nWren4.zip.\n\nThe projects contain winter wren data, collected at Montrave, Scotland in 2004, as described in (Buckland, 2006). Each project corresponds to a different method of data collection. Thirty-two points were defined through 33.2 ha of parkland (Fig. 1), and detection distances were measured in metres with the aid of a laser rangefinder. Three types of point transect data were collected: - standard five-minute counts - the ‘snapshot’ method and - a cue count method. In addition, line transect data were collected (method 4), and territory mapping was conducted, which gave an estimate of 43 wren territories (1.30 territories ha-1).\n\nSelect a single model for exploratory data analysis. Experiment with different truncation distances w, and select a suitable value for each method. Do you see potential problems with any of the data sets?\nTry other models and other model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density for each method. Record also the corresponding confidence intervals."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#savannah-sparrows-in-colorado-usa",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#savannah-sparrows-in-colorado-usa",
    "title": "Analysis of point transect survey data",
    "section": "Savannah sparrows in Colorado USA",
    "text": "Savannah sparrows in Colorado USA\n\nTwo additional point transect projects are available for download and analysis:\n\n\nSavannah Sparrow 1980.zip and\nSavannah Sparrow 1981.zip.\n\nThese were part of a large data set collected in Arapaho National Wildlife Refuge, Colorado. For both data sets, consider an appropriate truncation distance, decide on a suitable model for the detection function, and estimate density, both for each stratum individually and for the whole study area. You should include in your analysis an assessment of whether the detection function can be estimated from data pooled across strata, or whether separate estimates are needed per stratum. (This will be covered in the lecture discussing stratification if you don’t already know how to do it.)"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#simulated-point-transect-data",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#simulated-point-transect-data",
    "title": "Analysis of point transect survey data",
    "section": "Simulated point transect data",
    "text": "Simulated point transect data\n\nResults from selected model options; remember these are simulated data with a half normal detection function and true density 79.6:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nAdjustment\nterms\nw(m)\n\\(\\hat{D}\\)\n%CV\nLCB\nUCB\n\n\n\n\nHalf-normal\nNone\n0\n34.2\n79.6\n12.6\n62.1\n102.1\n\n\nHalf-normal\nNone\n0\n20\n70.8\n15.7\n52\n96.5\n\n\nUniform\nCosine\n1\n20\n75\n14.4\n56.5\n99.6\n\n\nHazard-rate\nNone\n0\n20\n62.4\n18.7\n43.2\n90\n\n\nNeg Exp\nSimple poly\n1\n20\n73.1\n29.2\n41.5\n128.6\n\n\n\nWe see a fair degree of variability between analyses–reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates. For these data, the hazard-rate model appears to have downward bias, and precision is very poor for the negative exponential model."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#winter-wren-data-analysis-with-bonus-species-included",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#winter-wren-data-analysis-with-bonus-species-included",
    "title": "Analysis of point transect survey data",
    "section": "Winter wren data analysis (with bonus species included)",
    "text": "Winter wren data analysis (with bonus species included)\n\nOur analyses of the winter wren data produced the following estimated densities (territories ha-1). Three other species were surveyed in the field. Estimates of density for those species are in the table below, although the Distance for Windows projects are not included. Method 5 is territory mapping (which does not use distance sampling, and as you note has no measure of precision associated because it is akin to a census method).\n\n\n\n\nMethod\nCommon Chaffinch\nGreat Tit\nEuropean Robin\nWinter Wren\n\n\n\n\n1\n1.03\n0.58\n0.52\n1.29\n\n\nCI\n(0.74,1.43)\n(0.36,0.94)\n(0.26,1.06)\n(0.80,2.11)\n\n\n2\n0.90\n0.22\n0.60\n1.02\n\n\nCI\n(0.62,1.29)\n(0.13,0.39)\n(0.38,0.94)\n(0.80,1.32)\n\n\n3\n0.71\n0.26\n0.82\n1.21\n\n\nCI\n(0.45,1.23)\n(0.09,0.76)\n(0.52,1.31)\n(0.82,1.79)\n\n\n4\n0.64\n0.26\n0.69\n1.07\n\n\nCI\n(0.46,0.90)\n(0.16,0.42)\n(0.47,1.00)\n(0.87,1.31)\n\n\n5\n0.75\n0.21\n0.84\n1.30\n\n\n\nTo obtain the above estimates, I used a truncation distance of 110m for methods 1 and 2, 92.5m for method 3, and 95m for method 4. For the wren data, I used the uniform key with two cosine adjustments for method 1, the hazard-rate model for methods 2 and 3, and the half-normal key with two Hermite polynomial adjustments for method 4.\nPoints to note about the wren data: the wren more than any of the other species showed evidence of observer avoidance. This didn’t cause too many difficulties, except that the model favoured by AIC for line transect sampling was the hazard-rate model, which had a very flat shoulder out to around 75m. It was implausible that detection was certain out to this distance, so that I selected a model with a slightly inferior AIC value, but with a more plausible fitted detection function. Analyses of the cue count data are necessarily rather subjective, as the data show substantial over-dispersion (a single bird may give many songbursts all from the same location during a five-minute count). In this circumstance, goodness-of-fit tests are very misleading, and care must be taken not to overfit the data."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#savannah-sparrow-analysis",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#savannah-sparrow-analysis",
    "title": "Analysis of point transect survey data",
    "section": "Savannah sparrow analysis",
    "text": "Savannah sparrow analysis\n\nGood fits to the 1980 Savannah sparrow data were obtained by truncating at 55m. The half-normal detection function without adjustments fitted well, as did the uniform with cosine adjustments. The hazard-rate model performed less well. There was a marginal preference for fitting the detection function separately in each stratum as judged by AIC, but pooling distance data across strata might offer rather more robust estimation. The estimates of density in the table correspond to a half-normal detection function, fitted separately in each stratum, with a truncation distance of 55m.\n\nFor 1981, w=55m was again satisfactory. There was now a clear preference for estimating the detection function separately by stratum, but little to choose between the half-normal model and the uniform model with cosine adjustments. For comparability with 1980, I chose the half-normal model, although AIC showed a very marginal preference for uniform + cosine. (Again, the hazard-rate model provided less good fits overall.)\nEstimated densities \\(\\hat{D}\\) (birds/ha) of Savannah sparrows\n\n\n\nYear\nPasture\n\\(\\hat{D}\\)\nLCB\nUCB\n\n\n\n\n1980\n1\n1.43\n0.94\n2.18\n\n\n\n2\n4.12\n3.15\n5.38\n\n\n\n3\n2.35\n1.72\n3.2\n\n\n\nAll\n2.63\n2.19\n3.16\n\n\n\n\n\n\n\n\n\n1981\n0\n1.39\n0.82\n2.36\n\n\n\n1\n0.52\n0.27\n1.03\n\n\n\n2\n1.7\n1.07\n2.71\n\n\n\n3\n1.35\n0.81\n2.26\n\n\n\nAll\n1.24\n0.95\n1.62"
  }
]