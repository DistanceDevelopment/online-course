[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory distance sampling training materials",
    "section": "",
    "text": "This is the landing page for the distance sampling training materials website.\n\nDescribe philosophy of lecture/exercise pairing.\ndifferentiate the two exercise pathways\ndiscuss role of video introduction/summary\nif there are self-assessed tutorials, describe them\ngiscus if it exists https://giscus.app/\n\n\n\n\nOther sundries\n\nmention distancesampling.org\nmention “other workshops” if they exist"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#and-everything-else",
    "href": "index.html#and-everything-else",
    "title": "async2024-2",
    "section": "And everything else",
    "text": "And everything else"
  },
  {
    "objectID": "02-detfns/detfnlanding.html",
    "href": "02-detfns/detfnlanding.html",
    "title": "Fitting detection functions to line transect data",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described in (Anderson & Pospahala, 1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit."
  },
  {
    "objectID": "02-detfns/detfnlanding.html#estimating-density-of-duck-nests",
    "href": "02-detfns/detfnlanding.html#estimating-density-of-duck-nests",
    "title": "Fitting detection functions to line transect data",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described in (Anderson & Pospahala, 1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit."
  },
  {
    "objectID": "02-detfns/truncation-decisions.html",
    "href": "02-detfns/truncation-decisions.html",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "This question arose in our discussion of truncation. The truncation decision is one of the first decisions an analyst will make when performing a distance sampling analysis. Is the truncation decision a decision that has profound consequences and therefore deserve lots of thought (answer: probably not). Let’s see\nI have written a function that simply repeatedly calls ds() to fit a detection function to a data set. Within the function is a for() loop containing the call to ds(). After each call to ds() the results of the fitted model object are stored within a data frame for subsequent plotting. The function retains the point estimate of density and the confidence interval bounds of the estimate.\n\n\nIs presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#the-function",
    "href": "02-detfns/truncation-decisions.html#the-function",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "Is presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#duck-nest-result",
    "href": "02-detfns/truncation-decisions.html#duck-nest-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Duck nest result",
    "text": "Duck nest result\nThe figure below shows there is very little change in point estimates of duck nest density until ~20% (1/5th) of the data have been truncated. The magnitude of the point estimate changes are minute in comparison to the width of the confidence intervals.\n\ntrange &lt;- 1:26\nplot(trange-1, duck.trunc$est, type=\"p\", \n     ylim=range(c(duck.trunc[,2], duck.trunc[,3])), pch=20,\n     main=\"Duck nest data\\ntruncation experiment\", ylab=\"Nest density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, duck.trunc[trange,2],\n         trange[trange]-1, duck.trunc[trange,3])\n\n\n\n\nEffects of truncation upon density estimates for duck nest data."
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#simulated-line-transect-result",
    "href": "02-detfns/truncation-decisions.html#simulated-line-transect-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Simulated line transect result",
    "text": "Simulated line transect result\nThe value of writing functions is they can be re-used if written with sufficient generality. I re-use the truncation function upon a different data set; the simulated data from Practical 3 where the true density is known to be 79.8 animals per hectare (depicted with horizontal dashed line).\n\ndata(\"LTExercise\")\nsim.trunc &lt;- trunc.experiment(mydata=LTExercise, trunc.range=0:25, cu=cu, type=\"line\")\n\n\nplot(trange-1, sim.trunc$est, type=\"p\", \n     ylim=range(c(sim.trunc[,2], sim.trunc[,3])), pch=20,\n     main=\"Simulated data\\ntruncation experiment\", ylab=\"Density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, sim.trunc[trange,2],\n         trange[trange]-1, sim.trunc[trange,3])\nabline(h=79.8, lwd=2, lty=3)\n\n\n\n\nEffects of truncation upon density estimates for simulated line transect data."
  },
  {
    "objectID": "02-detfns/Pr2-instructions.html",
    "href": "02-detfns/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "In this exercise, we use R (R Core Team, 2019) and the Distance package (Miller et al., 2019) to fit different detection function models to the duck nest data (introduced in Exercise 1) and estimate duck nest density and abundance.\n\nObjectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it from CRAN:\n\ninstall.packages(Distance)\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn &lt;- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "02-detfns/Prac2_solution.html",
    "href": "02-detfns/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks &lt;- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn &lt;- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel       : Half-normal key function \nAIC         :  928.1338 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "02-detfns/detfnlanding.html#lecture-heading",
    "href": "02-detfns/detfnlanding.html#lecture-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Lecture heading",
    "text": "Lecture heading\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#exercise-heading",
    "href": "02-detfns/detfnlanding.html#exercise-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Exercise heading",
    "text": "Exercise heading\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#supplement-heading",
    "href": "02-detfns/detfnlanding.html#supplement-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Supplement heading",
    "text": "Supplement heading\n\nEffect of truncation upon density estimates\n\nDemonstration of changing truncation distance"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#lecture-materials",
    "href": "02-detfns/detfnlanding.html#lecture-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#exercise-materials",
    "href": "02-detfns/detfnlanding.html#exercise-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#supplemental-materials",
    "href": "02-detfns/detfnlanding.html#supplemental-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nEffect of truncation upon density estimates\n\nDemonstration of changing truncation distance"
  },
  {
    "objectID": "03-criticism/criticismlanding.html",
    "href": "03-criticism/criticismlanding.html",
    "title": "Model criticism",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the R code inside the Quarto (.qmd) instructions.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird."
  },
  {
    "objectID": "03-criticism/criticismlanding.html#absolute-and-relative-fit-of-detection-function-models",
    "href": "03-criticism/criticismlanding.html#absolute-and-relative-fit-of-detection-function-models",
    "title": "Model criticism",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the R code inside the Quarto (.qmd) instructions.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird."
  },
  {
    "objectID": "03-criticism/criticismlanding.html#lecture-materials",
    "href": "03-criticism/criticismlanding.html#lecture-materials",
    "title": "Model criticism",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "03-criticism/criticismlanding.html#exercise-materials",
    "href": "03-criticism/criticismlanding.html#exercise-materials",
    "title": "Model criticism",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "03-criticism/criticismlanding.html#supplemental-materials",
    "href": "03-criticism/criticismlanding.html#supplemental-materials",
    "title": "Model criticism",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nA detailed look at model selection\n\nThe two stages of model selection"
  },
  {
    "objectID": "01-intro/introlanding.html#estimating-widehatp_a-with-a-pencil",
    "href": "01-intro/introlanding.html#estimating-widehatp_a-with-a-pencil",
    "title": "Fundamentals of distance sampling",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region."
  },
  {
    "objectID": "01-intro/introlanding.html#lecture-materials",
    "href": "01-intro/introlanding.html#lecture-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "01-intro/introlanding.html#exercise-materials",
    "href": "01-intro/introlanding.html#exercise-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Exercise materials",
    "text": "Exercise materials\n\nExercise\nThis exercise does not involve use of the computer. Instead, you will be graphing the distribution of perpendicular detection distances. You will fit (using nothing but your eye), a detection function curve to the histogram of detection distances.\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "01-intro/introlanding.html#supplemental-materials",
    "href": "01-intro/introlanding.html#supplemental-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNothing to add to first module\n\nNo supplement"
  },
  {
    "objectID": "01-intro/introlanding.html",
    "href": "01-intro/introlanding.html",
    "title": "Fundamentals of distance sampling",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region."
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html",
    "href": "03-criticism/R-prac/modelsel-demo.html",
    "title": "Demonstration two stages of model selection",
    "section": "",
    "text": "Demonstration\n\n\n\nEastern tropical Pacific dolphin data\nAfter my improvised description of selection of adjustment terms, I thought I should provide a more thorough description via an example. The purpose of the demonstration is to fit models with adjustments to a data set and expose, in detail, all models fitted during the process.\nFor this demonstration, I require a data set with an interesting shape to the histogram. I will not describe the data set, other than to note it contains roughly 1000 detections. We will see this data set again next week. More complete details of the data set, as well as a detailed analysis are in Marques & Buckland (2003)."
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#half-normal-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#half-normal-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Half-normal cosine",
    "text": "Half-normal cosine\n\nhncos &lt;- ds(bino, key=\"hn\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting half-normal key function\n\n\nAIC= 2816.871\n\n\nFitting half-normal key function with cosine(2) adjustments\n\n\nAIC= 2806.01\n\n\nFitting half-normal key function with cosine(2,3) adjustments\n\n\nAIC= 2807.589\n\n\n\nHalf-normal key function with cosine(2) adjustments selected.\n\n\nThree models with the half-normal key are fitted, with the preferred model being the second fitted, namely the model with a single adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hn', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   D --&gt; ID2\n   ID0(hn0&lt;br&gt;AIC=2817)\n   ID1(hn1&lt;br&gt;AIC=2806)\n   ID2(hn2&lt;br&gt;AIC=2808)\n   FIN(hn1)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN"
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#uniform-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#uniform-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Uniform cosine",
    "text": "Uniform cosine\n\nunicos &lt;- ds(bino, key=\"unif\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting uniform key function\n\n\nAIC= 2971.022\n\n\nFitting uniform key function with cosine(1) adjustments\n\n\nAIC= 2811.177\n\n\nFitting uniform key function with cosine(1,2) adjustments\n\n\nAIC= 2808.378\n\n\nFitting uniform key function with cosine(1,2,3) adjustments\n\n\nAIC= 2806.685\n\n\nFitting uniform key function with cosine(1,2,3,4) adjustments\n\n\nAIC= 2808.105\n\n\n\nUniform key function with cosine(1,2,3) adjustments selected.\n\n\nThe same pattern as with the half-normal key, with a small exception. Four models with the uniform key are fitted, with the preferred model being the third fitted, namely the model with a three adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='unif', adj='cos')\n   D --&gt; ID1\n   D --&gt; ID2\n   D --&gt; ID3\n   D --&gt; ID4\n   ID1(unif1&lt;br&gt;AIC=2811)\n   ID2(unif2&lt;br&gt;AIC=2808)\n   ID3(unif3&lt;br&gt;AIC=2807)\n   ID4(unif4&lt;br&gt;AIC=2808)\n   FIN(unif3)\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN\n   ID3 --&gt; FIN\n   ID4 --&gt; FIN"
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#hazard-rate-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#hazard-rate-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Hazard rate cosine",
    "text": "Hazard rate cosine\n\nhrcos &lt;- ds(bino, key=\"hr\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting hazard-rate key function\n\n\nAIC= 2805.467\n\n\nFitting hazard-rate key function with cosine(2) adjustments\n\n\nAIC= 2807.47\n\n\n\nHazard-rate key function selected.\n\n\nTwo models are fitted with the hazard rate key function. The addition of a single adjustment term does not improve the AIC score, so there is no point in fitting a more complex model with additional adjustment terms.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hr', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   ID0(hr0&lt;br&gt;AIC=2805)\n   ID1(hr1&lt;br&gt;AIC=2807)\n   FIN(hr0)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n\n\n\n\n\nThe contestants that emerge from the first round of model competition are:\n\nhalf-normal with 1 adjustment term\nuniform with 3 adjustment terms\nhazard rate with no adjustment terms"
  },
  {
    "objectID": "05-points/pointslanding.html",
    "href": "05-points/pointslanding.html",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "05-points/pointslanding.html#analysis-of-point-transect-data",
    "href": "05-points/pointslanding.html#analysis-of-point-transect-data",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "05-points/pointslanding.html#lecture-materials",
    "href": "05-points/pointslanding.html#lecture-materials",
    "title": "Analysis of point transect data",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "05-points/pointslanding.html#exercise-materials",
    "href": "05-points/pointslanding.html#exercise-materials",
    "title": "Analysis of point transect data",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "05-points/pointslanding.html#supplemental-materials",
    "href": "05-points/pointslanding.html#supplemental-materials",
    "title": "Analysis of point transect data",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html",
    "href": "05-points/R-prac/Prac5_solution.html",
    "title": "Point transect sampling solution",
    "section": "",
    "text": "Solution\n\n\n\nPoint transect analysis exercise"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#truncation-of-20m",
    "href": "05-points/R-prac/Prac5_solution.html#truncation-of-20m",
    "title": "Point transect sampling solution",
    "section": "Truncation of 20m",
    "text": "Truncation of 20m\n\nPTExercise.hn.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hn\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.hr.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hr\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.uf.cos.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"unif\", \n                        adjustment=\"cos\", truncation=20,convert_units=conversion.factor)\n\n\n\n\nResults from simulated point transect data.\n\n\n\n\n\n\n\n\n\n\n\n\nDetectionFunction\nAdjustments\nTruncation\nAIC\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\nNone\n34.2\n919.140\n79.630\n0.126\n62.121\n102.074\n\n\nHalf-normal\nNone\n20.0\n764.305\n70.826\n0.157\n51.978\n96.507\n\n\nHazard rate\nNone\n20.0\n767.211\n62.364\n0.187\n43.207\n90.015\n\n\nUniform\nCosine\n20.0\n765.503\n75.044\n0.144\n56.515\n99.648"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "href": "05-points/R-prac/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "title": "Point transect sampling solution",
    "section": "Plots of probability density functions to inspect fit",
    "text": "Plots of probability density functions to inspect fit\n\nplot(PTExercise.hn, main=\"Half normal, no truncation\", pdf=TRUE)\nplot(PTExercise.hn.t20m, main=\"Half normal, truncation 20m\", pdf=TRUE)\nplot(PTExercise.hr.t20m, main=\"Hazard rate, truncation 20m\", pdf=TRUE)\nplot(PTExercise.uf.cos.t20m, main=\"Uniform with cosine, truncation 20m\", pdf=TRUE)\n\n\n\n\n\n\nHalf normal without truncation\n\n\n\n\n\n\n\nHalf normal 20m truncation\n\n\n\n\n\n\n\n\n\nHazard rate 20m truncation\n\n\n\n\n\n\n\nUniform cosine 20m truncation\n\n\n\n\n\n\nWe see a fair degree of variability between analyses - reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates."
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "href": "05-points/R-prac/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "title": "Point transect sampling solution",
    "section": "Probability density functions for Buckland’s winter wren point transects",
    "text": "Probability density functions for Buckland’s winter wren point transects\n\nplot(wren5min.uf.cos.t110, main=\"5 minute count\")\nplot(wrensnap.hr.cos.t110, main=\"Snapshot moment\")\n\n\n\n\n\n\nDetection function 5 minute count\n\n\n\n\n\n\n\nDetection function snapshot\n\n\n\n\n\n\nAs the detection distance histograms indicate, winter wren showed evidence of observer avoidance, more than other species in the Montrave study. We show the detection function graph rather than the PDF to emphasise the evasive movement aspect of the data. If you conduct the goodness of fit test, using gof_ds(), you will find that the models still suitably fit the data."
  },
  {
    "objectID": "04-precision/precisionlanding.html",
    "href": "04-precision/precisionlanding.html",
    "title": "Precision of density estimates",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set."
  },
  {
    "objectID": "04-precision/precisionlanding.html#precision-of-density-estimates",
    "href": "04-precision/precisionlanding.html#precision-of-density-estimates",
    "title": "Precision of density estimates",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set."
  },
  {
    "objectID": "04-precision/precisionlanding.html#lecture-materials",
    "href": "04-precision/precisionlanding.html#lecture-materials",
    "title": "Precision of density estimates",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "04-precision/precisionlanding.html#exercise-materials",
    "href": "04-precision/precisionlanding.html#exercise-materials",
    "title": "Precision of density estimates",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "04-precision/precisionlanding.html#supplemental-materials",
    "href": "04-precision/precisionlanding.html#supplemental-materials",
    "title": "Precision of density estimates",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nCould use left-truncation\n\nThere is no supplemental material"
  },
  {
    "objectID": "03-criticism/DistWin-prac/temp.html",
    "href": "03-criticism/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "02-detfns/DistWin-prac/temp.html",
    "href": "02-detfns/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "04-precision/DistWin-prac/temp.html",
    "href": "04-precision/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "05-points/DistWin-prac/temp.html",
    "href": "05-points/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html",
    "href": "05-points/R-prac/Pr5-instructions.html",
    "title": "Point transect sampling",
    "section": "",
    "text": "The purpose of this exercise is to analyse point transect survey data: it can sometimes be more difficult than line transect data. In the first problem, the data are simulated and so the true density is known. In the second problem, two different data collection methods were used to survey song birds."
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html#probability-density-function",
    "href": "05-points/R-prac/Pr5-instructions.html#probability-density-function",
    "title": "Point transect sampling",
    "section": "Probability density function",
    "text": "Probability density function\nTo plot the more informative probability density function (pdf), an additional argument is required in the plot() function:\n\nplot(ptdat.hn, pdf=TRUE)"
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "href": "05-points/R-prac/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "title": "Point transect sampling",
    "section": "Analyse both conventional and snapshot data sets",
    "text": "Analyse both conventional and snapshot data sets\nWhat to do:\n\nSelect a simple model for exploratory data analysis. Experiment with different truncation distances, \\(w\\), and select a suitable value for each method. Are there any potential problems with any of the data sets?\nTry other models and model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density and corresponding confidence interval for each method."
  },
  {
    "objectID": "02-detfns/R-prac/Pr2-instructions.html",
    "href": "02-detfns/R-prac/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "In this exercise, we use R (R Core Team, 2019) and the Distance package (Miller et al., 2019) to fit different detection function models to the duck nest data (introduced in Exercise 1) and estimate duck nest density and abundance.\n\nObjectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it from CRAN:\n\ninstall.packages(Distance)\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn &lt;- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "02-detfns/R-prac/Prac2_solution.html",
    "href": "02-detfns/R-prac/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks &lt;- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn &lt;- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel       : Half-normal key function \nAIC         :  928.1338 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html",
    "href": "03-criticism/R-prac/Pr3-instructions.html",
    "title": "Assessing line transect detection functions",
    "section": "",
    "text": "In Exercise 2, we fitted different detection functions and compared them in terms of goodness of fit and AIC. Here, we continue to fit and assess different models and look at additional arguments in the ds package."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#accessing-the-data",
    "href": "03-criticism/R-prac/Pr3-instructions.html#accessing-the-data",
    "title": "Assessing line transect detection functions",
    "section": "Accessing the data",
    "text": "Accessing the data\nLoad the Distance package to access both the data and analytical functions.\n\n# Load library (if not already loaded)\nlibrary(Distance)\n# Access data\ndata(\"LTExercise\")\n# Check that it has been imported correctly\nhead(LTExercise, n=3)\n\n\nData format: no detections on a transect\nBefore fitting models, it is worth investigating the data a bit further: let’s start by summarising the perpendicular distances:\n\n# Summary of perpendicular distances\nsummary(LTExercise$distance)\n\nThe summary indicates that the minimum distance is min(hndat$distance) and the maximum is max(hndat$distance) metres and there is one missing value (indicated by the NA). If we print a few rows of the data, we can see that this missing value occurred on transect ‘Line 11’.\n\n# Print out a few lines of data\nLTExercise[100:102, ]\n\nThe NA indicates that there were no detections on this transect, but the transect information needs to be included otherwise the number of transects and the total line length will be incorrect."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#truncation",
    "href": "03-criticism/R-prac/Pr3-instructions.html#truncation",
    "title": "Assessing line transect detection functions",
    "section": "Truncation",
    "text": "Truncation\nLet’s start by fitting a basic model, i.e. no adjustment terms (by default a half normal model is fitted).\nFor this project, perpendicular distances are in metres and the transect lines are recorded in kilometres.\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n# Fit half normal, no adjustments\nlt.hn &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL,\n            convert_units=conversion.factor)\n\nLooking at a summary of the model object, how many objects are there in total? What is the maximum observed perpendicular distance?\n\n# Print a summary of the fitted detection function\nsummary(lt.hn)\n\nPlot the detection function and specify many histogram bins:\n\nplot(lt.hn, nc=30)\n\nThe histogram indicates that there is a large gap in detections therefore, to avoid a long right hand tail in the detection function, truncation is necessary. There are several ways to truncate: excluding distances beyond some specified distance or excluding a specified percentage of the largest distances. Note that here we only consider excluding large perpendicular distances, which is frequently referred to as right truncation.\n\nTruncation at a fixed distance\nThe following command truncates the perpendicular distances at 20 metres i.e. objects detected beyond 20m are excluded.\n\n# Truncate at 20metres\nlt.hn.t20m &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=20,\n                 convert_units=conversion.factor)\n\nGenerate a summary and plot of the detection function to see what effect this truncation has had on the number of objects.\n\n\nTruncating a percentage of distances\nAn alternative way to truncate distances is to specify a percentage of detected objects that should be excluded. In the command below, 10% of the largest distances are excluded.\n\n# Truncate largest 10% of distances\nlt.hn.t10per &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=\"10%\",\n                   convert_units=conversion.factor)\n\nGenerate a summary and plot to see what effect truncation has had.\n\nsummary(lt.hn.t10per)\nplot(lt.hn.t10per)"
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#exploring-different-models",
    "href": "03-criticism/R-prac/Pr3-instructions.html#exploring-different-models",
    "title": "Assessing line transect detection functions",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don’t spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‘wrong’ model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\n# Half normal detection, cosine adjustments, no truncation\nlt.hn.cos &lt;- ds(data=LTExercise, key=\"hn\", adjustment=\"cos\",\n                convert_units=conversion.factor)\n\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\)."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "href": "03-criticism/R-prac/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, …, 80 are specified.\n\ndata(capercaillie)\n# Specify cutpoint for bins\nbins &lt;- seq(from=0, to=80, by=10)\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"hectare\")\n# Fit model with binned distances\ncaper.bin &lt;- ds(data=capercaillie, key=\"hn\", cutpoints=bins, \n                convert_units=conversion.factor)\nplot(caper.bin)"
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html",
    "href": "03-criticism/R-prac/Prac3_solution.html",
    "title": "Assessing line transect detection functions solution",
    "section": "",
    "text": "Solution\n\n\n\nAssessing line transect detection functions"
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "href": "03-criticism/R-prac/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "title": "Assessing line transect detection functions solution",
    "section": "Fitting multiple models to exact distance data",
    "text": "Fitting multiple models to exact distance data\n\n# Half normal model \ncaper.hn.cos &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Hazard rate model  \ncaper.hr.cos &lt;- ds(data=capercaillie, key=\"hr\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Uniform model  \ncaper.uf.cos &lt;- ds(data=capercaillie, key=\"unif\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n\nThe detection functions and QQ plots are shown below:\n\nplot(caper.hn.cos, main=\"Half normal\")\nx &lt;- gof_ds(caper.hn.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.hr.cos, main=\"Hazard rate\")\nx &lt;- gof_ds(caper.hr.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.uf.cos, main=\"Uniform\")\nx &lt;- gof_ds(caper.uf.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nQQ plot half normal\n\n\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\n\nQQ plot hazard rate\n\n\n\n\n\n\n\n\n\nUniform with adjustment\n\n\n\n\n\n\n\nQQ plot uniform adj\n\n\n\n\n\n\nSummarise the goodness of fit statistics (in a pretty format). This table indicates that the hazard rate detection function had the lowest AIC but the difference in AIC between all three models was small.\n\nknitr::kable(summarize_ds_models(caper.hn.cos, caper.hr.cos, caper.uf.cos, output=\"plain\"),\n               caption=\"Summary of results of Capercaillie analysis.\", digits = 3)\n\n\nSummary of results of Capercaillie analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\n2\ncaper.hr.cos\nHazard-rate\n~1\n0.663\n0.703\n0.052\n0.000\n\n\n1\ncaper.hn.cos\nHalf-normal\n~1\n0.332\n0.613\n0.053\n0.031\n\n\n3\ncaper.uf.cos\nUniform with cosine adjustment terms of order 1,2\nNA\n0.613\n0.682\n0.098\n0.280\n\n\n\n\n\nThe results for the three different models are shown below: density is in birds per ha.\n\n\n\nCapercaillie point estimates of density and associated measures of precision.\n\n\nDetectionFunction\nAIC\nPa\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\n957.905\n0.613\n0.048\n0.148\n0.027\n0.083\n\n\nHazard rate\n957.874\n0.703\n0.042\n0.148\n0.020\n0.084\n\n\nUniform\n958.154\n0.682\n0.043\n0.191\n0.026\n0.069\n\n\n\n\n\nThese capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results."
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "href": "03-criticism/R-prac/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions solution",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nTo deal with rounding in the distance data, the exact distances can be converted into binned distances. The cutpoints need to be chosen with care so that the distance bins are sufficiently wide enough to ensure that the ‘correct’ perpendicular distance is in the band containing the rounded recorded value. The bin widths do not have to be equal, as shown in example here: the cutpoints are 0, 7.5, 17.5, 27.5, …, 67.5, 80.0 m. Note, that any distances beyond the largest bin will be excluded.\n\n# Specify (uneven) cutpoint for bins\nbins &lt;- c(0, seq(from=7.5, to=67.5, by=10), 80)\nprint(bins)\n\n[1]  0.0  7.5 17.5 27.5 37.5 47.5 57.5 67.5 80.0\n\ncaper.hn.bin &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\", cutpoints=bins,\n                   convert_units=conversion.factor)\nplot(caper.hn.bin, main=\"Capercaillie, binned distances\")\n\n\n\n# See a portion of the results\nknitr::kable(caper.hn.bin$dht$individuals$summary, row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\nmean.size\nse.mean\n\n\n\n\nMonaughty Forest\n1472\n3840\n240\n112\n1\n0.4666667\n0\n0\n1\n0\n\n\n\n\nknitr::kable(caper.hn.bin$dht$individuals$D[1:6], row.names = FALSE, digits=3)\n\n\n\n\nLabel\nEstimate\nse\ncv\nlcl\nucl\n\n\n\n\nTotal\n0.045\n0.007\n0.152\n0.026\n0.079\n\n\n\n\n\nNote that the binning of the data results in virtually identical estimates of density (0.045 birds per ha) and essentially no change in the precision of the density estimate compared with the estimates with analysis of exact distance data."
  },
  {
    "objectID": "04-precision/R-prac/Pr4-instructions.html",
    "href": "04-precision/R-prac/Pr4-instructions.html",
    "title": "Variance estimation for systematic survey designs",
    "section": "",
    "text": "In the lecture describing measures of precision, we explained that systematic survey designs usually have the best variance properties, but obtaining good estimates of the variance is a difficult problem for statisticians. In this exercise, we give an example of a situation where the systematic design gives a density estimate with much better precision than a random design. This means that the usual variance estimators used in the ds function, which are based on random transect placement, are far too high. The true variance is low, but the estimated variance is high.\nWe will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance. In Section 6, we examine another case to see that the unstratified variance estimates provided by ds are usually fine for a systematic design: things only go wrong when there are strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g. the highest densities always occur on the shortest lines, or vice versa).\nWe begin with a population and survey shown below. The data used for this exercise were simulated on a computer: they are not real data. Note the characteristics for the data in Figure 1: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design has covered a fairly large proportion of the survey area (the covered region is shaded). These are danger signals that the usual ds variance estimators might not work well and a post-stratification scheme should be considered.\n\n\n\n\n\nAn example of survey data where there is a strong trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\n\nObjectives\nThe aims of this exercise are to illustrate:\n\nDefault variance estimation,\nVariance estimation with bootstrapping,\nPost-stratification to improve variance estimation,\nWhen post-stratification is not needed (optional).\n\n\n\nGetting started\nDon’t forget to load the Distance package for your session.\n\nlibrary(Distance)\n\n\n\nBasic (default) variance estimation\nIn the code below, the necessary data file is imported and a simple model is fitted and a summary produced. Make a note of the CV of the density estimate - this is obtained using the default (analytical) estimator in the ds function and is based on the assumption that the lines were placed at random. This CV can then be compared with the CV estimates obtained from alternative methods.\n\n# Import data\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\n# Fit a simple model\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=NULL,\n                 convert_units=conversion.factor)\n# Summary\nprint(sysvar2.hn$dht$individuals$D)\nprint(sysvar2.hn$dht$individuals$N)\n\nThe true density and abundance are known (because the data were simulated): the true abundance in the survey region was \\(N=1000\\) and \\(D=2000 \\textrm{ animals per km}^2\\) (i.e. 1000 animals in an area of size \\(A=0.5 \\textrm{km}^2\\)). How do the point estimates compare with truth? What do you think about the precision of the estimates?\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\n# Bootstrap estimate of uncertainty\n# Run the bootstrap (this can take a while if nboot is large!) \nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize,\n                    convert_units=conversion.factor, nboot=199)\n\nThe arguments for this command are:\n\nmodel - fitted detection function model object\nflatfile - data frame of the survey data\nsummary_fun - function used to obtain the summary statistics from each bootstrap\nconvert_units - conversion units for abundance estimation\nnboot - number of bootstrap samples to generate. Note, it can take a long time to produce a large number of bootstraps and so perhaps try a small number at first.\n\n\n# See the results\nsummary(est.boot)\n\nThe summary includes:\n\nEstimate - the median value of the bootstrap estimates\nse is the standard deviation of the bootstrap estimates\nlcl and ucl are the limits for a 95% confidence interval.\ncv is the coefficient of variation (\\(CV = SE/Estimate\\))\n\nAre the bootstrapped confidence intervals for abundance and density similar to the analytical confidence intervals produced previously?\nRecall that we have a particular situation in which we have systematically placed transects which are unequal in length. Furthermore, there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study region with the highest density. In the next section, we examine a process by which we can use post-stratification to produce a better estimate of the variance in estimated abundance.\n\n\nPost-stratification to improve variance estimation\nThe estimation of encounter rate variance in the previous section used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic and, in some circumstances, taking this in account can substantially reduce the encounter rate variance. The data we are working with is an example of this, where there are very high densities on the very shortest lines. In samples of lines, collected using a completely random design, the sample, by chance, might not contain any very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance. Although there is no way of getting a variance estimate that is exactly unbiased for a systematic sample because it is effectively a sample of size 1- only the first line position was randomly chosen and the rest followed on deterministically from there. We can greatly improve on the random-based estimate by using a post-stratification scheme.\nThe post-stratification scheme works by grouping together pairs of adjacent lines from the systematic sample and each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample. This encounter rate estimator is called ‘O2’ (Fewster et al., 2009) and is implemented in the dht2 function.\n\n# Post-stratification - stratified variance estimation by grouping adjacent transects\n\n# Ensure that Sample.Labels are numeric, this is required for O2 ordering\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\n\n# Use the Fewster et al 2009, \"O2\" estimator \nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, strat_formula=~1, \n               convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nNote that this estimator assumes that the numbering of the transects (in this example Sample.Label takes values 1 to 20) has some geographical meaning (i.e. transect 1 is next to 2 and 2 is next to 3 etc.). If this is not the case, then the user can manually define some sensible grouping of transects and create a column called grouping in the data object.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThe simulated population shown in Figure 2 does not exhibit strong trends across the survey region, otherwise, the strip dimensions and systematic design are the same as for the previous example. These data are stored in the data set Systematic_variance_1.\n\n\n\n\n\nAn example of survey data that does not exhibit a trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\nIn the code below, these data are imported into R and a simple detection function model is fitted. The default estimate of variance is then compared to that obtained using the ‘O2’ estimator (Fewster et al., 2009).\n\n# When post-stratification is not needed\n# Import the data\ndata(\"Systematic_variance_1\")\n# Ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\n# First fit a simple model\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\n# Obtain default estimates for comparison\nprint(sysvar1.hn$dht$individuals$D)\nprint(sysvar1.hn$dht$individuals$N)\n# Now use Fewster et al 2009, \"O2\" estimator \nest1.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1, \n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est1.O2, report=\"both\")\n\nDid you see a difference in the CV and 95% confidence interval between the two estimators?\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "04-precision/R-prac/Prac4_solution.html",
    "href": "04-precision/R-prac/Prac4_solution.html",
    "title": "Variance estimation for systematic survey designs solution",
    "section": "",
    "text": "Solution\n\n\n\nVariance estimation for systematic designs\n\n\n\nBasic (default) variance estimation\nRecall the data for this example, in which we have a strong gradient in animal density across our study region and at the same time we have a difference in the lengths of the transects, such that short transects are in regions of high animal density and long transects are in regions of low animal density.\n\nlibrary(Distance)\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=\"cos\",\n                 convert_units=conversion.factor)\nprint(sysvar2.hn$dht$individuals$D)\n\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 2044.592 566.3958 0.2770214 1161.012 3600.614 20.74468\n\nprint(sysvar2.hn$dht$individuals$N)\n\n  Label Estimate       se        cv     lcl      ucl       df\n1 Total 1022.296 283.1979 0.2770214 580.506 1800.307 20.74468\n\n\nThe point estimates are good (\\(\\hat D = 2,044\\) animals per unit area and \\(\\hat N=1,022\\) - note the size of the area) but the precision obtained with the default estimator is poor: estimated abundance ranges from about 580 to 1,800 - a three-fold difference over which we are uncertain. Given that our survey covered 40% of the triangular region and had a good sample size (254 animals on 20 transects), this would be a disappointing result in practice.\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize, \n                    convert_units=conversion.factor, nboot=100)\n\n\nsummary(est.boot)\n\nBootstrap results\n\nBoostraps          : 100 \nSuccesses          : 100 \nFailures           : 0 \n\n     median   mean     se    lcl    ucl   cv\nNhat 985.58 981.06 254.52 550.31 1508.1 0.26\n\n\nThe bootstrap results are very similar to the analytical results, as we would expect, because again this process assumed the transects were placed at random.\n\n\nPost-stratification to improve variance estimation\n\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, \n               strat_formula=~1, convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k     ER se.ER cv.ER\n  Total  0.5      0.1922   9.61 254 20 26.431 1.459 0.055\n\nDensity estimates:\n .Label Estimate      se   cv      LCI      UCI     df\n  Total 2044.592 162.914 0.08 1744.988 2395.636 75.871\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     52.03 47.97\n\n\nThe precision of the estimated abundance has greatly improved in the post-stratified analysis (Fewster et al., 2009).\nIt must be remembered that we have not made any change to our data by the post-stratification; we are using getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be as extreme as they are in this example and post-stratification will not make a great difference. Such an situation is illustrated in the next problem.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThese data did not exhibit strong trends across the survey region and, hence, there are no great differences between the CVs and 95% confidence intervals using the two methods.\n\n# Access the data\ndata(\"Systematic_variance_1\")\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\nprint(sysvar1.hn$dht$individuals$D)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 1954.016 160.5554 0.08216688 1657.276 2303.888 50.59541\n\nprint(sysvar1.hn$dht$individuals$N)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 977.0078 80.27768 0.08216688 828.6378 1151.944 50.59541\n\nest2.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1,\n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est2.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total  0.5      0.2058  10.29 252 20 24.49 1.594 0.065\n\nDensity estimates:\n .Label Estimate      se    cv      LCI      UCI     df\n  Total 1954.015 162.491 0.083 1653.804 2308.723 49.172\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     38.76 61.24\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "01-intro/Pr1-instructions.html",
    "href": "01-intro/Pr1-instructions.html",
    "title": "Line transect detection function fitting",
    "section": "",
    "text": "In this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA (Anderson & Pospahala, 1970). Twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nTable 1. Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nObjectives\nThe aim of this exercise is to plot a histogram of the perpendicular distances to the detected duck nests and estimate (by eye) a detection function and hence estimate density of duck nests, i.e. the number of nests per square metre or per square kilometre (be careful of units).\n\n\nAnswer these questions in sequence\nThese steps will produce an estimate of duck nest density.\n\nWith the graph paper PDF found at this link, print the PDF and plot a histogram of the data in Table 1 and fit a detection function by eye.\nEstimate the areas under the rectangle and the fitted detection function curve and hence estimate the proportion of nests that are detected in the covered region, i.e. the region within 2.4m of the transect centre line.\n\n\\[ Area_{rectangle} = \\] \\[ Area_{curve} = \\]\n\\[ \\hat{P}_a = \\frac{Area_{curve}}{Area_{rectangle}} = \\]\n\nObtain an estimate of the number of nests in the covered region (Note \\(n=534\\)):\n\n\\[ \\hat{N}_a = \\frac{n}{\\hat{P}_a} = \\]\n\nEstimate density (Note \\(L = 20 \\times 128.75 = 2575\\) km):\n\n\\[\\hat{D} = \\frac{\\hat{N}_a}{a} = \\frac{\\hat{N}_a}{2wL} = \\]\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501"
  },
  {
    "objectID": "01-intro/Prac1_solution.html",
    "href": "01-intro/Prac1_solution.html",
    "title": "Line transect detection function fitting solution",
    "section": "",
    "text": "Solution\n\n\n\nEstimation of duck nest density by hand\n\n\nIn this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA: twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nTable 1. Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nHistogram of detected nests (black) overlaid with the estimated detection function (red) is shown below.\n\n\n\n\n\n\n\nTo estimate the area under the curve, I read off the heights of the mid points of my fitted curve (red) as follows: 75, 74, 72, 70, 66, 62, 58, 53. Therefore, my estimate of area under the curve is:\n\n\\[ Area_{curve} = (75+74+72+70+66+62+58+53) \\times 0.3 = 530 \\times 0.3 = 159 \\] There are lots of other ways to work out the area under a curve, e.g. counting the number of grid squares under the curve on your graph paper or using the trapezoidal rule.\n\\[Area_{rectangle} = height \\times width = 75 \\times 2.4 = 180\\]\nHence, my estimate of the proportion of nests detected in the covered region is:\n\\[\\hat P_a = \\frac{159}{180} = 0.883\\]\n\nHow many actual nests were there in the covered area? I saw 534 nests, and I estimate the proportion seen is 0.883, so my estimate of nests in the covered region is:\n\n\\[ \\hat N_a = \\frac{n}{\\hat P_a} =\\frac{534}{0.883} = 604.7 \\textrm{ nests in the covered area}\\] This estimate is for a covered area of \\(a = 2wL = 2 \\times (\\frac{2.4}{1000}) \\times 2575 = 12.36\\) km\\(^2\\).\n\nI therefore estimate nest density as:\n\n\\[\\hat D = \\frac{\\hat N_a}{2wL} = \\frac{604.7}{12.36} = 48.9 \\textrm{ nests per km}^2\\]"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "",
    "text": "Introduction to Distance Sampling with duck nest data"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#working-on-the-computer",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#working-on-the-computer",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Working on the computer",
    "text": "Working on the computer\n\nInstall the Distance software:\n\nDownload and install the Distance software from http://distancesampling.org/Distance/distance75download.html, if you have not already.\n\nAcquire the Distance project for this exercise:\n\nClick on this link to access data to be used in this exercise. You will be asked to save this file to your local computer. Remember the location where you saved the file, as you will be opening it in the next step.\n\nWe are going to work with a project called “Ducknest exercise”. It contains the same data you previously analysed by hand. Open the Distance software and select File followed by Open project. Under “Files of type” choose Zip archive files (*.zip).\n\n\n\n\n\n\n\n\nOther Distance projects\n\n\n\n\n\nThe Sample Projects folder created when you installed the Distance software, contains a number of other data sets, not related to this training workshop. You may want to look at those projects at a later date.\n\n\n\n\nDouble click on Ducknest exercise.zip. Click OK to unpack the project into the current directory and open it. Next time you open the project, you can open the file Ducknest exercise.dst directly."
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#examining-the-data",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#examining-the-data",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Examining the data",
    "text": "Examining the data\n\nClick on the Data tab of the Project Browser to show the Data Explorer. Look at the data structure, and in particular how the distance data have been entered. (You will need to click on Observation in the left hand pane of the Data Explorer to see this.)"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#studying-the-first-analysis",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#studying-the-first-analysis",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Studying the first analysis",
    "text": "Studying the first analysis\n\nClick on the Analysis tab of the Project Browser to show the Analysis Browser. You should see one analysis listed, called “Half-normal no adjustments.” Double-click on the grey status button for this analysis to open the Analysis Inputs tab for this analysis (you can do the same thing by clicking the 3rd button after “Analysis:” on the Analysis Browser menu bar, or by choosing Analyses then Analysis Details… from the menu bar at the top).\n\n\n\nA grey status icon indicates that this analysis has yet to be run. Click on the Run button in order to run the analysis. The Results tab should turn green.\nClick on the Results tab to see the results, and use the Next &gt; button to move through the pages of results, looking at each page and trying to relate the analysis given here to the one you did by hand. (Note: These are the analysis details (Inputs/Log/Results) for one analysis – you can resize this window so that you can view details from multiple analyses when you have more than one analysis to examine)."
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-solution-DistWin.html",
    "href": "02-detfns/DistWin-prac/Pr2-solution-DistWin.html",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "",
    "text": "Introduction to Distance Sampling with duck nest data\n\n\n\n\n\nDistance for Windows Solution\n\n\n\n\nWatch the solutions video, where we work through the exercise step by step.\nWhen you have the Ducknest project open in Distance and click on the Data tab, your screen should look like this:\n\n\n\nOnce you’ve run the analysis, and open the Analysis Details window for that analysis, your screen should look something like this (see below). Distance gets a density estimate of 49.697 nests km-2, similar to the value we obtained by hand of 48.9 nests km-2."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html",
    "title": "Analysis of point transect survey data",
    "section": "",
    "text": "Analysis of point transect survey data"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html",
    "title": "Variance estimation for systematic designs",
    "section": "",
    "text": "Variance estimation for systematic designs\nRecall the situation in which we have a strong gradient in animal density across our study region, and at the same time we also have a difference in the lengths of our transects; such that short transects are in areas of high animal density, and long transects are in areas of low animal density."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "title": "Variance estimation for systematic designs",
    "section": "Basic variance estimation, with bootstrapping",
    "text": "Basic variance estimation, with bootstrapping\n\nThe precision is very poor: estimated density ranges from about 1000 to 3600: a three-and-a half-fold difference over which we are uncertain. Given that our survey covered 40% of the triangle region, and had a good sample size (254 on 20 transects), this would be a very disappointing result in practice.\nBootstrap output [your results may differ slightly as these are created from a random process]:\n\nEstimate %CV \\# df 95% Confidence Interval\n--------------------------------------------------------\nHalf-normal/Cosine\nD 2129.2 27.40 999 20.74 1216.2 3727.5\n                         1164.0 3427.2\n\nHalf-normal/Cosine\nN 1064.6 27.40 999 20.74 608.00 1864.0\n                         582.00 1714.0\n\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals.\nInterval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\nThe bootstrap results are very similar to the analytic results, as we would expect. In fact, this did not used to be the case in previous versions of Distance, as the old analytic variance estimator did not perform well when there were extreme trends in both density and line length. You can access the previous default estimator under the Advanced… tab on the Variance page of the Model Definition Properties (its estimator R3), and more details are given in (Fewster et al., 2009).\nThe component percentages of variance are as follows:\n\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 4.3\nEncounter rate :       95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#post-stratification-to-improve-variance-estimation",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#post-stratification-to-improve-variance-estimation",
    "title": "Variance estimation for systematic designs",
    "section": "Post-stratification to improve variance estimation",
    "text": "Post-stratification to improve variance estimation\n\nThe precision is now greatly improved:\n\nEstimate %CV df 95% Confidence Interval\n-----------------------------------------------------\nHalf-normal/Cosine\n\nD 2044.6 8.64 31.41 1715.0 2437.5\nN 1022.0 8.64 31.41 858.00 1219.0\nand a much smaller and more reasonable (considering the sample size and survey coverage) proportion of the variation comes from estimating encounter rate:\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 44.3\nEncounter rate :        55.7\n\nThe CV is now even smaller, although it could have gone either way because this is an estimator of the same quantity as the last question–just a more precise estimator.\n\nEstimate %CV df 95% Confidence Interval\n-----------------------------------------------------\n\nHalf-normal/Cosine\n\nD 2044.6 7.97 75.87 1745.0 2395.6\nN 1022.0 7.97 75.87 872.00 1198.0\nThe encounter rate degrees of freedom are now 19 (number of lines – 1) rather than 10 (number of post-strata) for the previous question–which is why this is a more precise estimator of the variance.\nRemember we have not made any change to our data by the post-stratification; we are just getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be extreme as they are in this example, and post-stratification will not make a great difference. Such an example is shown below."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "title": "Variance estimation for systematic designs",
    "section": "Systematic designs where post-stratification is not needed",
    "text": "Systematic designs where post-stratification is not needed\n\nThe following simulated population does not exhibit strong trends across the survey region. Otherwise, the strip dimensions and systematic design are the same as for the previous example.\n\nWithout post-stratification: analytic output\nEstimate %CV df 95% Confidence Interval\n------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1954.0 8.22 50.60 1657.3 2303.9\nN 977.00 8.22 50.60 829.00 1152.0\nNote: Your bootstrap results will differ slightly, as bootstrapping is a random procedure.\nEstimate %CV \\# df 95% Confidence Interval\n--------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1947.4 10.03 999 50.60 1592.8 2380.8\n                         1565.0 2350.3\n\nHalf-normal/Cosine\n\nN 973.69 10.03 999 50.60 796.00 1190.0\n                         782.00 1175.0\n\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals.\nInterval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\n\nWith post-stratification (non-overlapping): analytic output\nEstimate %CV df 95% Confidence Interval\n------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1954.0 8.38 25.80 1645.4 2320.6\nN 977.00 8.38 25.80 823.00 1160.0"
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html",
    "href": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "",
    "text": "More detection function modelling and model criticism\nResults of estimating density from simulated data in which true density was 79.8 per km2. Findings from some candidate models:\nNot surprisingly for these data (simulated from a half normal detection function with a broad shoulder), the negative exponential model gives a higher estimate than the others, although the confidence interval still includes the true density. The other models provide very similar estimates, though precision is slightly worse for the hazard-rate model (because more parameters fitted). Agreement between the estimate and the known true density is less good if you do not truncate the data, or do not truncate them sufficiently."
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html#additional-question",
    "href": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html#additional-question",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "Additional question",
    "text": "Additional question\nThe capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results. Note the rounding in the distance data. This means that interval cutpoints for histograms and goodness-of-fit testing, and for the analysis of grouped data if this is required, should be chosen with care (i.e., distance bands ought to be sufficiently broad such that the ‘correct’ perpendicular distance is in the bands containing the rounded recorded value. e.g. 0, 7.5, 17.5, 27.5, … )\n\n\n\n\n\n\n\n\n\n\nFitted model\n\\(\\hat{D}\\)\n\\(\\hat{D}\\) LCB\n\\(\\hat{D}\\) UCB\n\\(\\hat{D}\\) CV\n\n\nHalf normal\n4.76\n4.01\n5.65\n0.09\n\n\nUniform cosine\n4.28\n3.22\n5.68\n0.14\n\n\nHazard rate\n4.20\n3.6\n4.9\n0.08\n\n\nHalf normal with grouped data\n4.52\n3.81\n5.36\n0.09\n\n\n\n\n\n\n\n\n\nNote regarding estimates in previous table\n\n\n\n\n\nI have reported the density estimates in the table above as numbers km-2, rather than numbers ha-1 to make them easier to read. There are 100 hectares in a square kilometer."
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html",
    "href": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "",
    "text": "More detection function modelling and model criticism\n1a). The line transect data below were generated from a half-normal detection function model. You may either enter the data by hand (if you want practice doing this) using instructions in the next bullet point, or skip to step 1b) to open a project containing the data to begin analysis.\nPerpendicular distances in metres generated from a half-normal line transect detection function model.\n1b) The full data set is in project Exercise3-2023.zip Download the file from this link and save the file in a location on your computer. Open Distance for Windows. Choose Open project and select zip file type to open the file and begin analysis."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html",
    "title": "Variance estimation for systematic designs",
    "section": "",
    "text": "Variance estimation for systematic designs\nIn the lecture describing measures of precision we explained systematic survey designs usually have the best variance properties, but obtaining good estimates of the variance is a difficult problem for statisticians. In this exercise we give an example of a situation where the systematic design gives a density estimate with much better precision than a random design.\nThis means that the usual variance estimators used in Distance, which are based on a random design, give variance estimates that are far too high. The true variance is low, but the estimated variance is high. We will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance.\nWe also look at another case to see that the unstratified variance estimates provided by Distance are usually fine for a systematic design: things only go wrong when there are very strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g., the highest densities always occur on the shortest lines, or vice versa).\nWe begin with the population and survey shown below. All the populations in this exercise are simulated on a computer: they are not real data. Note the characteristics: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design (search strips are shaded) covers a fairly large portion of the survey area. These are the danger signals that the usual Distance variance estimators might not work well, and a post-stratification scheme should be considered.\nThe survey region is a triangle, with dimensions 1km by 1km. The systematically placed search strips are shaded."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "title": "Variance estimation for systematic designs",
    "section": "Basic variance estimation, with bootstrapping",
    "text": "Basic variance estimation, with bootstrapping\n\nDownload and open the Distance project Systematic_variance_2.zip.\nOn the Analysis tab, click New Analysis. Rename it Without post-stratification.\nUnder Model definition, click Properties. Rename the new model: No_adjustments_plus_bootstrap.\nClick the tab for Detection function, and click Adjustment terms. Select Manual selection so that no adjustment terms are fitted. Select the Constraints button, and click No constraints. These options will reduce the work that has to be done during bootstrapping.\nClick the tab for Variance, and check the box for Bootstrap variance estimate: Select non-parametric bootstrap. The box Resample samples should be checked (this means resample transect lines). Leave the other settings at default, noting that there will be 999 bootstrap resamples conducted.\nClick OK and then run the model. You can see the progress of the bootstrap in the bar at the top. Wait a few moments until the bootstrapping is completed.\nYour analytic output should resemble this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\n%CV\ndf\nLower 95% Confidence bound\nUpper 95% Confidence bound\n\n\n\n\nD\n2044.6\n27.70\n20.74\n1161.0\n3600.6\n\n\nN\n1022.0\n27.70\n20.74\n581.0\n1800.0\n\n\n\n\nBecause we have simulated these data, we know the true values. The true number of animals in the survey region is N=1000, and the true density is D=2000 km-2 (1000 animals in an area of size A=0.5 km2). The point estimates are good, but what do you think about the precision in the output above?\nFind the bootstrapped confidence intervals for D and N, and check whether they are similar to the confidence intervals above.\nWhat percentage of the total density variance is attributed to encounter rate estimation and what percentage to the detection function estimation?\n\n\nVariance estimation for systematic designs using post-stratification\nRecall we have a particular situation in which we have systematically placed transects, unequal in length. Furthermore there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study area with the highest density. We examine a means by which we can use post-stratification to produce a better estimate of the variance in estimated density."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#post-stratification-to-improve-variance-estimation",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#post-stratification-to-improve-variance-estimation",
    "title": "Variance estimation for systematic designs",
    "section": "Post-stratification to improve variance estimation",
    "text": "Post-stratification to improve variance estimation\nThe estimation of encounter rate variance above used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic. In some circumstances, this can reduce the encounter rate variance a great deal.\nThe data we are working with is an example of this. There are very high densities on the very shortest lines. In samples of lines collected using a completely random design, the sample by chance might not contain any of these very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance.\nAlthough there is no way of getting a variance estimate that is exactly unbiased for a systematic sample1, we can greatly improve on the random-based estimate by using a post-stratification scheme. This works by grouping together pairs of adjacent lines from the systematic sample. Each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample.\nFollow the steps below.\n\nOpen the Distance project we used in the previous section (Systematic_variance_2.dst; is has the “.dst” extension because you uncompressed it minutes ago).\nClick the Analyses tab, and click the “New analysis” button to create a new analysis. Double click the grey ball and the Analysis Details Window should come up. Name the new analysis something like With post stratification.\nUnder Model Definition, click New. Change the name at the bottom of the dialogue box to Poststratified_no_adjustments_no_bootstrap. (We don’t want to conduct a bootstrap for our poststratified data, because it would involve some extra confusion and is not necessary.) In the Variance tab, click Advanced..., and select the option “Post-stratify, grouping adjacent pairs of samplers”. Un-tick the option “Select non-parametric bootstrap”.\nClick OK and then Run to run the analysis. How does the variance and confidence limits compare with those you obtained in the previous section? What are the implications? Note what percentage of the overall variance now comes from encounter rate and from estimating the detection function, and compare this with the earlier percentages.\nNow try the overlapping post-stratification option. A simulation study in (Fewster et al., 2009) concluded that its performance was very similar to, but marginally better than the regular post-stratification. When the sample size of lines is small, it gives more post-strata and so is to be preferred for that reason. Create a new analysis, called say With overlapping post stratification, and then a new Model Definition for that analysis, in which you choose the Advanced variance option “Post-stratify, with overlapping strata made up of adjacent samplers”. How does the variance compare with those you previous obtained? How do the degrees of freedom in the Estimation Summary – Encounter Rate page of output compare with that from the previous question?\n(Optional) If you wish, you can try manual post-stratification. This is good practice if you need to do post-stratification for point transect studies. In this case you will have to add a new field to the sample layer, and then set up a new model definition in which you tell Distance to use post-stratification. Here goes:\n\nClick the Data tab. Click the padlock button on the toolbar to unlock the data sheet for modification.\nOn the left-hand outline, click Line transect. The data sheet expands to 20 rows, each row corresponding to one line transect. This is the best format for the data sheet to be in when entering a new stratum number for each transect.\nClick on the cell corresponding to Line transect Label 1. Several buttons on the tool-bar should become live. Click on the button corresponding to Append field after current. (The button has an arrow pointing sideways then downwards.)\nYou are prompted for Field name: enter VarGroup to indicate that you are grouping lines together for the purpose of variance estimation. Click Field type: Integer, and click OK.\nYou can now enter the line groupings for post-stratified variance estimation. Enter label 1 for lines 1 and 2; label 2 for lines 3 and 4; label 3 for lines 5 and 6; and so on, to finish with label 10 for lines 19 and 20. You have now defined 10 strata, each containing two adjacent transect lines from the systematic sample of lines.\nAfter entering the column of VarGroup labels, click the padlock button again to lock the data sheet.\nNow we will analyse the post-stratified data. Click the Analyses tab. Create a new analysis with a suitable name - .e.g, Manual post stratification\nCreate a new Model definition, with a suitable name. In the Estimate tab, click the button for Poststratify. Select Layer type: sample, and Field name: VarGroup. This means that we want to poststratify at the sample (transect) level, using our newly defined groupings VarGroup to delimit the strata.\nFor the levels of resolution, select the following:\n\n\n\nDensity: Global and Stratum\nEncounter Rate: Stratum only\nDetection function: Global only\nCluster size (not required): Global only\n\n\nThese settings ensure that it is only encounter rate variance that is affected by the post-stratification scheme; the detection function is still pooled over all observations as before.\n\n\nIn the next field, enter Global density estimate is Mean of stratum estimates, and in the next field select Weighted by Total effort in stratum. Do not tick the box saying Strata are Replicates.\nClick OK and run the new model. The point estimates should be the same as the previous non-overlapping post stratification run.\n\n\nNote: The precision of D and N are greatly improved in the post-stratified analyses. Note that we are not getting something for nothing: the second analysis is giving us an answer much closer to the true answer, while the first analysis was simply giving us the wrong answer. We have not changed the true variance by our post-stratification scheme: we are just getting a better estimate of the true variance. Because the data above were generated by simulation, we can use repeated simulated surveys to check that the second answer is indeed close to the true density variance over the repeats."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "title": "Variance estimation for systematic designs",
    "section": "Systematic designs where post-stratification is not needed",
    "text": "Systematic designs where post-stratification is not needed\nThe following simulated population does not exhibit strong trends across the survey region. Otherwise, the strip dimensions and systematic design are the same as for the previous example.\n\nDownload and open the project Systematic_variance_1.zip. Add the new data column VarGroup before conducting any analyses this time. With the augmented data, repeat the analyses you performed on the Systematic_variance_2.zip project. Find the relevant outputs. Has the post-stratification scheme been necessary in this case?"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#footnotes",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#footnotes",
    "title": "Variance estimation for systematic designs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nbecause it is effectively a sample of size 1 – only the first line position was randomly chosen, and the rest followed on deterministically from there.↩︎"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html",
    "title": "Analysis of point transect survey data",
    "section": "",
    "text": "Analysis of point transect survey data"
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html#additional-question-with-another-line-transect-data-set",
    "href": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html#additional-question-with-another-line-transect-data-set",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "Additional question with another line transect data set",
    "text": "Additional question with another line transect data set\n\n2. Below are perpendicular distance data (m) from line transect surveys of capercaillie (a large grouse) in Scotland. Total line length was 240km. The data are also in a text file capercaillie.txt you may download from this link and save on your computer. Description of a line transect survey of this species is presented in (Catt et al., 1998)\nIn the text file, column 1 is the transect number, column 2 is the transect length and column 3 is perpendicular distance as shown.\n\n\n  V1  V2 V3\n1  1 240 28\n2  1 240 17\n3  1 240 15\n4  1 240 14\n5  1 240 18\n6  1 240  0\n\n\nColumns are separated by tab characters. Create a new Distance project and either enter the data by hand or use the Data Import Wizard (Tools &gt; Import Data Wizard) to import the data from the text file. Then decide on a suitable model for the detection function and estimate bird density.\nCapercaillie, Monaughty Forest 112 detections\n28.0 17.0 15.0 14.0 18.0 0.0 38.0 6.0 50.0 65.0\n75.0 1.0 70.0 28.0 40.0 40.0 40.0 15.0 40.0 30.0\n5.0 55.0 60.0 40.0 24.0 30.0 0.0 50.0 55.0 10.0\n40.0 10.0 30.0 34.0 24.0 30.0 15.0 20.0 14.0 48.0\n0.0 30.0 2.0 52.0 11.0 48.0 28.0 38.0 25.0 35.0\n45.0 0.0 16.0 12.0 2.0 14.0 12.0 24.0 70.0 50.0\n49.0 40.0 80.0 18.0 27.0 30.0 30.0 60.0 58.0 14.0\n0.0 56.0 40.0 19.0 21.0 0.0 38.0 20.0 28.0 30.0\n20.0 16.0 0.0 69.0 40.0 46.0 50.0 40.0 70.0 67.0\n28.0 12.0 12.0 22.0 40.0 48.0 48.0 15.0 12.0 0.0\n15.0 20.0 17.0 30.0 30.0 32.0 48.0 20.0 10.0 20.0\n42.0 30.0"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard",
    "title": "Variance estimation for systematic designs",
    "section": "Component Percentages of Var(D)",
    "text": "Component Percentages of Var(D)\nDetection probability : 4.3 Encounter rate : 95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard-1",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard-1",
    "title": "Variance estimation for systematic designs",
    "section": "Component Percentages of Var(D)",
    "text": "Component Percentages of Var(D)\nDetection probability : 44.3 Encounter rate : 55.7\n\nThe CV is now even smaller, although it could have gone either way because this is an estimator of the same quantity as the last question–just a more precise estimator.\n\n\nEstimate %CV df 95% Confidence Interval\n\nHalf-normal/Cosine\nD 2044.6 7.97 75.87 1745.0 2395.6\nN 1022.0 7.97 75.87 872.00 1198.0\nThe encounter rate degrees of freedom are now 19 (number of lines – 1) rather than 10 (number of post-strata) for the previous question–which is why this is a more precise estimator of the variance.\n\nRemember we have not made any change to our data by the post-stratification; we are just getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be extreme as they are in this example, and post-stratification will not make a great difference. Such an example is shown below."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#estimate-cv-df-95-confidence-interval",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#estimate-cv-df-95-confidence-interval",
    "title": "Variance estimation for systematic designs",
    "section": "Estimate %CV # df 95% Confidence Interval",
    "text": "Estimate %CV # df 95% Confidence Interval\nHalf-normal/Cosine D 2129.2 27.40 999 20.74 1216.2 3727.5 1164.0 3427.2\nHalf-normal/Cosine N 1064.6 27.40 999 20.74 608.00 1864.0 582.00 1714.0\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals. Interval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\nThe bootstrap results are very similar to the analytic results, as we would expect. In fact, this did not used to be the case in previous versions of Distance, as the old analytic variance estimator did not perform well when there were extreme trends in both density and line length. You can access the previous default estimator under the Advanced… tab on the Variance page of the Model Definition Properties (its estimator R3), and more details are given in (Fewster et al., 2009).\nThe component percentages of variance are as follows:\n\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 4.3\nEncounter rate :       95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#simulated-point-transect-data",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#simulated-point-transect-data",
    "title": "Analysis of point transect survey data",
    "section": "Simulated point transect data",
    "text": "Simulated point transect data\n\nSimulated point transect data from 30 points are given in project PTExercise1.zip. These data were generated from a half-normal detection function, and true density was 79.6 animals ha-1.\n\nDownload and analyse this data set. Experiment with keys other than the half-normal (uniform, hazard-rate and negative exponential), to assess whether these data can be satisfactorily analysed using the wrong model. For each key, determine a suitable truncation point, and decide on whether, and which, adjustments are needed. (Truncation points come under the data filter.) How do bias and precision compare between models?"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#winter-wrens-in-scotland",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#winter-wrens-in-scotland",
    "title": "Analysis of point transect survey data",
    "section": "Winter wrens in Scotland",
    "text": "Winter wrens in Scotland\n\nThe following projects are available for download and analysis:\n\n\nWren1.zip 5min count,\nWren2.zip snapshot,\nWren3.zip cue count and\nWren4.zip line transect.\n\nThe projects contain winter wren data, collected at Montrave, Scotland in 2004, as described in (Buckland, 2006). Each project corresponds to a different method of data collection. Thirty-two points were defined through 33.2 ha of parkland (Fig. 1), and detection distances were measured in meters with the aid of a laser rangefinder. Three types of point transect data were collected:\n\nstandard five-minute counts\nthe ‘snapshot’ method and\na cue count method.\n\nIn addition, line transect data were collected (method 4), and territory mapping was conducted, which gave an estimate of 43 wren territories (1.30 territories ha-1).\n\nSelect a single model for exploratory data analysis. Experiment with different truncation distances w, and select a suitable value for each method. Do you see potential problems with any of the data sets?\nTry other models and other model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density for each method. Record also the corresponding confidence intervals."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#savannah-sparrows-in-colorado-usa",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#savannah-sparrows-in-colorado-usa",
    "title": "Analysis of point transect survey data",
    "section": "Savannah sparrows in Colorado USA",
    "text": "Savannah sparrows in Colorado USA\n\nTwo additional point transect projects are available for download and analysis:\n\n\nSavannah Sparrow 1980.zip and\nSavannah Sparrow 1981.zip.\n\nThese were part of a large data set collected in Arapaho National Wildlife Refuge, Colorado. For both data sets, consider an appropriate truncation distance, decide on a suitable model for the detection function, and estimate density, both for each stratum individually and for the whole study area. You should include in your analysis an assessment of whether the detection function can be estimated from data pooled across strata, or whether separate estimates are needed per stratum. (This will be covered in the lecture discussing stratification if you don’t already know how to do it.)"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#simulated-point-transect-data",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#simulated-point-transect-data",
    "title": "Analysis of point transect survey data",
    "section": "Simulated point transect data",
    "text": "Simulated point transect data\n\nResults from selected model options; remember these are simulated data with a half normal detection function and true density 79.6:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nAdjustment\nterms\nw(m)\n\\(\\hat{D}\\)\n%CV\nLCB\nUCB\n\n\n\n\nHalf-normal\nNone\n0\n34.2\n79.6\n12.6\n62.1\n102.1\n\n\nHalf-normal\nNone\n0\n20\n70.8\n15.7\n52\n96.5\n\n\nUniform\nCosine\n1\n20\n75\n14.4\n56.5\n99.6\n\n\nHazard-rate\nNone\n0\n20\n62.4\n18.7\n43.2\n90\n\n\nNeg Exp\nSimple poly\n1\n20\n73.1\n29.2\n41.5\n128.6\n\n\n\nWe see a fair degree of variability between analyses–reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates. For these data, the hazard-rate model appears to have downward bias, and precision is very poor for the negative exponential model."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#winter-wren-data-analysis-with-bonus-species-included",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#winter-wren-data-analysis-with-bonus-species-included",
    "title": "Analysis of point transect survey data",
    "section": "Winter wren data analysis (with bonus species included)",
    "text": "Winter wren data analysis (with bonus species included)\n\nOur analyses of the winter wren data produced the following estimated densities (territories ha-1). Three other species were surveyed in the field. Estimates of density for those species are in the table below, although the Distance for Windows projects are not included. Method 5 is territory mapping (which does not use distance sampling, and as you note has no measure of precision associated because it is akin to a census method).\n\n\n\n\nMethod\nCommon Chaffinch\nGreat Tit\nEuropean Robin\nWinter Wren\n\n\n\n\n1\n1.03\n0.58\n0.52\n1.29\n\n\nCI\n(0.74,1.43)\n(0.36,0.94)\n(0.26,1.06)\n(0.80,2.11)\n\n\n2\n0.90\n0.22\n0.60\n1.02\n\n\nCI\n(0.62,1.29)\n(0.13,0.39)\n(0.38,0.94)\n(0.80,1.32)\n\n\n3\n0.71\n0.26\n0.82\n1.21\n\n\nCI\n(0.45,1.23)\n(0.09,0.76)\n(0.52,1.31)\n(0.82,1.79)\n\n\n4\n0.64\n0.26\n0.69\n1.07\n\n\nCI\n(0.46,0.90)\n(0.16,0.42)\n(0.47,1.00)\n(0.87,1.31)\n\n\n5\n0.75\n0.21\n0.84\n1.30\n\n\n\nTo obtain the above estimates, I used a truncation distance of 110m for methods 1 and 2, 92.5m for method 3, and 95m for method 4. For the wren data, I used the uniform key with two cosine adjustments for method 1, the hazard-rate model for methods 2 and 3, and the half-normal key with two Hermite polynomial adjustments for method 4.\nPoints to note about the wren data: the wren more than any of the other species showed evidence of observer avoidance. This didn’t cause too many difficulties, except that the model favoured by AIC for line transect sampling was the hazard-rate model, which had a very flat shoulder out to around 75m. It was implausible that detection was certain out to this distance, so that I selected a model with a slightly inferior AIC value, but with a more plausible fitted detection function. Analyses of the cue count data are necessarily rather subjective, as the data show substantial over-dispersion (a single bird may give many songbursts all from the same location during a five-minute count). In this circumstance, goodness-of-fit tests are very misleading, and care must be taken not to overfit the data."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#savannah-sparrow-analysis",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#savannah-sparrow-analysis",
    "title": "Analysis of point transect survey data",
    "section": "Savannah sparrow analysis",
    "text": "Savannah sparrow analysis\n\nGood fits to the 1980 Savannah sparrow data were obtained by truncating at 55m. The half-normal detection function without adjustments fitted well, as did the uniform with cosine adjustments. The hazard-rate model performed less well. There was a marginal preference for fitting the detection function separately in each stratum as judged by AIC, but pooling distance data across strata might offer rather more robust estimation. The estimates of density in the table correspond to a half-normal detection function, fitted separately in each stratum, with a truncation distance of 55m.\n\nFor 1981, w=55m was again satisfactory. There was now a clear preference for estimating the detection function separately by stratum, but little to choose between the half-normal model and the uniform model with cosine adjustments. For comparability with 1980, I chose the half-normal model, although AIC showed a very marginal preference for uniform + cosine. (Again, the hazard-rate model provided less good fits overall.)\nEstimated densities \\(\\hat{D}\\) (birds/ha) of Savannah sparrows\n\n\n\nYear\nPasture\n\\(\\hat{D}\\)\nLCB\nUCB\n\n\n\n\n1980\n1\n1.43\n0.94\n2.18\n\n\n\n2\n4.12\n3.15\n5.38\n\n\n\n3\n2.35\n1.72\n3.2\n\n\n\nAll\n2.63\n2.19\n3.16\n\n\n\n\n\n\n\n\n\n1981\n0\n1.39\n0.82\n2.36\n\n\n\n1\n0.52\n0.27\n1.03\n\n\n\n2\n1.7\n1.07\n2.71\n\n\n\n3\n1.35\n0.81\n2.26\n\n\n\nAll\n1.24\n0.95\n1.62"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html",
    "href": "06-design/R-prac/Pr6-instructions.html",
    "title": "Distance sampling survey design",
    "section": "",
    "text": "We provide two exercises in survey design so you can choose the one you feel is most useful to you.\n\nThe first example involves designing a line transect survey to estimate the abundance of porpoise, common dolphins and seals in and around St Andrews Bay.\n\nIt considers how you choose your design based on effort limitations.\nIt also compares an aerial survey based on systematic parallel lines with a boat based survey using zigzags.\n\nThe second example involves designing a point transect bird survey in Tentsmuir Forest.\n\nThis looks at how to project your study area from latitude and longitude on to a flat plane using R.\nIt also involves defining a design for multiple strata with different coverage in each strata."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#study-region",
    "href": "06-design/R-prac/Pr6-instructions.html#study-region",
    "title": "Distance sampling survey design",
    "section": "Study Region",
    "text": "Study Region\nSet up the study region and plot it. The shapefile for this study area is contained within the dssd R library. This shapefile has already been projected from latitude and longitude on to a flat plane and its units are in metres. The first line of code below returns the path for the shapefile within the R library and may vary on different computers. You will then pass this shapefile pathway to the make.region function to set up the survey region. As this shapefile does not have a projection (.prj) file associated with it we should tell dssd the units (m) when we create the survey region.\n\n#Find the pathway to the file\nshapefile.name &lt;- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\n#Create the region using this shapefile\nregion &lt;- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\n#Plot the region\nplot(region)\n\n\n\n\nStudy Region"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#coverage",
    "href": "06-design/R-prac/Pr6-instructions.html#coverage",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nThe next step is to set up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover &lt;- make.coverage(region, n.grid.points = 500)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#systematic-parallel-design",
    "href": "06-design/R-prac/Pr6-instructions.html#systematic-parallel-design",
    "title": "Distance sampling survey design",
    "section": "Systematic Parallel Design",
    "text": "Systematic Parallel Design\nThe small survey plane available can complete a total flight time of around 250km (excluding the flight time to and from the landing strip at Fife Ness). Generally, systematic parallel line designs are preferable for aerial surveys as they allow some rest time for observers as the plane travels between transects and avoids the sharp turns associated with zigzag designs.\nFirstly, we will consider the design angle. Often animal density is affected by distance to coast so it is probably wise for this survey to orientate lines approximately perpendicular to the coast. To do this we can select a design angle of 90 degrees. We can therefore expect to spend a little more than 40 km (the height of the survey region) on off-effort transit time and might hope to be able to complete around 200km of transects. dssd lets us specify the desired line length as a design parameter and will then choose an appropriate value for transect spacing. We will choose a minus sampling strategy and set the truncation distance to 2km. Note that as our survey region coordinates are in metres we also need to supply the design parameters in metres.\n\n# Define the design\ndesign.LL200 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      line.length = 200000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nNow we have defined the design we should check it visually by creating a survey (a single set of transects).\n\n# Create a single survey from the design\nsurvey.LL200 &lt;- generate.transects(design.LL200)\n# Plot the region and the survey\nplot(region, survey.LL200)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nThe survey consists of parallel systematically spaced transects running horizontally across the survey region roughly perpendicular to the coast as we wanted. We can also view the details of the survey which will tell us what spacing dssd used to try and achieve a line length of 200 km.\n\n# Display the survey details\nsurvey.LL200\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced parallel transects\nSpacing:  4937.5\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nDesign angle:  90\nEdge protocol:  minus\nCovered area:  779476845\nStrata coverage: 78.93%\nStrata area:  987500079\n\n   Study Area Totals:\n   _________________\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nCovered area:  779476845\nAverage coverage: 78.93%\n\n\nThe spacing used by dssd was 4938m which gives us 8 samplers and a coverage of just under 80%. In addition, this example survey has a line length of just under 200km and a trackline length of just over 248 km. However, given the random nature of the design and the fact that the width of the study region is not constant everybody should get slightly different values. Although the trackline length was just under 250km it is not sufficient to only look at one survey, we need to know that all surveys under this design will have a trackline length of &lt; 250km.\nTo assess the design statistics across many surveys we will now run a coverage simulation. This simulation will randomly generate many surveys from our design and record coverage as well as various statistics including line length and trackline length. As we know that coverage for a parallel line design is largely uniform (apart from edge effects due to minus sampling) we do not need to run too many repetitions, 100 should be sufficient to give us an indication of the range of line lengths and trackline lengths for this design.\n\n# Run the coverage simulation\ndesign.LL200 &lt;- run.coverage(design.LL200, reps = 100)\ndesign.LL200\n\nExamine the design statistics. The mean line length should be around 200km (200,000 m). Now look at the maximum trackline length, we need this value to be less than 250km (250,000 m).\nUse the results of this simulation to create some new designs based on various spacings to find the maximum line length that can be achieved without risking exceeding the maximum trackline length of 250km (remember to generate a line length of 200km dssd selected a spacing of 4938m). Try transect spacings of 5000m or 5500m.\n\n# Define the design\ndesign.space500 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat spacing would you select for this design?\nWhat is the maximum trackline length for the design you have selected?\nWhat on-effort line length are we likely to achieve?"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#zigzag-design",
    "href": "06-design/R-prac/Pr6-instructions.html#zigzag-design",
    "title": "Distance sampling survey design",
    "section": "Zigzag Design",
    "text": "Zigzag Design\nZigzag designs are often more efficient in their use of effort having less off-effort transit time between transects. For this survey another option would be to complete a boat-based survey. The boat survey will have the same total effort available allowing us a trackline length of 250km.\nDefine a zigzag design for the same region. For zigzag designs the design angle has a different definition, it describes the angle across which the zigzags are constructed. Set the vertical design angle to 0. Zigzag designs also require an additional argument as zigzags can only be created inside convex shapes. Specify the bounding shape, choose a convex hull as it is more efficient than a minimum bounding rectangle. A convex hull works as if we were stretching an elastic band around the survey region. The code below shows you how to create the zigzag design, you should then create a single realisation of this design and plot it to check it looks acceptable.\n\n# Define the zigzag design\ndesign.zz.4500 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nsurvey.zz &lt;- generate.transects(design.zz.4500)\nplot(region, survey.zz)\n\n\n\n\nSingle survey generated from the equalspaced zigzag design\n\n\n\n\nRun a coverage simulation to verify that we have stayed within the restraints of our survey effort; a total trackline length of &lt; 250km. Run the coverage simulation with more repetitions to also assess the coverage.\nExamine the design statistics.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDoes this design meet our survey effort constraint?\nWhat is the maximum total trackline length for this design?\nWhat line length are we likely to achieve with this design?\nIs this higher or lower than the systematic parallel design?\n\n\n\n\n# Run the coverage simulation\ndesign.zz.4500 &lt;- run.coverage(design.zz.4500, reps = 500)\n# Display the design statistics\ndesign.zz.4500\n\nExamine the coverage. Sometimes with zigzag surveys generated inside convex hulls produce areas of higher coverage in narrower parts of the survey region at either end of the design axis. One of the easiest ways to assess coverage is visually by plotting the coverage grid.\n\n# Plot the coverage grid\nplot(design.zz.4500)\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDo you think the coverage scores look uniform across the study region?\nGiven the lack of uniformity, where are they higher/lower?\nWhy do you think this is?\n\nNote, revisit one of the parallel line designs and plot the coverage scores to compare (although there are fewer repetitions you can still get an idea of coverage)."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#projecting-your-study-region",
    "href": "06-design/R-prac/Pr6-instructions.html#projecting-your-study-region",
    "title": "Distance sampling survey design",
    "section": "Projecting your Study Region",
    "text": "Projecting your Study Region\nThis exercise demonstrates how to deal with unprojected shapefiles. Study areas should always be projected onto a flat plane before you use them to design your survey. This is because in most parts of the world one degree latitude is not the same in distance as one degree longitude. If we didn’t project, our study region and any surveys generated in it, would be distorted possibly leading to non-uniform coverage.\nLoad the study region and project it onto a flat plane using an Albers Equal Area Conical projection. As we have to project the shapefile we load the shape object separately instead of directly into a region object.\n\n#Load the unprojected shapefile\nshapefile.name &lt;- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape &lt;- read_sf(shapefile.name)\n# Check current coordinate reference system\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n# Define a European Albers Equal Area projection\nproj4string &lt;- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\n# Project the study area on to a flat plane\nprojected.shape &lt;- st_transform(sf.shape, crs = proj4string)\n\nCreate the region object for dssd using the projected shape and plot it to check what it looks like.\n\n# Create the survey region in dssd\nregion.tm &lt;- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n# Plot the survey region\nplot(region.tm)\n\n\n\n\nTentsmuir Forest: showing the main stratumand the Morton Loch stratum."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#coverage-1",
    "href": "06-design/R-prac/Pr6-instructions.html#coverage-1",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nSet up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover.tm &lt;- make.coverage(region.tm, n.grid.points = 1000)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#design",
    "href": "06-design/R-prac/Pr6-instructions.html#design",
    "title": "Distance sampling survey design",
    "section": "Design",
    "text": "Design\nSet up a systematic point transect design. We will assume that we have sufficient resources to survey 40 point transects. As the Morton Lochs stratum is of special interest we will give it higher coverage. We will therefore explicitly allocate 25 samplers to the main stratum and 15 to the Morton Lochs stratum (note that the area of the Morton Lochs stratum is much small than the main stratum). If we wanted to allocate the same effort to both stratum we could provide the samplers argument with the single value of 40 and it would divide the effort equally between the strata. We will leave the design angle as 0 and set the truncation distance to 100 m. We will use a minus sampling approach at the edges.\n\n\n\n\n\n\nQuestion:\n\n\n\n\nWhat are the analysis implications of a design with unequal coverage?\n\n\n\n\n# Set up a multi strata systematic point transect design\ndesign.tm &lt;- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#generate-a-survey",
    "href": "06-design/R-prac/Pr6-instructions.html#generate-a-survey",
    "title": "Distance sampling survey design",
    "section": "Generate a Survey",
    "text": "Generate a Survey\nYou will now generate a single survey from this design and plot it inside the survey region to check what it looks like. If you want to check whether the covered areas of the samplers in the Morton Lochs stratum overlap add the argument covered.area = TRUE to the plot function.\n\n# Create a single survey from the design\nsurvey.tm &lt;- generate.transects(design.tm)\n# Plot the region and the survey\nplot(region.tm, survey.tm, covered.area = TRUE)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nExamine the survey information.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers?\nDid your survey achieve exactly the number of samplers you requested?\nHow much does coverage differ between the two strata for this realisation?\n\n\n\n\n# Display survey information\nsurvey.tm\n\n\nSave coordinates to a file\nIf this survey is to be conducted in the field, you will want the coordinates that you can load into a handheld GPS. The function write.transects() can write waypoints of the survey (in this case the point transect stations) to text, comma-separated value or GPX files.\n\nwrite.transects(survey.tm,\n                dsn = \"tentsmuir-points.gpx\",\n                layer = \"points\",\n                dataset.options = \"GPX_USE_EXTENSIONS=yes\",\n                proj4string=sf::st_crs(sf.shape))\n\nThe GPX file can be transferred to a GPS, or viewed using Google Earth.\n\n\n\n\n\nRealised survey with locations written to GPX file and imported into Google Earth."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "href": "06-design/R-prac/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "title": "Distance sampling survey design",
    "section": "Assessing Coverage and Design Statistics",
    "text": "Assessing Coverage and Design Statistics\nWe will now run a coverage simulation to assess how much the number of samplers and average coverage varies between surveys. We will also be able to assess how coverage varies spatially to see if edge effects are of concern.\nView the design statistics.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is the minimum number of samplers you will achieve in each strata?\nIs this sufficient to complete separate analyses in each stratum?\n\n\n\nPlot the coverage scores.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDoes it appear that there is even coverage within each strata?\n\nAs there is such a difference in the range of coverage scores between strata you may need to plot each strata individually.\n\n\n\n\n\n# View the design statistics\nprint(design.tm)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  100\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum      22.0         13.0  36.0\nMean         25.1         15.1  40.2\nMedian       25.0         15.0  40.0\nMaximum      27.0         18.0  44.0\nsd            0.9          1.1   1.3\n\n    Covered area:\n    \n        Main Area Morton Lochs      Total\nMinimum 683662.63    363258.99 1076099.58\nMean    768186.34    420283.45 1188469.79\nMedian  772479.34    417730.22 1188447.46\nMaximum 816669.52    468461.70 1264365.86\nsd       25071.81     25394.87   32875.01\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.85        50.79  7.26\nMean         5.44        58.76  8.02\nMedian       5.48        58.40  8.02\nMaximum      5.79        65.49  8.53\nsd           0.18         3.55  0.22\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00000000    0.2200000 0.00000000\nMean    0.05483087    0.5750000 0.08094378\nMedian  0.05000000    0.6450000 0.06000000\nMaximum 0.15000000    0.7800000 0.78000000\nsd      0.02997747    0.1456197 0.12170445\n\n# Plot the coverage scores\nplot(design.tm)\n\n\n\n# coverage scores for individual strata could be plotted separately\n# plot(design.tm, strata.id = 1)\n# plot(design.tm, strata.id = 2)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html",
    "href": "06-design/R-prac/Pr6-solution.html",
    "title": "Distance sampling survey design solution",
    "section": "",
    "text": "Solution\n\n\n\nDistance sampling survey design\nlibrary(dssd)\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE"
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html#systematic-parallel-line-design",
    "href": "06-design/R-prac/Pr6-solution.html#systematic-parallel-line-design",
    "title": "Distance sampling survey design solution",
    "section": "Systematic Parallel Line Design",
    "text": "Systematic Parallel Line Design\n\n\n\n\n\n\nAnswers\n\n\n\nWhat spacing would you select for this design? What is the maximum trackline length for the design you have selected? What on-effort line length are we likely to achieve?\n\nThe spacing chosen by dssd of 4937.5m to generate a line length of 200km resulted in a maximum trackline length of around 261km (each exact answer will vary due to the random generate of surveys). If we choose this design then it is possible that when we randomly generate our survey we may not be able to complete it with the effort we have available.\n\n\nWe should therefore increase the spacing between the transects and re-run the coverage simulations. A spacing of 5000m gave a maximum trackline length of around 249km (see summary table of Trackline length in the output below) so we can be fairly confident that we will be able to complete any survey which we randomly generate from this design. This spacing should allow us to achieve an on-effort line length of 199km (see Line length section of design summary below). The minimum line length we would expect to achieve is 184km and the maximum is 206km. [Note your values might differ to those below]\n\n\n\n\nshapefile.name &lt;- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\nregion.sab &lt;- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\ncover.sabay &lt;- make.coverage(region.sab, n.grid.points = 5000)\ndesign.spacing5km &lt;- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.spacing5km &lt;- run.coverage(design.spacing5km, reps = 250, quiet=TRUE)\nplot(design.spacing5km)\n\n\n\n\nCoverage grid plot for parallel design of St Andrews Bay.\n\n\n\n\n\nprint(design.spacing5km)\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced transects\nSpacing:  5000\nNumber of samplers:  NA\nLine length: NA\nDesign angle:  90\nEdge protocol:  minus\n\nStrata areas:  987500079\nRegion and effort units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        St Andrews Bay Total\nMinimum            7.0   7.0\nMean               7.9   7.9\nMedian             8.0   8.0\nMaximum            8.0   8.0\nsd                 0.3   0.3\n\n    Covered area:\n    \n        St Andrews Bay     Total\nMinimum      725958409 725958409\nMean         761888043 761888043\nMedian       767391438 767391438\nMaximum      778634413 778634413\nsd            15651735  15651735\n\n    % of region covered:\n    \n        St Andrews Bay Total\nMinimum          73.51 73.51\nMean             77.15 77.15\nMedian           77.71 77.71\nMaximum          78.85 78.85\nsd                1.58  1.58\n\n    Line length:\n    \n        St Andrews Bay     Total\nMinimum      184446.63 184446.63\nMean         197888.25 197888.25\nMedian       199119.48 199119.48\nMaximum      205774.00 205774.00\nsd             5917.77   5917.77\n\n    Trackline length:\n    \n        St Andrews Bay     Total\nMinimum      220534.33 220534.33\nMean         242554.64 242554.64\nMedian       246237.52 246237.52\nMaximum      248778.16 248778.16\nsd             8224.54   8224.54\n\n    Cyclic trackline length:\n    \n        St Andrews Bay     Total\nMinimum      251950.06 251950.06\nMean         279355.02 279355.02\nMedian       283828.05 283828.05\nMaximum      285964.52 285964.52\nsd             9626.55   9626.55\n\n    Coverage Score Summary:\n    \n        St Andrews Bay      Total\nMinimum     0.35600000 0.35600000\nMean        0.77156543 0.77156543\nMedian      0.78800000 0.78800000\nMaximum     0.85200000 0.85200000\nsd          0.08596836 0.08596836"
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html#equal-spaced-zigzag-design",
    "href": "06-design/R-prac/Pr6-solution.html#equal-spaced-zigzag-design",
    "title": "Distance sampling survey design solution",
    "section": "Equal Spaced Zigzag Design",
    "text": "Equal Spaced Zigzag Design\n\n\n\n\n\n\nAnswers\n\n\n\nDoes this design meet our survey effort constraint? What is the maximum total trackline length for this design? What line length are we likely to achieve with this design? Is this higher or lower than the systematic parallel design?\n\nYou were asked to then run a coverage simulation and check if the trackline length was within our effort constraints. I found the maximum trackline length to be 242km (see Trackline length summary table in the output below) so within our constraint of 250km. I then got a mean line length of 221km and minimum and maximum line lengths of 212km and 227km, respectively (see Line length summary table in the output below). We can therefore expect to achieve just over 20km more on-effort survey line length with the zigzag design than the systematic parallel line design - 10% gain. [Note your values may differ]\n\n\n\n\ndesign.zz.4500 &lt;- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.zz.4500 &lt;- run.coverage(design.zz.4500, reps = 250, quiet=TRUE)\n# Plot coverage\nplot(design.zz.4500)\n\n\n\n\nCoverage grid plot for zigzag design of St Andrews Bay.\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nDo you think the coverage scores look uniform across the study region? Where are they higher/lower? Why do you think this is?\n\nYou were finally asked to look at the coverage scores across the survey region to see if this design has even coverage. There are some points with lower coverage around the survey region boundary. This is actually down to the fact we are using a minus sampling strategy. If we plotted coverage scores from a systematic parallel design we would see a similar pattern. Usually edge effects from minus sampling are minor unless we have a very long survey region boundary containing a small study area. If the fact that we are using a zigzag design was causing us issues with coverage we would expect to see higher coverage at the very top or very bottom of the survey region (as our design angle is 0). We do not see this. The survey region boundaries at the top and bottom are both quite wide and perpendicular to the design angle, in this situation zigzag designs perform well with regard to even coverage."
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html#coverage",
    "href": "06-design/R-prac/Pr6-solution.html#coverage",
    "title": "Distance sampling survey design solution",
    "section": "Coverage",
    "text": "Coverage\nOrganise the study area shape file.\n\nshapefile.name &lt;- system.file(\"extdata\", \"TentsmuirUnproj.shp\", \n                              package = \"dssd\")\nsf.shape &lt;- read_sf(shapefile.name)\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nproj4string &lt;- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\nprojected.shape &lt;- st_transform(sf.shape, crs = proj4string)\nregion.tm &lt;- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n\nCreate the coverage grid.\n\ncover.tm &lt;- make.coverage(region.tm, n.grid.points = 5000)\ndesign.tm &lt;- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)\nsurvey.tentsmuir &lt;- generate.transects(design.tm)\n\n\nprint(survey.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  751.2295\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  775261.3\nStrata coverage: 5.49%\nStrata area:  14108643\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  218.3674\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  414517\nStrata coverage: 57.95%\nStrata area:  715264.9\n\n   Study Area Totals:\n   _________________\nNumber of samplers:  40\nCovered area:  1189778\nAverage coverage: 8.03%\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers? Did your survey achieve exactly the number of samplers you requested? How much does coverage differ between the two strata for this realisation?\n\nA spacing of 751m was used in the main stratum and 218m in the Morton Lochs stratum - these values are calculated based on the stratum areas and should not vary between surveys generated from the same design. You may or may not have achieved the number of transects you requested, this will depend on the random start point calculated for your particular survey. There will also be some variability in coverage, my survey achieved a coverage of 5.7% in the main strata and 64.8% in the Morton Loch strata.\n\n\n\n\ncoverage.tentsmuir &lt;- run.coverage(design.tm, reps=250, quiet=TRUE)\nprint(coverage.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum      22.0         12.0  36.0\nMean         25.0         14.9  39.9\nMedian       25.0         15.0  40.0\nMaximum      27.0         18.0  44.0\nsd            1.1          1.1   1.6\n\n    Covered area:\n    \n        Main Area Morton Lochs      Total\nMinimum 674426.40    347371.99 1092100.47\nMean    764496.81    414334.54 1178831.35\nMedian  766263.37    413091.36 1178916.49\nMaximum 819101.39    468675.57 1274992.68\nsd       29267.45     25896.97   40367.31\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.78        48.57  7.37\nMean         5.42        57.93  7.95\nMedian       5.43        57.75  7.95\nMaximum      5.81        65.52  8.60\nsd           0.21         3.62  0.27\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00800000     0.232000 0.00800000\nMean    0.05397142     0.578750 0.07917087\nMedian  0.05200000     0.632000 0.05200000\nMaximum 0.11200000     0.696000 0.69600000\nsd      0.01433357     0.110141 0.11561763\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nView the design statistics. What is the minimum number of samplers you will achieve in each strata? Is this sufficient to complete separate analyses in each stratum?\n\nMy design statistics indicated I should achieve between 22 and 27 transects in the main stratum and between 12 and 18 in the Morton Lochs stratum. I might be a bit concerned about the possibility of only achieving 12 transects in the Morton Lochs stratum (remember I cannot just discard a survey due to the number of transects and generate another as it will affect my coverage properties) but whether this is sufficient will depend on a number of things… what are the objectives of the study? how many detections are you likely to get from each transect? etc. Information from a pilot study would be useful to help decide how many transects are required as a minimum.\n\n\n\n\nplot(coverage.tentsmuir, strata=1)\nplot(coverage.tentsmuir, strata=2)\n\n\n\n\n\n\nCoverage scores main stratum Tentsmuir Forest.\n\n\n\n\n\n\n\nCoverage scores Morton Lochs stratum Tentsmuir Forest.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nDoes it appear that you that there is even coverage within strata?\n\nThe main strata looks to have fairly uniform coverage. The values appear to have such small levels of variability that the variability that is seen will be down to stochasticity as it is seen across the entire strata. The Morton Lochs strata we can see has areas of lower coverage around the edge of the study region. This grid is a bit too coarse to allow us to properly judge how much of an issue edge effects will be in this strata. It may be wise to re-run the coverage simulation with a finer coverage grid and more repetitions too. Edge effects could potentially be problematic in such small areas."
  },
  {
    "objectID": "08-covariates/covariateslanding.html",
    "href": "08-covariates/covariateslanding.html",
    "title": "Including covariates in detection function modelling",
    "section": "",
    "text": "It is not just distance from the transect that influences the detectability of animals. In most situations, inference regarding animal density is not hindered if additional causes of variation in detectability are unaccounted.\nThere are some situations in which covariates in addition to distance can be added to models of the detection function. One example of this is animal group size. It is commonly the case that small groups at large distances are not detected and do not enter our sample that is used to estimate the average group size in the population. By failing to have the small groups in our sample, the sample is biased; we estimate that the average group size in the field is larger than it really is; producing biased estimates of population size. The use of group size as a covariate is the recommended way to remove that bias.\nThis exercise presents three sets of data: Hawaiian amakihi point transect data collected by multiple observers at varying times during the morning. Eastern Tropical Pacific dolphin surveys where there were different types of vessels, sea state and widely varying dolphin school sizes. Finally, additional bird point transect data from Colorado where the study area was divided into geographic strata–we examine whether the geographic stratum effect can be modelled as a covariate."
  },
  {
    "objectID": "08-covariates/covariateslanding.html#analysis-of-point-transect-data",
    "href": "08-covariates/covariateslanding.html#analysis-of-point-transect-data",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "08-covariates/covariateslanding.html#lecture-materials",
    "href": "08-covariates/covariateslanding.html#lecture-materials",
    "title": "Including covariates in detection function modelling",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "08-covariates/covariateslanding.html#exercise-materials",
    "href": "08-covariates/covariateslanding.html#exercise-materials",
    "title": "Including covariates in detection function modelling",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "08-covariates/covariateslanding.html#supplemental-materials",
    "href": "08-covariates/covariateslanding.html#supplemental-materials",
    "title": "Including covariates in detection function modelling",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "06-design/designlanding.html",
    "href": "06-design/designlanding.html",
    "title": "Design of distance sampling surveys",
    "section": "",
    "text": "Fundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with our software, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add “incentives” to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area."
  },
  {
    "objectID": "06-design/designlanding.html#analysis-of-point-transect-data",
    "href": "06-design/designlanding.html#analysis-of-point-transect-data",
    "title": "Analysis of data from stratified designs",
    "section": "",
    "text": "Fundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with our software, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add “incentives” to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area."
  },
  {
    "objectID": "06-design/designlanding.html#lecture-materials",
    "href": "06-design/designlanding.html#lecture-materials",
    "title": "Design of distance sampling surveys",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "06-design/designlanding.html#exercise-materials",
    "href": "06-design/designlanding.html#exercise-materials",
    "title": "Design of distance sampling surveys",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "06-design/designlanding.html#supplemental-materials",
    "href": "06-design/designlanding.html#supplemental-materials",
    "title": "Design of distance sampling surveys",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "07-stratify/stratifylanding.html",
    "href": "07-stratify/stratifylanding.html",
    "title": "Analysis of stratified surveys",
    "section": "",
    "text": "Carrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function."
  },
  {
    "objectID": "07-stratify/stratifylanding.html#analysis-of-point-transect-data",
    "href": "07-stratify/stratifylanding.html#analysis-of-point-transect-data",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "07-stratify/stratifylanding.html#lecture-materials",
    "href": "07-stratify/stratifylanding.html#lecture-materials",
    "title": "Analysis of stratified surveys",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "07-stratify/stratifylanding.html#exercise-materials",
    "href": "07-stratify/stratifylanding.html#exercise-materials",
    "title": "Analysis of stratified surveys",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "07-stratify/stratifylanding.html#supplemental-materials",
    "href": "07-stratify/stratifylanding.html#supplemental-materials",
    "title": "Analysis of stratified surveys",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "07-stratify/R-prac/Prac7_solution.html",
    "href": "07-stratify/R-prac/Prac7_solution.html",
    "title": "Analysis of data from stratified surveys solution",
    "section": "",
    "text": "Solution\n\n\n\nAnalysis of data from stratified surveys\n\n\nReading in the data from the stratified survey in the Southern Ocean:\n\nlibrary(Distance)\nlibrary(kableExtra)\n# Load data\ndata(minke)\nhead(minke, n=3)\n\n  Region.Label  Area Sample.Label Effort distance object\n1        South 84734            1  86.75     0.10      1\n2        South 84734            1  86.75     0.22      2\n3        South 84734            1  86.75     0.16      3\n\n# Specify truncation distance\nminke.trunc &lt;- 1.5\n\n\nStrata treated distinctly\nFit detection function and encounter rate separately in each strata.\n\n## Fit to each region separately - full geographical stratification\n# Create data set for South\nminke.S &lt;- minke[minke$Region.Label==\"South\", ]\nminke.df.S.strat &lt;- ds(minke.S, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.S.strat)\n\n\nSummary for distance analysis \nNumber of observations :  39 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  8.617404 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.5102606 0.1921723\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.242147 0.3770239\n\n                      Estimate         SE       CV\nAverage p            0.4956459  0.0662961 0.133757\nN in covered region 78.6852058 13.8143714 0.175565\n\nSummary statistics:\n  Region  Area CoveredArea Effort  n  k         ER      se.ER     cv.ER\n1  South 84734     1453.23 484.41 39 13 0.08051031 0.01809954 0.2248102\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 4587.926 1200.166 0.2615924 2687.497 7832.219 21.14052\n\nDensity:\n  Label   Estimate         se        cv        lcl        ucl       df\n1 Total 0.05414505 0.01416393 0.2615924 0.03171687 0.09243301 21.14052\n\n# Combine selection and detection function fitting for North\nminke.df.N.strat &lt;- ds(minke[minke$Region.Label==\"North\", ],\n                       truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.N.strat)\n\n\nSummary for distance analysis \nNumber of observations :  49 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  37.27825 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.0108104 0.2203526\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.022021 0.6907906\n\n                      Estimate         SE        CV\nAverage p            0.7592309 0.09987673 0.1315499\nN in covered region 64.5389972 9.62021387 0.1490605\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 9986.683 3878.031 0.3883203 4469.865 22312.49 13.98197\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 Total 0.01583725 0.006149924 0.3883203 0.007088475 0.03538397 13.98197\n\n\n\n\nDetections combined across strata\nNext we fitted a pooled detection function.\n\nminke.df.all &lt;- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\n\nSummary for distance analysis \nNumber of observations :  88 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  48.63688 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.2967912 0.1765812\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 0.964833 0.3605009\n\n                       Estimate         SE        CV\nAverage p             0.6224396  0.0668011 0.1073214\nN in covered region 141.3791725 17.7757788 0.1257312\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n2  South  84734     1453.23  484.41 39 13 0.08051031 0.01809954 0.2248102\n3  Total 715316     5528.37 1842.79 88 25 0.04133635 0.01181436 0.2858103\n\nAbundance:\n  Label  Estimate        se        cv      lcl       ucl       df\n1 North 12181.419 4638.6284 0.3807954 5499.950 26979.692 12.96781\n2 South  3653.345  910.0975 0.2491135 2181.595  6117.971 17.96263\n3 Total 15834.764 4834.2848 0.3052957 8388.423 29891.167 15.25496\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 North 0.01931774 0.007356107 0.3807954 0.008722023 0.04278538 12.96781\n2 South 0.04311546 0.010740641 0.2491135 0.025746391 0.07220207 17.96263\n3 Total 0.02213674 0.006758251 0.3052957 0.011726878 0.04178736 15.25496\n\n\nCompute combined AIC for entire study area.\n\naic.all &lt;- summary(minke.df.all$ddf)$aic\naic.S &lt;- summary(minke.df.S.strat$ddf)$aic\naic.N &lt;- summary(minke.df.N.strat$ddf)$aic\naic.SN &lt;- aic.S + aic.N\n\n\n\nDetermining the correct stratification to use\nThe AIC value for the detection function in the South was 8.617 and the AIC for the North was 37.28. This gives a total AIC of 45.9. The AIC value for the pooled detection function was 48.64. Because 48.64 is greater than 45.9, estimation of separate detection functions in each stratum is preferable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiffering abundance estimates from stratification decision\nIn the full geographical stratification, both encounter rate and detection function were estimated separately for each region (or strata). This resulted in the following abundances:\n\n\n\nAbundance estimates using full geographical stratification.\n\n\nLabel\nEstimate\n\n\n\n\nNorth\n9987\n\n\nSouth\n4588\n\n\nTotal\n14575\n\n\n\n\n\n\n\nNext, the distances were combined to fit a pooled detection function but encounter rate was obtained for each region. This resulted in the following abundances:\n\n\n\nAbundance estimates calculating encounter rate by strata and a pooled detection function.\n\n\nLabel\nEstimate\n\n\n\n\nNorth\n12181\n\n\nSouth\n3653\n\n\nTotal\n15835\n\n\n\n\n\n\n\n\n\nAnother approach to stratification (advanced)\nAn equivalent result for full geographic stratification could be produced using the dht2 function, which does not require the disaggregation of the data set into two data sets.\n\n# Geographical stratification with stratum-specific detection function \nstrat.specific.detfn &lt;- ds(data=minke, truncation=minke.trunc, key=\"hr\", \n                           adjustment=NULL, formula=~Region.Label)\nabund.by.strata &lt;- dht2(ddf=strat.specific.detfn, flatfile=minke, \n                        strat_formula=~Region.Label, stratification=\"geographical\")\nprint(abund.by.strata, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : R2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n Region.Label   Area CoveredArea  Effort  n  k    ER se.ER cv.ER\n        North 630582     4075.14 1358.38 49 12 0.036 0.013 0.365\n        South  84734     1453.23  484.41 39 13 0.081 0.018 0.225\n        Total 715316     5528.37 1842.79 88 25 0.048 0.011 0.237\n\nAbundance estimates:\n Region.Label Estimate       se    cv  LCI   UCI     df\n        North     9865 3761.296 0.381 4451 21860 13.035\n        South     4651 1224.818 0.263 2719  7955 22.163\n        Total    14515 3970.578 0.274 8215 25648 16.065\n\nComponent percentages of variance:\n Region.Label Detection    ER\n        North      8.18 91.82\n        South     27.13 72.87\n        Total     12.49 87.51\n\n\nI won’t say anything just now about the wrinkle I introduced with the formula argument in the call to ds(). Recognise there is an alternative (easier) way to perform the full geographic stratification analysis without tearing apart the data. The abundance estimates presented in the last output do not identically match the estimates shown earlier for full geographic stratification, but they are close. The added benefit of this latter analysis is that the uncertainty in the total population size is computed within dht2 rather than needing to be calculated manually using the delta method.\n\n\n\n\n\n\nAn aside\n\n\n\nIf geographic stratification were ignored, the abundance estimate of would be 18,293 minkes. This estimate is substantially larger than the estimates above. The reason is that the survey design was geographically stratified with a smaller proportion of the north stratum receiving sampling effort and a greater proportion of the southern stratum receiving survey effort. Ignoring this inequity in the unstratified analysis would lead us to believe that the more heavily sampled southern stratum is indicative of whale density throughout the study area."
  },
  {
    "objectID": "07-stratify/stratifylanding.html#analysis-of-stratified-surveys",
    "href": "07-stratify/stratifylanding.html#analysis-of-stratified-surveys",
    "title": "Analysis of stratified surveys",
    "section": "",
    "text": "Carrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function."
  },
  {
    "objectID": "07-stratify/R-prac/Pr7-instructions.html",
    "href": "07-stratify/R-prac/Pr7-instructions.html",
    "title": "Analysis of stratified survey data",
    "section": "",
    "text": "For this exercise, we use data from a survey of Antarctic minke whales. The study region was divided into two strata (North and South) the two strata were surveyed by different vessels at the same time. The minkes tend to be found in high densities against the ice edge, where they feed, and so the density in the southern stratum is typically higher than in the northern stratum (Figure 1). This is the primary reason for using a stratified survey design. It is also the reason for covering the southern stratum more intensely: in the southern stratum the transect length per unit area is more than 2.5 times that of the northern stratum.\n\nFigure 1. An example of the sort of survey design used and a typical minke density gradient. The irregular bottom border is the ice-edge. The ‘stepped’ black line defines the boundary between the strata; dotted lines are transects and dots are detections.\n\nObjectives\nThe objectives of this exercise are to:\n\nCreate subsets of the data\nDecide whether to fit separate detection functions or a pooled detection function\nSpecify different stratification options using the dht2 function.\n\n\n\nGetting started\nBegin by reading in the data. Distances are in kilometers and a truncation distance of 1.5km is specified and used in the following detection function fitting. Perpendicular distances, transect lengths and study area size are all measured in kilometers; hence convert_units argument to ds is 1 and has been omitted. To keep things simple, a hazard rate detection function with no adjustments is used for all detection functions.\n\nlibrary(Distance)\ndata(minke)\nhead(minke)\n# Specify truncation distance\nminke.trunc &lt;- 1.5\n\nYou will see that these data contain a column called ‘Region.Label’: this contains values ‘North’ or ‘South’.\n\n\nFull geographical stratification\nFirst, we want to fit encounter rate and detection function separately in each strata. This is easily performed by splitting the data by region and using ds on each subset. The commands below do this for the southern region (note, there are alternative ways to select a subset of data).\n\n# Create dataset for South \nminke.S &lt;- minke[minke$Region.Label==\"South\", ]\n# Fit df to south\nminke.df.S.strat &lt;- ds(minke.S, key=\"hr\", adjustment=NULL, truncation=minke.trunc)\nsummary(minke.df.S.strat)\n\nMake a note of the AIC. Perform a similar commands to obtain estimates for the northern region. What is the total AIC?\nAlso make a note of the abundance in each region. What is the total abundance in the study region?\n\n\nFitting a pooled detection function\nWe want to compare the total AIC found previously with the AIC from fitting a detection function to all data combined. This is easy to obtain:\n\nminke.df.all &lt;- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\nGiven the AIC value for the detection function from the pooled data, would you fit a separate detection function in each strata or not?\n\n\nStratification options using dht2\nThe command summary(minke.df.all) will provide the abundance estimates for each region and the total and for this simple example, this is sufficient. However, if we want to consider different stratification options, then the dht2 function is useful.\nAfter fitting a detection function, the dht2 function, allows abundance estimates to be computed over some specified regions. In the command below, the pooled detection function is used to obtain estimates in each strata and over all (like the summary function previously used).\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~Region.Label, stratification=\"geographical\")\n\nThe arguments are:\n\nddf\n\nthe detection function (fitted by ds)\n\nflatfile\n\nthe data object containing all the necessary information\n\nData is referred to as being in a flatfile format if it contains information on region, transects and observations. An alternative is to use a hierarchical structure and have region, transect and observation information in separate data files with links between them to ensure that transects are mapped to the relevant region and observations to the relevant transect. We’ve not used the hierarchical structure during this workshop.\n\n\nstrat_formula=~Region.Label\n\nformula (hence the ~) giving the stratification structure\n\nstratification=\"geographical\"\n\nin this example, we specify that each strata (specified in strat_formula) represents a geographical region.\n\nconvert_units\n\ngetting units conversion correct, same purpose as the convert_units argument in ds. For the minke data all measurements are in the same units, so the argument is not needed in this case.\n\n\nMake a note the total abundance in the study region.\n\n\n\n\n\n\nFailure to respect design during analysis\n\n\n\nWhat happens if we were to ignore the regions and treat the data as though it came from one large study region? This can (dangerously) be done by changing the stratification formula, as shown below.\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~1, stratification=\"geographical\")\n\nHas this changed the abundance estimate? Of course it has; the question is why has this changed the abundance estimate; which estimate is proper?"
  },
  {
    "objectID": "08-covariates/covariateslanding.html#including-covariates-in-detection-function-modelling",
    "href": "08-covariates/covariateslanding.html#including-covariates-in-detection-function-modelling",
    "title": "Including covariates in detection function modelling",
    "section": "",
    "text": "It is not just distance from the transect that influences the detectability of animals. In most situations, inference regarding animal density is not hindered if additional causes of variation in detectability are unaccounted.\nThere are some situations in which covariates in addition to distance can be added to models of the detection function. One example of this is animal group size. It is commonly the case that small groups at large distances are not detected and do not enter our sample that is used to estimate the average group size in the population. By failing to have the small groups in our sample, the sample is biased; we estimate that the average group size in the field is larger than it really is; producing biased estimates of population size. The use of group size as a covariate is the recommended way to remove that bias.\nThis exercise presents three sets of data: Hawaiian amakihi point transect data collected by multiple observers at varying times during the morning. Eastern Tropical Pacific dolphin surveys where there were different types of vessels, sea state and widely varying dolphin school sizes. Finally, additional bird point transect data from Colorado where the study area was divided into geographic strata–we examine whether the geographic stratum effect can be modelled as a covariate."
  },
  {
    "objectID": "08-covariates/R-prac/Prac8_solution.html",
    "href": "08-covariates/R-prac/Prac8_solution.html",
    "title": "Covariates in the detection function solution",
    "section": "",
    "text": "Solution\n\n\n\nCovariates in the detection function"
  },
  {
    "objectID": "08-covariates/R-prac/Prac8_solution.html#colourful-plot-noting-effect-of-cue-type",
    "href": "08-covariates/R-prac/Prac8_solution.html#colourful-plot-noting-effect-of-cue-type",
    "title": "Covariates in the detection function solution",
    "section": "Colourful plot noting effect of cue type",
    "text": "Colourful plot noting effect of cue type\nJust an example of using the function add_df_covar_line to visually explore consequences of covariates on the detection function. A regular call to plot() is first used to produce the histogram and average detection function line; subsequent calls to the new function with different values of the covariate of interest completes the plot.\n\nplot(etp.hr.cue, main=\"ETP dolphin survey\", showpoints=FALSE)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=1), col='red', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=2), col='blue', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=3), col='green', lwd=2, lty=1)\nadd_df_covar_line(etp.hr.cue, data = data.frame(Cue.type=4), col='purple', lwd=2, lty=1)\nlegend(\"topright\", legend=c(\"Birds\",\"Splashes\",\"Unspecified\",\"Floating objects\"),\n       col=c(\"red\", \"blue\", \"green\", \"purple\"), lwd=2, title = \"Cue type\")\n\n\n\n\nDetection function with cue type as covariate."
  },
  {
    "objectID": "08-covariates/R-prac/Pr8-instructions.html",
    "href": "08-covariates/R-prac/Pr8-instructions.html",
    "title": "Covariates in detection function model",
    "section": "",
    "text": "This exercise consists of three data sets of increasing difficulty. The first problem, MCDS with point transects, is complicated and (using the functionality available in R) also includes some basic exploratory analysis of the covariates. Section 2 and 3 are optional but will take you deeper into the heart of understanding multiple covariates."
  },
  {
    "objectID": "08-covariates/R-prac/Pr8-instructions.html#exploratory-data-analysis",
    "href": "08-covariates/R-prac/Pr8-instructions.html#exploratory-data-analysis",
    "title": "Covariates in detection function model",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nIt is important to gain an understanding of the data prior to fitting detection functions (Buckland et al., 2015). With this in mind, preliminary analysis of distance sampling data involves:\n\nassessing the shape of the collected data,\nconsidering the level of truncation of distances, and\nexploring patterns in potential covariates.\n\nWe begin by assessing the distribution of distances to decide on a truncation distance.\n\nhist(amakihi$distance)\n\nTo see if there are differences in the distribution of distances recorded by the different observers and in each hour after sunrise, boxplots can be used. Note how the ~ symbol is used to define the discrete groupings (i.e. observer and hour).\n\n# Boxplots by obs\nboxplot(amakihi$distance~amakihi$OBs, xlab=\"Observer\", ylab=\"Distance (m)\")\n# Boxplots by hour after sunrise\nboxplot(amakihi$distance~amakihi$HAS, xlab=\"Hour\", ylab=\"Distance (m)\")\n\nThe components of the boxplot are:\n\nthe thick black line indicates the median\nthe lower limit of the box is the first quartile (25th percentile) and the upper limit is the third quartile (75th percentile)\nthe height of the box is the interquartile range (75th - 25th quartiles)\nthe whiskers extend to the most extreme points which are no more than 1.5 times the interquartile range.\ndots indicate ‘outliers’ if there are any, i.e. points beyond the range of the whiskers.\n\nFor minutes after sunrise (a continuous variable), we create a scatterplot of MAS (on the \\(x\\)-axis) against distances (on the \\(y\\)-axis). The plotting symbol (or character) is selected with the argument pch:\n\n# Plot of MAS vs distance (using dots)\nplot(x=amakihi$MAS, y=amakihi$distance, xlab=\"Minutes after sunrise\",\n     ylab=\"Distance (m)\", pch=20)\n\nYou may also want to think about potential collinerity (linear relationship) between the covariates - if collinear variables are included in the detection function, they will be explaining some of the same variation in the distances and this will reduce their importance as a potential covariate. How might you investigate the relationship between HAS and MAS?\nFrom these plots can you tell if any of the covariates will be useful in explaining the distribution of distances?"
  },
  {
    "objectID": "08-covariates/R-prac/Pr8-instructions.html#adjusting-the-raw-covariates",
    "href": "08-covariates/R-prac/Pr8-instructions.html#adjusting-the-raw-covariates",
    "title": "Covariates in detection function model",
    "section": "Adjusting the raw covariates",
    "text": "Adjusting the raw covariates\nWe would like to treat OBs and HAS as factor variables as in the original analysis; OBs is, by default, treated as a factor variable because it consists of characters rather than numbers. HAS, on the other hand, consists of numbers and so by default would be treated as a continuous variable (i.e. non-factor). That is fine if we want the effect of HAS to be monotonic (i.e. detectability either increases or decreases as a function of HAS). If we want HAS to have a non-linear effect on detectability, then we need to indicate to R to treat it as a factor as shown below.\n\n# Convert HAS to a factor\namakihi$HAS &lt;- factor(amakihi$HAS)\n\nThe next adjustment is to change the reference level of the observer and hour factor covariates - the only reason to do this is to get the estimated parameters in the detection function to match the parameters estimated in T. A. Marques et al. (2007). You would not carry out this step on your own data. By default R uses the first factor level but by using the relevel function, this can be changed:\n\n# Set the reference level \namakihi$OBs &lt;- relevel(amakihi$OBs, ref=\"TKP\")\namakihi$HAS &lt;- relevel(amakihi$HAS, ref=\"5\")"
  },
  {
    "objectID": "08-covariates/R-prac/Pr8-instructions.html#candidate-models",
    "href": "08-covariates/R-prac/Pr8-instructions.html#candidate-models",
    "title": "Covariates in detection function model",
    "section": "Candidate models",
    "text": "Candidate models\nWith three potential covariates, there are 8 possible models for the detection function:\n\nNo covariates\nOBs\nHAS\nMAS\nOBs + HAS\nOBs + MAS\nHAS + MAS\nOBs + HAS + MAS\n\nEven without considering covariates there are also several possible key function/adjustment term combinations available: if all key function/covariate combinations are considered the number of potential models is large. Note that covariates are not allowed if a uniform key function is chosen and if covariate terms are included, adjustment terms are not allowed. Even with these restrictions, it is not best practice to take a scatter gun approach to detection function model fitting. Buckland et al. (2015) considered 13 combinations of key function/covariates. Here, we look at a subset of these.\nFit a hazard rate model with no covariates or adjustment terms and make a note of the AIC. Note, that 10% of the largest distances are truncated - you may have decided on a different truncation distance.\n\nconversion.factor &lt;- convert_units(\"meter\", NULL, \"hectare\")\namak.hr &lt;- ds(amakihi, transect=\"point\", key=\"hr\", truncation=\"10%\",\n              adjustment=NULL, convert_units = conversion.factor)\n\nMake a note of the AIC for this model.\nNow fit a hazard rate model with OBs as a covariate in the detection function and make a note of the AIC. Has the AIC reduced by including a covariate?\n\nconversion.factor &lt;- convert_units(\"meter\", NULL, \"hectare\")\namak.hr.obs &lt;- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs,\n                  truncation=\"10%\", convert_units = conversion.factor)\n\nFit a hazard rate model with OBs and HAS in the detection function:\n\namak.hr.obs.has &lt;- ds(amakihi, transect=\"point\", key=\"hr\", formula=~OBs+HAS,\n                      truncation=\"10%\", convert_units = conversion.factor)\n\nTry fitting other possible formula and decide which model is best in terms of AIC. To quickly compare AIC values from different models, use the AIC command as follows (note only models with the same truncation distance can be compared):\n\n# AIC values\nAIC(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nAnother useful function is summarize_ds_models - this has the advantage of ordering the models by AIC (smallest to largest).\n\n# Compare models\nsummarize_ds_models(amak.hr, amak.hr.obs, amak.hr.obs.has)\n\nOnce you have decided on a model, plot your selected detection function."
  },
  {
    "objectID": "08-covariates/R-prac/Pr8-instructions.html#analysis",
    "href": "08-covariates/R-prac/Pr8-instructions.html#analysis",
    "title": "Covariates in detection function model",
    "section": "Analysis",
    "text": "Analysis\nThe data are available in the Distance package:\n\ndata(ETP_Dolphin)\nhead(ETP_Dolphin, n=3)\n\nStart by running a set of conventional distance analyses. Are there any problems in the data and if so how might you mitigate them? (Hint - try dividing the histogram of distances into a large number of intervals.)\nAs there are a number of potential covariates to be used in this example (i.e. search method, cue, Beaufort class and month), try fitting models with different covariates and combinations of the covariates. All of the covariates in this example are factor covariates except group size and because they have numeric codes, use the factor function to let R know to treat them as factors.\nNote that both distances and transect lengths were recorded in nautical miles and area in nautical miles squared and so the argument convert_units does not need to be specified.\nKeep in mind that this is a large dataset (&gt; 1000 observations), and hence estimation may take a while. You will likely end up with quite a few models as there are several potential covariates and no ‘right’ answers. Discuss your choice of final model (or models) with your colleagues - did you make the same choices?"
  },
  {
    "objectID": "06-design/designlanding.html#distance-sampling-survey-design",
    "href": "06-design/designlanding.html#distance-sampling-survey-design",
    "title": "Design of distance sampling surveys",
    "section": "",
    "text": "Fundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with our software, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add “incentives” to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area."
  },
  {
    "objectID": "07-stratify/DistWin-prac/Pr7-solution-DistWin.html",
    "href": "07-stratify/DistWin-prac/Pr7-solution-DistWin.html",
    "title": "Analysis of stratified data",
    "section": "",
    "text": "Analysis of stratified data\n\n\n\n\n\nDistance for Windows exercise Solution\n\n\n\nOutline Solutions\nExample analyses, which were used in getting these solutions, and which are referred to below, are in the project file Stratify solutions.zip.\n\nRelevant results are in Analysis “Full geog stratification”.\n\nThe AICs are 127.90 for the southern stratum and 187.90 for the northern stratum. Detection function model fits are adequate visually and by goodness-of-fit test. Sample sizes are relatively small but not alarmingly so. The southern stratum appears to have a much narrower effective strip width.\n\nRelevant results are in Analysis “Pooled f(0)”.\n\nThe AIC for the pooled detection function fit is 318.72. The detection function model fit is adequate visually and by goodness-of-fit test. Because\n\\[318.72 &gt; (127.9+187.9=315.8)\\]\nestimation of separate detection function in each stratum is preferable.\n\nRelevant results are in Analysis “No stratification”.\n\nThe whale density estimate from the unstratified analysis is around 25% larger than the corresponding estimates from 1. and 2. above. The reason is that the survey design was geographically stratified, with less survey effort in the north stratum, and this is being ignored in the unstratified analysis.\nWhat is not included in this project are cluster sizes of the observed minke whale groups (we didn’t want to clutter the analysis with that detail). However, there is a bit of a story in geographic variation in cluster sizes. Cluster densities are higher in the southern stratum, but transects from both strata are being treated as if they are representative of the whole survey region. This results in a positively biased cluster density for the region as a whole. In addition, cluster sizes are higher in the South stratum. The estimate of E(s) from the unstratified analysis is a positively biased estimate of E(s) for the North stratum and a negatively biased estimate of E(s) for the South stratum. When it is applied to both strata, it results in a positively biased estimate of whale abundance because the North stratum is much larger and contains roughly twice as many whales as the south stratum.\n\n\n\n\n\n\nMoral\n\n\n\n\n\nDo not perform analyses without taking the survey design into account."
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html",
    "title": "Analysis of double-platform data",
    "section": "",
    "text": "Analysis of double-platform data\nThis version of the practical is for those who would like to conduct the analysis in R using the package mrds (Laake et al., 2023). There is a separate version for conducting the analysis in Distance for Windows (Thomas et al., 2010).\nThe first part of this practical involves initial analysis of a survey of a known number of golf tees. This is intended mainly to familiarise you with the mrds function.\nThe second part of the practical involves more detailed analysis of the golf tee data, and an exploration of the double-platform data structure.\nThe third part of the practical involves analysis of the pack-ice seal survey data of (Borchers et al., 2006) and (Southwelll2007?).\nTo help understand the terminology used in MRDS and the output produced by mrds, there is a guide available ‘Interpreting MRDS output’."
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#golf-tee-survey",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#golf-tee-survey",
    "title": "Analysis of double-platform data",
    "section": "Golf tee survey",
    "text": "Golf tee survey\nThese data come from a survey of golf tees which conducted by statistics students at the University of St Andrews. The data were collected along transect lines, 210 metres in total. A distance of 4 metres out from the centre line was searched and, for the purposes of this exercise, we assume that this comprised the total study area, which was divided into two strata. There were 250 clusters of tees in total and 760 individual tees in total.\nThe population was independently surveyed by two observer teams. The following data were recorded for each detected group: perpendicular distance, cluster size, observer (team 1 or 2), ‘sex’ (males are yellow and females are green and golf tees occur in single-sex clusters) and ‘exposure’. Exposure was a subjective judgment of whether the cluster was substantially obscured by grass (exposure=0) or not (exposure=1). The lengths of grass varied along the transect line and the grass was slightly more yellow along one part of the line compared to the rest.\nThe golf tee dataset is provided as part of the mrds package.\nOpen R and load the mrds package and golf tee dataset (called book.tee.data). The elements required for an MRDS analysis are contained within the object dataset. These data are in a hierarchical structure (rather than in a ‘flat file’ format) so that there are separate elements for observations, samples and regions. In the code below, each of these tables is extracted to avoid typing long names.\n\n# Load libraries\nlibrary(knitr) #Used to knit this markdown document together\nlibrary(mrds)\n#Note - the Distance package is also used in the Crabeater seal example, to access\n# the checkdata function, and to do an MCDS analysis.\n\n# Access the golf tee data\ndata(book.tee.data)\n\n# Extract the list elements from the dataset into easy-to-access objects\ndetections &lt;- book.tee.data$book.tee.dataframe # detection information\nregion &lt;- book.tee.data$book.tee.region # region info\nsamples &lt;- book.tee.data$book.tee.samples # transect info\nobs &lt;- book.tee.data$book.tee.obs # links detections to transects and regions\n\n# In the detections data frame, define sex and exposure \n# as factor variables \ndetections$sex &lt;- as.factor(detections$sex)\ndetections$exposure &lt;- as.factor(detections$exposure)"
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#golf-tee-survey-analyses",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#golf-tee-survey-analyses",
    "title": "Analysis of double-platform data",
    "section": "Golf tee survey analyses",
    "text": "Golf tee survey analyses\n\nEstimation of p: distance only\nWe will start by analysing these data assuming that Observer 2 was generating trials for Observer 1 but not vice versa, i.e. trial configuration where Observer 1 is the primary and Observer 2 is the tracker. (The data could also be analysed in independent observer configuration - you are welcome to try this for yourself). We begin by assuming full independence (i.e. detections between observers are independent at all distances): this requires only a mark-recapture (MR) model and, to start with, perpendicular distance will be included as the only covariate.\nRemember that ? or help can be used to find out more about any of the functions used – e.g., ?ddf will tell you more about the ddf function.\n\n# Fit trial configuration with full independence model\nfi.mr.dist &lt;- ddf(method='trial.fi', mrmodel=~glm(link='logit',formula=~distance),\n                  data=detections, meta.data=list(width=4))\n\n\nExamining mrds output\nHaving fitted the model, we can create tables summarizing the detection data. In the commands below, the tables are created using the det.tables function and saved to detection.tables.\n\n# Create a set of tables summarizing the double observer data \ndetection.tables &lt;- det.tables(fi.mr.dist)\n# Print these detection tables\ndetection.tables\n\n\nObserver 1 detections\n           Detected\n            Missed Detected\n  [0,0.4]        1       25\n  (0.4,0.8]      2       16\n  (0.8,1.2]      2       16\n  (1.2,1.6]      6       22\n  (1.6,2]        5        9\n  (2,2.4]        2       10\n  (2.4,2.8]      6       12\n  (2.8,3.2]      6        9\n  (3.2,3.6]      2        3\n  (3.6,4]        6        2\n\nObserver 2 detections\n           Detected\n            Missed Detected\n  [0,0.4]        4       22\n  (0.4,0.8]      1       17\n  (0.8,1.2]      0       18\n  (1.2,1.6]      2       26\n  (1.6,2]        1       13\n  (2,2.4]        2       10\n  (2.4,2.8]      3       15\n  (2.8,3.2]      4       11\n  (3.2,3.6]      2        3\n  (3.6,4]        1        7\n\nDuplicate detections\n\n  [0,0.4] (0.4,0.8] (0.8,1.2] (1.2,1.6]   (1.6,2]   (2,2.4] (2.4,2.8] (2.8,3.2] \n       21        15        16        20         8         8         9         5 \n(3.2,3.6]   (3.6,4] \n        1         1 \n\nObserver 1 detections of those seen by Observer 2\n          Missed Detected Prop. detected\n[0,0.4]        1       21      0.9545455\n(0.4,0.8]      2       15      0.8823529\n(0.8,1.2]      2       16      0.8888889\n(1.2,1.6]      6       20      0.7692308\n(1.6,2]        5        8      0.6153846\n(2,2.4]        2        8      0.8000000\n(2.4,2.8]      6        9      0.6000000\n(2.8,3.2]      6        5      0.4545455\n(3.2,3.6]      2        1      0.3333333\n(3.6,4]        6        1      0.1428571\n\n\nThe information in detection summary tables can be plotted, but, in the interest of space, only one (out of six possible plots) is shown below.\n\n# Plot detection information, change number to see other plots\nplot(detection.tables, which=1)\n\n\n\n\nThe plot numbers are:\n\nHistograms of distances for detections by either, or both, observers. The shaded regions show the number for observer 1.\nHistograms of distances for detections by either, or both, observers. The shaded regions show the number for observer 2.\nHistograms of distances for duplicates (detected by both observers).\nHistogram of distances for detections by either, or both, observers. Not shown for trial configuration.\nHistograms of distances for observer 2. The shaded regions indicate the number of duplicates - for example, the shaded region is the number of clusters in each distance bin that were detected by Observer 1 given that they were also detected by Observer 2 (the “|” symbol in the plot legend means “given that”).\nHistograms of distances for observer 1. The shaded regions indicate the number of duplicates as for plot 5. Not shown for trial configuration.\n\nNote that if an independent observer configuration had been chosen, all plots would be available.\nA summary of the detection function model is available using the summary function. The Q-Q plot has the same interpretation as a Q-Q plot in a conventional, single platform analysis.\n\n# Produce a summary of the fitted detection function object\nsummary(fi.mr.dist)\n\n\nSummary for trial.fi object \nNumber of observations               :  162 \nNumber seen by primary               :  124 \nNumber seen by secondary (trials)    :  142 \nNumber seen by both (detected trials):  104 \nAIC                                  :  452.8094 \n\n\nConditional detection function parameters:\n             estimate        se\n(Intercept)  2.900233 0.4876238\ndistance    -1.058677 0.2235722\n\n                        Estimate          SE         CV\nAverage p              0.6423252  0.04069410 0.06335435\nAverage primary p(0)   0.9478579  0.06109656 0.06445750\nN in covered region  193.0486185 15.84826582 0.08209469\n\n# Produce goodness of fit statistics and a qq plot\ngof.result &lt;- ddf.gof(fi.mr.dist, \n                      main=\"Full independence, trial configuration\\ngoodness of fit Golf tee data\")\n\n\n\ngof.result\n\n\nGoodness of fit results for ddf object\n\nChi-square tests\n\nDistance sampling component:\n          [0,0.4] (0.4,0.8] (0.8,1.2] (1.2,1.6] (1.6,2] (2,2.4] (2.4,2.8]\nObserved   25.000    16.000    16.000    22.000   9.000  10.000    12.000\nExpected   18.068    17.479    16.650    15.527  14.079  12.327    10.361\nChisquare   2.659     0.125     0.025     2.698   1.833   0.439     0.259\n          (2.8,3.2] (3.2,3.6] (3.6,4]   Total\nObserved      9.000     3.000   2.000 124.000\nExpected      8.335     6.419   4.753 124.000\nChisquare     0.053     1.821   1.595  11.508\n\nNo degrees of freedom for test\n\nMark-recapture component:\nCapture History 01\n          [0,0.4] (0.4,0.8] (0.8,1.2] (1.2,1.6] (1.6,2] (2,2.4] (2.4,2.8]\nObserved        1         2         2         6       5       2         6\nExpected        1         2         3         5       4       4         7\nChisquare       0         0         0         0       1       1         0\n          (2.8,3.2] (3.2,3.6] (3.6,4] Total\nObserved          6         2       6    38\nExpected          6         2       5    38\nChisquare         0         0       0     2\nCapture History 11\n          [0,0.4] (0.4,0.8] (0.8,1.2] (1.2,1.6] (1.6,2] (2,2.4] (2.4,2.8]\nObserved       21        15        16        20       8       8         9\nExpected       21        15        15        21       9       6         8\nChisquare       0         0         0         0       0       0         0\n          (2.8,3.2] (3.2,3.6] (3.6,4] Total\nObserved          5         1       1   104\nExpected          5         1       2   104\nChisquare         0         0       0     1\n\n\nTotal chi-square = 14.888  P = 0.60351 with 17 degrees of freedom\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.294564 p-value = 0.140034\n\n# Extract chi-square statistics for reporting in the text below (see Markdown file for how this is done).\nchi.distance &lt;- gof.result$chisquare$chi1$chisq\nchi.markrecap &lt;- gof.result$chisquare$chi2$chisq\nchi.total &lt;- gof.result$chisquare$pooled.chi\n\nThe \\(\\chi^2\\) goodness-of-fit assessment shows the \\(\\chi^2\\) contribution from the distance sampling component to be 11.5 and the \\(\\chi^2\\) contribution from the mark-recapture component to be 3.4. The combination of these elements produces a total \\(\\chi^2\\) of 14.9 with 17 degrees of freedom, resulting in a \\(p\\)-value of 0.604\nThe mark-recapture model detection function can be plotted with the following code:\n\n# Divide the plot region into 2 columns\npar(mfrow=c(1,2))\n# Plot detection functions\nplot(fi.mr.dist)\n\n\n\n\nThe plot headed\n\n“Conditional detection probability” (the right hand plot) shows the proportion of Obs 2’s detections that were detected by Obs 1 (also see the detection tables). The fitted line is the estimated detection probability function for Obs 1 (given detection by Obs 2) - this is the MR model. Dots are estimated detection probabilities for each Obs 1 detection.\n“Observer=1 detections” (left hand plot) shows a histogram of Observer 1 detections with the estimated Observer 1 detection function (from the MR model) overlaid on it and adjusted for the estimated p(0). The dots show the estimated detection probability for all Observer 1 detections. Note that the MR model was not fitted to these data – it’s the conditional data (right hand plot) that was used to fit the model. This plot is just shown to help you diagnose any issues in the fit – as we discussed in class, if there is dependency between observer detections then we’d expect the detection function to decrease slower than the histograms.\n\nIs there evidence of unmodelled heterogeneity? What do these results tell you about the estimates of p (average detection probability) and p(0) (detection probability at 0 distance)?\n\n\nEstimating abundance\nAbundance is estimated using the dht function. In this function, we need to supply information about the transects and survey regions.\n\n# Calculate density estimates using the dht function\ntee.abund &lt;- dht(model=fi.mr.dist, region.table=region, sample.table=samples, \n                 obs.table=obs)\n\n# Print out results in a nice format\nknitr::kable(tee.abund$individuals$summary, digits=2, \n      caption=\"Survey summary statistics for golftees\")\n\n\nSurvey summary statistics for golftees\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\nmean.size\nse.mean\n\n\n\n\n1\n1040\n1040\n130\n229\n6\n1.76\n0.12\n0.07\n3.18\n0.21\n\n\n2\n640\n640\n80\n152\n5\n1.90\n0.33\n0.18\n2.92\n0.23\n\n\nTotal\n1680\n1680\n210\n381\n11\n1.81\n0.15\n0.08\n3.07\n0.15\n\n\n\n\nknitr::kable(tee.abund$individuals$N, digits=2, \n      caption=\"Abundance estimates for golftee population with two strata\")\n\n\nAbundance estimates for golftee population with two strata\n\n\nLabel\nEstimate\nse\ncv\nlcl\nucl\ndf\n\n\n\n\n1\n356.52\n32.35\n0.09\n294.54\n431.53\n17.13\n\n\n2\n236.64\n44.14\n0.19\n147.33\n380.09\n5.06\n\n\nTotal\n593.16\n60.38\n0.10\n478.32\n735.57\n16.06\n\n\n\n\n\n\n\n\nEstimation of p: distance and other explanatory variables\nBelow is a model that includes size, sex and exposure covariates. Please spend a bit of time examining the model coefficients, goodness of fit, plots of the fitted detection functions, etc (you’ll need to write your own code for this).\n\n# Example of adding covariates to MR detection function\nfi.mr.dist.size.sex.exposure &lt;- ddf(method='trial.fi', \n                      mrmodel=~glm(link='logit',formula=~distance+size+sex+exposure),\n                      data=detections, meta.data=list(width=4))\n\nWe can use AIC to compare this with the previous model that just had distance as a covariate. The function AIC works on these objects:\n\nAIC(fi.mr.dist, fi.mr.dist.size.sex.exposure)\n\n                             df      AIC\nfi.mr.dist                    2 452.8094\nfi.mr.dist.size.sex.exposure  5 407.3974"
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#further-analyses-of-golf-tee-data",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#further-analyses-of-golf-tee-data",
    "title": "Analysis of double-platform data",
    "section": "Further analyses of golf tee data",
    "text": "Further analyses of golf tee data\n\nEstimation of p: distance and other explanatory variables\nThe size covariate is the least significant of the covariates in the model fi.mr.dist.size.sex.exposure – its estimate is 0.078 with SE 0.183. So try creating a new model definition and analysis without this covariate. Does it have a lower AIC?\nYou can also try some models with interaction terms - you can specify these in the usual way in the model foruma with a * symbol. Don’t spend too long on full independence models, but move on to the next section.\n\n\nPoint independence\nA less restrictive assumption than full independence is point independence, which assumes that detections are only independent on the transect centre line i.e. at perpendicular distance zero.\nLet’s start by seeing if a simple point independence model is better than a simple full independence one. This requires that a distance sampling (DS) model is specified as well a MR model. Here we try a half-normal key function for the DS model.\n\n# Fit trial configuration with point independence model\npi.mr.dist &lt;- ddf(method='trial', \n                  mrmodel=~glm(link='logit', formula=~distance),\n                  dsmodel=~cds(key='hn'), \n                  data=detections, meta.data=list(width=4))\n\n# Summary pf the model \nsummary(pi.mr.dist)\n\n\nSummary for trial.fi object \nNumber of observations               :  162 \nNumber seen by primary               :  124 \nNumber seen by secondary (trials)    :  142 \nNumber seen by both (detected trials):  104 \nAIC                                  :  140.8887 \n\n\nConditional detection function parameters:\n             estimate        se\n(Intercept)  2.900233 0.4876238\ndistance    -1.058677 0.2235722\n\n                      Estimate         SE         CV\nAverage primary p(0) 0.9478579 0.02409996 0.02542571\n\n\n\nSummary for ds object\nNumber of observations :  124 \nDistance range         :  0  -  4 \nAIC                    :  311.1385 \nOptimisation           :  mrds (nlminb) \n\nDetection function:\n Half-normal key function \n\nDetection function parameters \nScale coefficient(s): \n             estimate         se\n(Intercept) 0.6632435 0.09981249\n\n           Estimate         SE         CV\nAverage p 0.5842744 0.04637627 0.07937412\n\n\nSummary for trial object\n\nTotal AIC value =  452.0272 \n                       Estimate          SE         CV\nAverage p             0.5538091  0.04615832 0.08334697\nN in covered region 223.9038534 22.99246338 0.10268900\n\n# Produce goodness of fit statistics and a qq plot\ngof.results &lt;- ddf.gof(pi.mr.dist, main=\"Point independence, trial configuration\\n goodness of fit Golftee data\")\n\n\n\n\nCompare the results with the corresponding full independence model. Which has the lower AIC? Which has an estimate closer to known true abundance.\n\nCovariates in the DS model\nTo include covariates in the DS detection function, we need to specify an MCDS model as follows:\n\n# Fit the PI-trial model - DS sex and MR distance \npi.mr.dist.ds.sex &lt;- ddf(method='trial', \n                         mrmodel=~glm(link='logit',formula=~distance),\n                         dsmodel=~mcds(key='hn',formula=~sex), \n                         data=detections, meta.data=list(width=4))\n\nUse the summary or AIC function to check the AIC and decide if you are going to include any additional covariates in the detection function.\nNow try a point independence model that has the preferred MR model from your full independence analyses. Which has the lower AIC and bias?\nFeel free to experiment some more with different models. What is your final best model? What is the estimate of p and p(0) for this model? Was all this modelling necessary in this instance, given the value of p(0)? How else could you have obtained a robust estimate of abundance?"
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#data-structure",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#data-structure",
    "title": "Analysis of double-platform data",
    "section": "Data structure",
    "text": "Data structure\nBefore moving on to the second dataset, let’s have a look at the columns in the detections data because, for all mrds analyses, it needs to have a particular structure.\n\n# Check detections\nhead(detections)\n\n   object observer detected distance size sex exposure\n1       1        1        1     2.68    2   1        1\n21      1        2        0     2.68    2   1        1\n2       2        1        1     3.33    2   1        0\n22      2        2        0     3.33    2   1        0\n3       3        1        1     0.34    1   0        0\n23      3        2        0     0.34    1   0        0\n\n\nThe structure of the detection is as follows:\n\neach detected object (in this case the object was a group or cluster of golf tees) is given a unique number in the object column,\neach object occurs twice - once for observer 1 and once for observer 2,\nthe detected column indicates whether the object was seen (detected=1) or not seen (detected=0) by the observer,\nperpendicular distance is in the distance column and cluster size is in the size column (the same default names as for the ds function).\n\nTo ensure that the variables sex and exposure are treated correctly, we defined them as factor variables."
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#crabeater-seal-data",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#crabeater-seal-data",
    "title": "Analysis of double-platform data",
    "section": "Crabeater seal data",
    "text": "Crabeater seal data\nThis analysis is described in Borchers et al. (2006) and Southwell et al. (2007). These data come from a helicopter survey of crabeater seals conducted by the Australian Antarctic Division within the pack-ice seals programme. The helicopter could only operate within a relatively short distance from the ice-breaker ship which acted as its base. The ice-breaker could only go where the pack ice was thin enough and so the aerial transects could not be located at random. This means that design-based estimation was not a valid option and so, in the published analysis, abundance was estimated using density surface modelling. For the purposes of this exercise, we concentrate on detection function estimation and create an artificial region as a device to produce abundance estimates.\nThere were four independent observers in the helicopter, two on each side (front and back). The front observers were considered to be one ‘team’ and the back observers were considered to be the other ‘team’. Various environmental factors were recorded. In addition to perpendicular distance, observer (1=front or 2=back) and cluster size, the following explanatory variables are available:\n\nside – the side of the helicopter from which seal were seen (L and R)\nexp – the experience (in survey hours) of the observer\nfatigue – the number of minutes the observer had been on duty on the current flight\ngscat – group size category (1, 2 and greater than or equal to 3)\nvis – visibility category (Poor, Good and Excellent)\nglare – whether there was glare (Yes or No)\nssmi – a measure of ice cover\naltitude – the height of the aircraft in metres\nobsname – unique identifier of observer\n\nThe data from the survey has been saved in a .csv file. This file is read into R using read.csv.\n\ncrabseal &lt;- read.csv(\"crabbieMRDS.csv\")\nstr(crabseal)\n\n'data.frame':   3480 obs. of  20 variables:\n $ Study.area  : chr  \"Nominal_area\" \"Nominal_area\" \"Nominal_area\" \"Nominal_area\" ...\n $ Region.Label: int  1 1 1 1 1 1 1 1 1 1 ...\n $ Area        : int  1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000 1000000 ...\n $ Sample.Label: chr  \"99A21\" \"99A21\" \"99A21\" \"99A21\" ...\n $ Effort      : num  59.7 59.7 59.7 59.7 59.7 ...\n $ distance    : num  144 144 125 125 421 ...\n $ size        : num  1 1 1 1 1 1 1 1 1 1 ...\n $ object      : int  54464 54464 54465 54465 54466 54466 54467 54467 54468 54468 ...\n $ observer    : int  1 2 1 2 1 2 1 2 1 2 ...\n $ detected    : int  1 1 1 1 0 1 1 1 1 1 ...\n $ side        : chr  \"R\" \"R\" \"L\" \"L\" ...\n $ exp         : num  0 178 212 0 212 ...\n $ fatigue     : num  61.9 61.9 62.6 62.6 62.9 ...\n $ gscat       : int  1 1 1 1 1 1 1 1 1 1 ...\n $ vis         : chr  \"G\" \"G\" \"G\" \"G\" ...\n $ glare       : chr  \"N\" \"N\" \"N\" \"N\" ...\n $ ssmi        : int  79 79 79 79 79 79 79 79 79 79 ...\n $ altitude    : num  43.1 43.1 43.1 43.1 43.1 ...\n $ obsname     : chr  \"YH\" \"HZ\" \"MF\" \"MH\" ...\n $ fold        : int  1 1 2 2 3 3 4 4 5 5 ...\n\n\nIf you’ve used Distance for Windows you might recognize some of the column names from that - indeed the dataset has been exported from Distance for Windows into the .csv file. The first column can be ignored. Region.Label and Area relate to the strata (their names and area - there’s only one in this dataset). Sample.Label and Effort relate to the transects. The columns distance onwards relate to the observations. You can see there are columns for each explanatory variable above, plus one additional one called fold that will enable use to pick a subset of the data (see later on).\nFirst, one thing we should do is turn the explanatory variables that should be fitted as factor variables into factors.\n\ncrabseal$side &lt;- as.factor(crabseal$side)\ncrabseal$vis &lt;- as.factor(crabseal$vis)\ncrabseal$glare &lt;- as.factor(crabseal$glare)\ncrabseal$observer &lt;- as.factor(crabseal$observer)\ncrabseal$obsname &lt;- as.factor(crabseal$obsname)"
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#crabeater-seal-analyses",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#crabeater-seal-analyses",
    "title": "Analysis of double-platform data",
    "section": "Crabeater seal analyses",
    "text": "Crabeater seal analyses\nThe observer teams acted independently and so an ‘independent observer’ (IO) configuration can be specified. The code below fits simple models (i.e. distance only) with the full independence assumption and the point independence assumption. For each model, make a note of the estimated values for \\(p(0)\\) for each observer and the observers combined. Check goodness-of-fit and plot the detection function.\nFit an IO configuration assuming full independence:\n\n# IO configuration - full independence\n# MR model - distance only\n# Truncation 700m\nfi.mr.dist &lt;- ddf(method=\"io.fi\", mrmodel=~glm(link=\"logit\", formula=~distance),\n                 data=crabseal, meta.data=list(width=700))\nsummary(fi.mr.dist)\n\n\nSummary for io.fi object \nNumber of observations   :  1740 \nNumber seen by primary   :  1394 \nNumber seen by secondary :  1471 \nNumber seen by both      :  1125 \nAIC                      :  25681.52 \n\n\nConditional detection function parameters:\n                estimate           se\n(Intercept)  2.107762345 0.0994391199\ndistance    -0.003087713 0.0003159216\n\n                           Estimate           SE          CV\nAverage p                 0.9071952  0.009684936 0.010675692\nAverage primary p(0)      0.8916554  0.007472286 0.008380240\nAverage secondary p(0)    0.8916554  0.007472286 0.008380240\nAverage combined p(0)     0.9882614  0.004402403 0.004454695\nN in covered region    1917.9995344 24.808749358 0.012934700\n\n\nNext fit IO configuration assuming point independence. Specify a half-normal key function for the DS model; again only include perpendicular distance in the MR model.\n\n# IO configuration - point independence\n# MR model - distance only\n# DS model - half normal detection function, no additional covars\n# Truncation at 700m\npi.mr.dist.ds.hn &lt;- ddf(method=\"io\", dsmodel=~cds(key=\"hn\"),\n                 mrmodel=~glm(link=\"logit\", formula=~distance),\n                 data=crabseal, meta.data=list(width=700))\nsummary(pi.mr.dist.ds.hn)\n\n\nSummary for io.fi object \nNumber of observations   :  1740 \nNumber seen by primary   :  1394 \nNumber seen by secondary :  1471 \nNumber seen by both      :  1125 \nAIC                      :  3011.463 \n\n\nConditional detection function parameters:\n                estimate           se\n(Intercept)  2.107762345 0.0994391199\ndistance    -0.003087713 0.0003159216\n\n                        Estimate          SE          CV\nAverage primary p(0)   0.8916554 0.009606424 0.010773696\nAverage secondary p(0) 0.8916554 0.009606424 0.010773696\nAverage combined p(0)  0.9882614 0.002081609 0.002106335\n\n\nSummary for ds object\nNumber of observations :  1740 \nDistance range         :  0  -  700 \nAIC                    :  22314.4 \nOptimisation           :  mrds (nlminb) \n\nDetection function:\n Half-normal key function \n\nDetection function parameters \nScale coefficient(s): \n            estimate         se\n(Intercept) 5.828703 0.02685781\n\n           Estimate         SE         CV\nAverage p 0.5845871 0.01247826 0.02134542\n\n\nSummary for io object\nTotal AIC value :  25325.86 \n\n                        Estimate          SE         CV\nAverage p              0.5777249  0.01239167 0.02144909\nN in covered region 3011.8139214 79.84149372 0.02650944\n\n\nWhich of these models do you prefer?\nIf you have time, try a few models with covariates. However, this is a large dataset and so fitting models and obtaining model summaries can take a long time with complicated models. Given this and the fact that the exercise is just for practice, you may want to work with just a subset of the data – which is where the fold column in the dataset comes in. With this column, each observation is associated with a number from 1 to 10 (they are assigned systematically in order of the data, so the first observation is given number 1, the second 2, …, the 10th 10, the 11th 1 again, the 12th 2, etc). So, for example, you could pick out just the first and fifth ``fold’’ (i.e., just one fifth of the data) with\n\ncrabseal15 &lt;- crabseal[crabseal$fold %in% c(1, 5), ]\n\nand then use this to investigate covariate selection - for example comparing point independence models where the mr model has just distance in it vs one with distance and observer:\n\npi.mr.dist.ds.hn.fold15 &lt;- ddf(method=\"io\", dsmodel=~cds(key=\"hn\"),\n                 mrmodel=~glm(link=\"logit\", formula=~distance),\n                 data=crabseal15, meta.data=list(width=700))\npi.mr.dist.observer.ds.hn.fold15 &lt;- ddf(method=\"io\", dsmodel=~cds(key=\"hn\"),\n                 mrmodel=~glm(link=\"logit\", formula=~distance+observer),\n                 data=crabseal15, meta.data=list(width=700))\nAIC(pi.mr.dist.ds.hn.fold15, pi.mr.dist.observer.ds.hn.fold15)\n\n                                 df      AIC\npi.mr.dist.ds.hn.fold15           3 5068.746\npi.mr.dist.observer.ds.hn.fold15  4 5065.830\n\n\n\nEstimating abundance\nFollowing model criticism and selection, the abundance can be estimated – below we use the simple point independence model on the full dataset. The estimates of abundance for the study area are arbitrary because inference of the study was restricted to the covered region, hence, the estimates of abundance here are artificial. Nevertheless, we illustrate the method to estimate abundance. We require tables of the region, transects and detections – these can easily be created from the data using the checkdata function in the Distance package (the ::: is shorthand for accessing a function without loading a package). Using these tables, Horvitz-Thompson-like1 estimators can be applied to produce estimates of \\(\\hat{N}\\). The use of convert.units=0.001 adjusts the units of perpendicular distance measurement (m) to units of transect effort (km).\n\n# Create tables for estimating abundance \n# Selecting observer==1 ensures that observations in the obs.table are unique \ntables &lt;- Distance:::checkdata(crabseal[crabseal$observer==1, ])\n\n# Estimate abundance in covered region\npi.abund &lt;- dht(model=pi.mr.dist.ds.hn,\n                region=tables$region.table, \n                sample=tables$sample.table, obs=tables$obs.table,\n                se=TRUE, options=list(convert.units=0.001))\n\n# Pretty tables of data summary\nknitr::kable(pi.abund$individuals$summary, digits=3,\n      caption=\"Summary information from crabeater seal aerial survey.\")\n\n\nSummary information from crabeater seal aerial survey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\nmean.size\nse.mean\n\n\n\n\n1\n1e+06\n8594.082\n6138.63\n2053\n118\n0.334\n0.033\n0.097\n1.18\n0.013\n\n\n\n\n\n\n# Pretty tables of estimates of individual abundance\nknitr::kable(pi.abund$individual$N, digits=3,\n      caption=\"Crabeater seal abundance estimates for study area of arbitrary size.\")\n\n\nCrabeater seal abundance estimates for study area of arbitrary size.\n\n\nLabel\nEstimate\nse\ncv\nlcl\nucl\ndf\n\n\n\n\nTotal\n413493.2\n41201.47\n0.1\n339670.9\n503359.5\n128.625\n\n\n\n\n\n\n\nCrabeater seals with MCDS (optional)\nWe can also analyse the crabeater seals data as if it were single platform data (i.e. ignoring that \\(p(0)\\) is less than 1). These data are available in crabbieMCDS.csv.\nThis short exercise guides you through the import of these data into R and fits a simple half-normal detection function examining the possible improvement of the model by incorporating side of plane and visibility covariates (using the full dataset).\n\n# Load Distance for MCDS \nlibrary(Distance)\n# Read in data\ncrab.covar &lt;- read.csv(\"crabbieMCDS.csv\")\n# Check data imported OK\nhead(crab.covar, n=3)\n\n    Study.area Region.Label    Area Sample.Label Effort distance size side\n1 Nominal_area            1 1000000        99A21  59.72   144.49    1    R\n2 Nominal_area            1 1000000        99A21  59.72   125.16    1    L\n3 Nominal_area            1 1000000        99A21  59.72   421.40    1    L\n    exp fatigue gscat vis glare ssmi altitude obsname fold\n1   0.0   61.90     1   G     N   79 43.05763      YH    1\n2 211.7   62.61     1   G     N   79 43.05763      MF    2\n3   0.0   62.86     1   G     N   79 43.05763      MH    3\n\n\nAfter checking that the data have been read into R appropriately, we are ready to fit a detection function.\nAs before, side of plane and visibility are assigned characters and so we need to tell R to treat them as factors.\n\n# Define factor variables\ncrab.covar$side &lt;- as.factor(crab.covar$side)\ncrab.covar$vis &lt;- as.factor(crab.covar$vis)\n\nWith two potential explanatory variables, there are a number of possible models. We start by fitting a detection function with side of plane as a covariate using a half-normal key function.\n\n# Fit HN key function with side of plane\nds.side &lt;- ds(crab.covar, key=\"hn\", formula=~side, truncation=700)\n\nModel contains covariate term(s): no adjustment terms will be included.\n\n\nFitting half-normal key function\n\n\nAIC= 22304.742\n\n\nWe would now like to assess the fit of this function to our data. Two visual assessments are provided by the panels below: histogram and fitted function on the left and Q-Q plot on the right.\n\n# Divide plot region\npar(mfrow = c(1, 2))\n# Create a title for the plot\nplot.title &lt;- \"Two sets of points\\none for each 'side' of plane\"\n# Plot model\nplot(ds.side, pch=19, cex=0.5, main=plot.title)\n# Plot qq plot\ngof.result &lt;- gof_ds(ds.side, lwd = 2, lty = 1, pch = \".\", cex = 0.5)\n# Extract gof statistics\nmessage &lt;- paste(\"CVM GOF p-value=\", round(gof.result$dsgof$CvM$p, 4))\n# Add gof stats to plot\ntext(0.6, 0.2, message, cex=0.5)\n\n\n\n\nHistogram and fitted half-normal detection function on left. Q-Q plot of detection function and data on right.\n\n\n\n\nThe code below fits the model without any covariates.\n\n# Fit HN key function with no covars and no adjustments\nds.nocov &lt;- ds(crab.covar, key=\"hn\", adjustment=NULL, truncation=700)\n\nFitting half-normal key function\n\n\nAIC= 22314.398\n\n\nAIC score for model without covariates is 22314.4 and AIC score for model with side as a covariate is 22304.74 so the model with side as a covariate is preferred.\nWe could also fit further detection functions and contrast the resulting models:\n\nwith visibility only\nwith side of plane and visibility (excluding an interaction).\n\nOut of the four possible models which is to be preferred?\nFurther modelling is possible. For example, we typically allow adjustment terms in CDS analyses (i.e., where there are not covariates), and it is also possible to include adjustment terms in MCDS analyses. Below is a model with half-normal key and AIC-based selection of cosine adjustments. How does the AIC of this model compare with those fitted previously?\n\n# Fit HN key function with no covars and no adjustments\nds.nocov.hncos &lt;- ds(crab.covar, key=\"hn\", adjustment=\"cos\", truncation=700)\n\nStarting AIC adjustment term selection.\n\n\nFitting half-normal key function\n\n\nAIC= 22314.398\n\n\nFitting half-normal key function with cosine(2) adjustments\n\n\nAIC= 22308.645\n\n\nFitting half-normal key function with cosine(2,3) adjustments\n\n\nAIC= 22304.015\n\n\nFitting half-normal key function with cosine(2,3,4) adjustments\n\n\nAIC= 22305.943\n\n\n\nHalf-normal key function with cosine(2,3) adjustments selected.\n\n\nWe could go on to produce abundance estimates from our preferred model using the dht function if we had provided information about the size of the crabeater seal study area.\nRather than fitting an MRDS model, as above, would an MCDS analyses have been adequate?"
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#references",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#references",
    "title": "Analysis of double-platform data",
    "section": "References",
    "text": "References\nBorchers DL, Laake JL, Southwell C and Paxton CGM (2006) Accommodating unmodeled heterogeneity in double-observer distance sampling surveys. Biometrics 62: 371-378\nLaake JL, Borchers DL, Thomas L, Miller DL and Bishop JRB (2019) mrds: Mark-Recapture Distance Sampling. R package version 2.2.1.\nSouthwell C, Borchers DL, Paxton CGM, Burt ML and de la Mare W (2007) Estimation of detection probability in aerial surveys of Antarctic pack-ice seals. Journal of Agricultural, Biological and Environmental Statistics 12:41-54\nThomas L, Buckland ST, Rexstad EA, Laake JL, Strindberg S, Hedley SL, Bishop JRB, Marques TA, and Burnham KP (2010) Distance software: design and analysis of distance sampling surveys for estimating population size. Journal of Applied Ecology 47: 5-14. DOI: 10.1111/j.1365-2664.2009.01737.x"
  },
  {
    "objectID": "11-mrds/R-prac/Pr11-instructions-R.html#footnotes",
    "href": "11-mrds/R-prac/Pr11-instructions-R.html#footnotes",
    "title": "Analysis of double-platform data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(\\hat{N} = \\sum_{i=1}^n \\frac{1}{\\pi_i \\hat{p}_i}\\) where \\(n\\) is the number of observations, \\(\\pi_i\\) is the “coverage probability”, i.e., the probability the object is covered by a transect, and \\(\\hat{p}_i\\) is the estimated detection probability, i.e., the probability the object is detected given it is covered by a transect.↩︎"
  },
  {
    "objectID": "07-stratify/DistWin-prac/Pr7-instructions-DistWin.html",
    "href": "07-stratify/DistWin-prac/Pr7-instructions-DistWin.html",
    "title": "Analysis of stratified data",
    "section": "",
    "text": "Analysis of stratified data\n\n\n\n\n\n\n\n\n\nThe Data\nThe Distance project Stratify exercise.zip contains data from a stratified survey of Antarctic minke whales. The data are “exact” insofar as they are calculated directly from the estimates of radial distance and angle recorded by the observers. While angle boards and reticle binoculars were used for estimation of angles and distances when possible, the transitory nature of cues (usually blows) and the pitch and roll of the vessel, among other things, leads to in errors in estimating angles and distances. Angular errors are typically of the order of a degree or two; the coefficient of variation of distance estimation errors is typically of the order of 10%.\nThe two strata were surveyed by different vessels at the same time. Because the whales tend to be found in high densities against the ice edge, where they feed, densities in southern strata are typically higher than those in northern strata. In fact this is the primary reason for using a stratified survey design. It is also the reason for covering the southern strata more intensely; in this survey the transect length per unit area in the southern stratum, is more than 2.5 times that in the northern stratum.\nHere are pictures of the sort of design used and a typical density gradient. The irregular bottom border is the ice-edge; the “steps” define the boundary between southern and northern strata; dotted lines are transects; solid dots are detections.\n \nAnalysis Exercises\nOpen the project from its archive Stratify exercise.zip. The project contains one analysis specification, called “Full geog stratification”. This is a fully stratified analysis of the data. Seven equal perpendicular distance intervals, truncation at 1.5 nautical miles (nm), and a hazard rate detection function form with no adjustment terms are used to estimate the detection function. As the focus of these exercises is stratification, do not investigate other perpendicular distance intervals and detection function forms; the given models are adequate. Use the Analysis browser to familiarise yourself with the details of this analysis specification.\n\nHaving done that, run the analysis “Full geog stratification”. Look at the results, and note the AIC statistics from each detection function fit.\nTo stratify \\(f(0)\\) or not: Create a new analysis identical to “Full geog stratification” by clicking the New Analysis icon in the Analysis tab of the Project browser after selecting the existing analysis. The new analysis will be a copy of the existing one.\n\nCreate a new model definition for this new analysis by going to the Inputs tab and highlighting the “haz rate+no adj full strat” model, then clicking the New tab. This will copy the existing model definition – modify the new model definition so that f(0) is to be estimated from the pooled strata (click the Detection function x Global cell of the table on the Estimate tab of the Model Definition Properties window you get after clicking New). Give this new model definition a suitable name and then click OK.\nRun the new analysis and look at the output. By comparing the AIC from this analysis with the sum of the AICs from the analysis “Full geog stratification”, and considering the fits of each detection function, decide whether or not to pool strata for estimation of f(0).\n\n\n\n\n\n\nIf you have time, here’s a more difficult question.\n\n\n\n\n\nCreate an analysis without any stratification and estimate density using it. Why is the density estimate so much higher than those from 1. and 2. above?"
  },
  {
    "objectID": "08-covariates/DistWin-prac/Pr8-instructions-DistWin.html",
    "href": "08-covariates/DistWin-prac/Pr8-instructions-DistWin.html",
    "title": "Covariates in the detection function",
    "section": "",
    "text": "Covariates in the detection function\nThis exercise consists of two datasets of increasing difficulty. The first will show you the rudiments of conducting an analysis, while the remaining analyses take you deeper into the heart of understanding multiple covariates. Feel free to complete as many as you like."
  },
  {
    "objectID": "08-covariates/DistWin-prac/Pr8-instructions-DistWin.html#passerine-data-from-marques2007",
    "href": "08-covariates/DistWin-prac/Pr8-instructions-DistWin.html#passerine-data-from-marques2007",
    "title": "Covariates in the detection function",
    "section": "1 Passerine data from Marques et al. (2007)",
    "text": "1 Passerine data from Marques et al. (2007)\nThe data from the Auk paper by (Marques et al., 2007) is also available. It is zipped as the project fTAMAUK07.zip. See if you can produce results comparable to those presented in the manuscript."
  },
  {
    "objectID": "08-covariates/DistWin-prac/Pr8-instructions-DistWin.html#dolphin-sightings-data",
    "href": "08-covariates/DistWin-prac/Pr8-instructions-DistWin.html#dolphin-sightings-data",
    "title": "Covariates in the detection function",
    "section": "2 Dolphin Sightings Data",
    "text": "2 Dolphin Sightings Data\nIn this exercise there are several potential covariates and no ‘right’ answers!\n\n3.1 Reviewing the data\nIn this example we have a sample of eastern tropical Pacific (ETP) offshore spotted dolphin sightings data, collected by observers placed on board tuna vessels (the data were kindly made available to us by the Inter-American Tropical Tuna Commission – IATTC). In the ETP, schools of yellow fin tuna commonly associate with schools of certain species of dolphins, and so vessels fishing for tuna often search for dolphins in the hopes of also locating tuna. For each school detected by the tuna vessels, the observer records the species, sighting angle and distance (later converted to perpendicular distance and truncated at 5 nautical miles), school size, and a number of covariates associated with each detected school. Many of these covariates potentially affect the detection function, as they reflect how the search was being carried out.\nA variety of search methods are used to find the dolphins, but currently the most commonly used are 20x binoculars from the crow’s nest, 20x binoculars from another location on the vessel, from a helicopter, or through “bird radar” (high power radars which are able to detect seabirds flying above the dolphin schools). In the example dataset these are coded as 0, 2, 3, and 5, respectively. Some of these methods may have a wider range of search than the others, and so it is possible that the effective strip width varies according to the method being used.\nFor each sighting the initial cue type is recorded. This may be birds flying above the school, splashes on the water, floating objects such as logs, or some other unspecified cue. In the example they have been coded as 1, 2, 4 and 3, respectively.\nAnother covariate that potentially affects the detection function is sea state, as measured by Beaufort. In rougher conditions (i.e. higher Beaufort levels), visibility and/or detectability may be reduced. For this example, Beaufort levels are grouped into two categories, the first including Beaufort values ranging from 0 to 2 (coded as 1) and the second containing values from 3 to 5 (coded as 2).\nThe sample data encompasses sightings made over a three month period: June, July and August (months 6, 7 and 8, respectively).\nBegin by extracting and opening the project from the archive Dolphin.zip. Once it is open, you will see the Project Browser, from which you can have a look at the data (Data tab).\n\n\n3.2 Analysis of Dolphin Sightings data\nStart by running a set of conventional distance analyses. Are there any problems in the data and if so how might you mitigate them? (Hint – check out the q-q plot, and also try dividing the data into a large number of intervals in the Model Definition | Detection Function | Diagnostics.)\nAs there are a number of potential covariates to be used in this example, try fitting models with different covariates and combinations of the covariates. All of the covariates in this example are factor covariates except cluster size.\nKeep in mind that this is a large dataset (&gt; 1000 observations), and hence estimation may take a while, particularly if you are allowing up to 5 adjustment terms to be fitted. It will be generally more efficient to start fitting models without any adjustment terms, and then adding one at a time if appropriate. Consider also whether to standardize by w or by σ (or try both!).\nYou will likely end up with quite a few models, so think about how you are going to name and organize them in the Project Browser (for analyses) and Analysis Components window (for model definitions)."
  },
  {
    "objectID": "08-covariates/DistWin-prac/Pr8-solution-DistWin.html",
    "href": "08-covariates/DistWin-prac/Pr8-solution-DistWin.html",
    "title": "Covariates in the detection function",
    "section": "",
    "text": "Covariates in the detection function\n\n\n\n\n\nDistance for Windows exercise Solution\n\n\n\nIntroduction to Distance Sampling\n\n\n1 Hawaiian Passerines\nWe provide no sample solution to these data; consult the Marques et al. (2007) reprint.\n2 Analysis of dolphin sightings data\nTo obtain an overall impression of the data it is useful to fit a detection function histogram with many intervals (you may have problems fitting to the maximum number of 30, but 25 intervals should be OK). The spikes in the histogram suggest that the data has been rounded to zero and possibly other values. The q-q plot also indicates problems with the model at zero distances. To mitigate these problems, use the Intervals tab in the Data Filter to pool the data into a few intervals – 10 to 15 intervals work OK.\nFor the MCDS analysis, cluster size was fitted as a continuous variable, whereas, month, Beaufort, cue and search position were fitted as factor variables. Table 3 summarises the results. The number of adjustment terms allowed was limited to a maximum of two. In most cases a half normal function was chosen with either no, or one, adjustment term.\nTable 1. Parameter estimates for the different models. Percentage CVs are given in parentheses. Note that CVs for the model containing cluster size are obtained by bootstrapping.\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCDS\nCluster size\nMonth\nBeaufort\nCue\nSearch\n\n\nAIC\n3365.9\n3359.5\n3362.6\n3366.9\n3368.3\n3339.8\n\n\nesw (nm)\n3.00 (4.5)\n3.08 (1.9)\n3.00 (1.9)\n3.00 (1.9)\n3.00 (1.9)\n2.93 (2.3)\n\n\nDs (clusters per nm2)\n181 (4.5)\n177\n181 (1.9)\n181 (1.9)\n181 (1.9)\n185 (2.3)\n\n\nE[S]\n507 (5.3)\n460\n529 (5.3)\n507 (5.3)\n495 (5.3)\n589 (5.3)\n\n\nD (animals per nm2)\n91965 (7.0)\n81454\n96009 (5.7)\n91921 (5.6)\n89729 (5.6)\n109420 (5.8)\n\n\n\nBased on the AIC, it seems as though the model including search method is best, however, there were warning messages about the detection function fitting and cluster size estimation. Before going on and looking at models which include two covariates, it is worth looking at the search model in more detail. The detection functions have very different scale parameters, for example, the detection function for search method 3 (using a helicopter) has a very wide shoulder and so the scale parameter is very large. This suggests that the observers were seeing everything out to 5 nm and so detection does not decrease with distance as it does with the other methods. One assumption of MCDS is that the perpendicular distance distributions of the covariate factor levels have the same shape. It may be worth refitting the model ignoring the observations made by the helicopter. Data can easily be selected/ignored using the Data filter | Data selection tab. The selection criteria will be of the form ‘[Search method] IN (0,2,5)’"
  }
]