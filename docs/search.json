[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory distance sampling training materials",
    "section": "",
    "text": "This is the landing page for the distance sampling training materials website.\n\nDescribe philosophy of lecture/exercise pairing.\ndifferentiate the two exercise pathways\ndiscuss role of video introduction/summary\nif there are self-assessed tutorials, describe them\ngiscus if it exists https://giscus.app/\n\n\n\n\nOther sundries\n\nmention distancesampling.org\nmention “other workshops” if they exist"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#and-everything-else",
    "href": "index.html#and-everything-else",
    "title": "async2024-2",
    "section": "And everything else",
    "text": "And everything else"
  },
  {
    "objectID": "02-detfns/detfnlanding.html",
    "href": "02-detfns/detfnlanding.html",
    "title": "Fitting detection functions to line transect data",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described in (Anderson & Pospahala, 1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit."
  },
  {
    "objectID": "02-detfns/detfnlanding.html#estimating-density-of-duck-nests",
    "href": "02-detfns/detfnlanding.html#estimating-density-of-duck-nests",
    "title": "Fitting detection functions to line transect data",
    "section": "",
    "text": "The data are the famous “ducknest” data similar to those data described in (Anderson & Pospahala, 1970). Data exist within the Distance package, so emphasis will be upon performing analysis rather than acquiring data.\nYou will do some exploratory analysis of the detection distances, then use the function ds() to fit detection functions to the data. You’ll also use the function convert_units() for properly dealing with different units of measure in the data.\nThe estimated detection functions will be plotted and assessed for goodness of fit."
  },
  {
    "objectID": "02-detfns/truncation-decisions.html",
    "href": "02-detfns/truncation-decisions.html",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "This question arose in our discussion of truncation. The truncation decision is one of the first decisions an analyst will make when performing a distance sampling analysis. Is the truncation decision a decision that has profound consequences and therefore deserve lots of thought (answer: probably not). Let’s see\nI have written a function that simply repeatedly calls ds() to fit a detection function to a data set. Within the function is a for() loop containing the call to ds(). After each call to ds() the results of the fitted model object are stored within a data frame for subsequent plotting. The function retains the point estimate of density and the confidence interval bounds of the estimate.\n\n\nIs presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#the-function",
    "href": "02-detfns/truncation-decisions.html#the-function",
    "title": "Effect of truncation upon density estimates",
    "section": "",
    "text": "Is presented here\n\ntrunc.experiment &lt;- function(mydata, trunc.range=0:25, cu, type=\"line\") {\n  result &lt;- data.frame(est=numeric(),\n                       lcl=numeric(),\n                       ucl=numeric())\n  for (i in seq_along(trunc.range)) {\n    this &lt;- paste0(i-1,\"%\")\n    m &lt;- ds(mydata, key = \"hn\", adj = NULL, convert_units = cu, \n            transect = type, truncation = this)\n    result[i, ] &lt;- m$dht$individuals$D[c(2,5,6)]\n  }\n  return(result)\n}"
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#duck-nest-result",
    "href": "02-detfns/truncation-decisions.html#duck-nest-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Duck nest result",
    "text": "Duck nest result\nThe figure below shows there is very little change in point estimates of duck nest density until ~20% (1/5th) of the data have been truncated. The magnitude of the point estimate changes are minute in comparison to the width of the confidence intervals.\n\ntrange &lt;- 1:26\nplot(trange-1, duck.trunc$est, type=\"p\", \n     ylim=range(c(duck.trunc[,2], duck.trunc[,3])), pch=20,\n     main=\"Duck nest data\\ntruncation experiment\", ylab=\"Nest density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, duck.trunc[trange,2],\n         trange[trange]-1, duck.trunc[trange,3])\n\n\n\n\nEffects of truncation upon density estimates for duck nest data."
  },
  {
    "objectID": "02-detfns/truncation-decisions.html#simulated-line-transect-result",
    "href": "02-detfns/truncation-decisions.html#simulated-line-transect-result",
    "title": "Effect of truncation upon density estimates",
    "section": "Simulated line transect result",
    "text": "Simulated line transect result\nThe value of writing functions is they can be re-used if written with sufficient generality. I re-use the truncation function upon a different data set; the simulated data from Practical 3 where the true density is known to be 79.8 animals per hectare (depicted with horizontal dashed line).\n\ndata(\"LTExercise\")\nsim.trunc &lt;- trunc.experiment(mydata=LTExercise, trunc.range=0:25, cu=cu, type=\"line\")\n\n\nplot(trange-1, sim.trunc$est, type=\"p\", \n     ylim=range(c(sim.trunc[,2], sim.trunc[,3])), pch=20,\n     main=\"Simulated data\\ntruncation experiment\", ylab=\"Density\",\n     xlab=\"Percent data truncated\")\nsegments(trange[trange]-1, sim.trunc[trange,2],\n         trange[trange]-1, sim.trunc[trange,3])\nabline(h=79.8, lwd=2, lty=3)\n\n\n\n\nEffects of truncation upon density estimates for simulated line transect data."
  },
  {
    "objectID": "02-detfns/Pr2-instructions.html",
    "href": "02-detfns/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "In this exercise, we use R (R Core Team, 2019) and the Distance package (Miller et al., 2019) to fit different detection function models to the duck nest data (introduced in Exercise 1) and estimate duck nest density and abundance.\n\nObjectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it from CRAN:\n\ninstall.packages(Distance)\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn &lt;- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "02-detfns/Prac2_solution.html",
    "href": "02-detfns/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks &lt;- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn &lt;- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel       : Half-normal key function \nAIC         :  928.1338 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "02-detfns/detfnlanding.html#lecture-heading",
    "href": "02-detfns/detfnlanding.html#lecture-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Lecture heading",
    "text": "Lecture heading\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#exercise-heading",
    "href": "02-detfns/detfnlanding.html#exercise-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Exercise heading",
    "text": "Exercise heading\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#supplement-heading",
    "href": "02-detfns/detfnlanding.html#supplement-heading",
    "title": "Fitting detection functions to line transect data",
    "section": "Supplement heading",
    "text": "Supplement heading\n\nEffect of truncation upon density estimates\n\nDemonstration of changing truncation distance"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#lecture-materials",
    "href": "02-detfns/detfnlanding.html#lecture-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#exercise-materials",
    "href": "02-detfns/detfnlanding.html#exercise-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "02-detfns/detfnlanding.html#supplemental-materials",
    "href": "02-detfns/detfnlanding.html#supplemental-materials",
    "title": "Fitting detection functions to line transect data",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nEffect of truncation upon density estimates\n\nDemonstration of changing truncation distance"
  },
  {
    "objectID": "03-criticism/criticismlanding.html",
    "href": "03-criticism/criticismlanding.html",
    "title": "Model criticism",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the R code inside the Quarto (.qmd) instructions.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird."
  },
  {
    "objectID": "03-criticism/criticismlanding.html#absolute-and-relative-fit-of-detection-function-models",
    "href": "03-criticism/criticismlanding.html#absolute-and-relative-fit-of-detection-function-models",
    "title": "Model criticism",
    "section": "",
    "text": "This practical continues the theme of fitting detection functions to line transect distance sampling data. Follow along with the R code inside the Quarto (.qmd) instructions.\nYou will proceed to fit a number of key function/adjustment term combinations to the data. You will also experiment with setting truncation distances.\nThe data you are analysing are simulated, where the true density and the true detection function is known. You will see how well your analyses perform in producing confidence intervals that include the true density. Also examine how much variability there is between different detection functions in their density estimates.\nAfter completing that main task, you also have the opportunity to analyse another line transect data set of capercaillie, a native Scottish bird."
  },
  {
    "objectID": "03-criticism/criticismlanding.html#lecture-materials",
    "href": "03-criticism/criticismlanding.html#lecture-materials",
    "title": "Model criticism",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "03-criticism/criticismlanding.html#exercise-materials",
    "href": "03-criticism/criticismlanding.html#exercise-materials",
    "title": "Model criticism",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "03-criticism/criticismlanding.html#supplemental-materials",
    "href": "03-criticism/criticismlanding.html#supplemental-materials",
    "title": "Model criticism",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nA detailed look at model selection\n\nThe two stages of model selection"
  },
  {
    "objectID": "01-intro/introlanding.html#estimating-widehatp_a-with-a-pencil",
    "href": "01-intro/introlanding.html#estimating-widehatp_a-with-a-pencil",
    "title": "Fundamentals of distance sampling",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region."
  },
  {
    "objectID": "01-intro/introlanding.html#lecture-materials",
    "href": "01-intro/introlanding.html#lecture-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "01-intro/introlanding.html#exercise-materials",
    "href": "01-intro/introlanding.html#exercise-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Exercise materials",
    "text": "Exercise materials\n\nExercise\nThis exercise does not involve use of the computer. Instead, you will be graphing the distribution of perpendicular detection distances. You will fit (using nothing but your eye), a detection function curve to the histogram of detection distances.\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "01-intro/introlanding.html#supplemental-materials",
    "href": "01-intro/introlanding.html#supplemental-materials",
    "title": "Fundamentals of distance sampling",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNothing to add to first module\n\nNo supplement"
  },
  {
    "objectID": "01-intro/introlanding.html",
    "href": "01-intro/introlanding.html",
    "title": "Fundamentals of distance sampling",
    "section": "",
    "text": "This exercise is to familiarise you with distribution of perpendicular distances measured in a distance sampling survey. You will complete a histogram from tablular data provided.\nRather than using a computer, use your eye to fit a smooth function to the distribution depicted in the histogram. This forms the basis for estimating \\(\\widehat{P_a}\\); computed as the ratio of the area under the fitted detection function to the area of the rectangle.\nSubsequent calculations produce an estimate of the density of nests in the sampled region."
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html",
    "href": "03-criticism/R-prac/modelsel-demo.html",
    "title": "Demonstration two stages of model selection",
    "section": "",
    "text": "Demonstration\n\n\n\nEastern tropical Pacific dolphin data\nAfter my improvised description of selection of adjustment terms, I thought I should provide a more thorough description via an example. The purpose of the demonstration is to fit models with adjustments to a data set and expose, in detail, all models fitted during the process.\nFor this demonstration, I require a data set with an interesting shape to the histogram. I will not describe the data set, other than to note it contains roughly 1000 detections. We will see this data set again next week. More complete details of the data set, as well as a detailed analysis are in Marques & Buckland (2003)."
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#half-normal-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#half-normal-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Half-normal cosine",
    "text": "Half-normal cosine\n\nhncos &lt;- ds(bino, key=\"hn\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting half-normal key function\n\n\nAIC= 2816.871\n\n\nFitting half-normal key function with cosine(2) adjustments\n\n\nAIC= 2806.01\n\n\nFitting half-normal key function with cosine(2,3) adjustments\n\n\nAIC= 2807.589\n\n\n\nHalf-normal key function with cosine(2) adjustments selected.\n\n\nThree models with the half-normal key are fitted, with the preferred model being the second fitted, namely the model with a single adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hn', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   D --&gt; ID2\n   ID0(hn0&lt;br&gt;AIC=2817)\n   ID1(hn1&lt;br&gt;AIC=2806)\n   ID2(hn2&lt;br&gt;AIC=2808)\n   FIN(hn1)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN"
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#uniform-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#uniform-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Uniform cosine",
    "text": "Uniform cosine\n\nunicos &lt;- ds(bino, key=\"unif\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting uniform key function\n\n\nAIC= 2971.022\n\n\nFitting uniform key function with cosine(1) adjustments\n\n\nAIC= 2811.177\n\n\nFitting uniform key function with cosine(1,2) adjustments\n\n\nAIC= 2808.378\n\n\nFitting uniform key function with cosine(1,2,3) adjustments\n\n\nAIC= 2806.685\n\n\nFitting uniform key function with cosine(1,2,3,4) adjustments\n\n\nAIC= 2808.105\n\n\n\nUniform key function with cosine(1,2,3) adjustments selected.\n\n\nThe same pattern as with the half-normal key, with a small exception. Four models with the uniform key are fitted, with the preferred model being the third fitted, namely the model with a three adjustment term.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='unif', adj='cos')\n   D --&gt; ID1\n   D --&gt; ID2\n   D --&gt; ID3\n   D --&gt; ID4\n   ID1(unif1&lt;br&gt;AIC=2811)\n   ID2(unif2&lt;br&gt;AIC=2808)\n   ID3(unif3&lt;br&gt;AIC=2807)\n   ID4(unif4&lt;br&gt;AIC=2808)\n   FIN(unif3)\n   ID1 --&gt; FIN\n   ID2 --&gt; FIN\n   ID3 --&gt; FIN\n   ID4 --&gt; FIN"
  },
  {
    "objectID": "03-criticism/R-prac/modelsel-demo.html#hazard-rate-cosine",
    "href": "03-criticism/R-prac/modelsel-demo.html#hazard-rate-cosine",
    "title": "Demonstration two stages of model selection",
    "section": "Hazard rate cosine",
    "text": "Hazard rate cosine\n\nhrcos &lt;- ds(bino, key=\"hr\", adj=\"cos\")\n\nStarting AIC adjustment term selection.\n\n\nFitting hazard-rate key function\n\n\nAIC= 2805.467\n\n\nFitting hazard-rate key function with cosine(2) adjustments\n\n\nAIC= 2807.47\n\n\n\nHazard-rate key function selected.\n\n\nTwo models are fitted with the hazard rate key function. The addition of a single adjustment term does not improve the AIC score, so there is no point in fitting a more complex model with additional adjustment terms.\n\n\n\n\n%%{init: {'theme': 'forest' } }%%\ngraph TD\n   D(ds_data, key='hr', adj='cos')\n   D --&gt; ID0\n   D --&gt; ID1\n   ID0(hr0&lt;br&gt;AIC=2805)\n   ID1(hr1&lt;br&gt;AIC=2807)\n   FIN(hr0)\n   ID0 --&gt; FIN\n   ID1 --&gt; FIN\n\n\n\n\n\nThe contestants that emerge from the first round of model competition are:\n\nhalf-normal with 1 adjustment term\nuniform with 3 adjustment terms\nhazard rate with no adjustment terms"
  },
  {
    "objectID": "05-points/pointslanding.html",
    "href": "05-points/pointslanding.html",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "05-points/pointslanding.html#analysis-of-point-transect-data",
    "href": "05-points/pointslanding.html#analysis-of-point-transect-data",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "05-points/pointslanding.html#lecture-materials",
    "href": "05-points/pointslanding.html#lecture-materials",
    "title": "Analysis of point transect data",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "05-points/pointslanding.html#exercise-materials",
    "href": "05-points/pointslanding.html#exercise-materials",
    "title": "Analysis of point transect data",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "05-points/pointslanding.html#supplemental-materials",
    "href": "05-points/pointslanding.html#supplemental-materials",
    "title": "Analysis of point transect data",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html",
    "href": "05-points/R-prac/Prac5_solution.html",
    "title": "Point transect sampling solution",
    "section": "",
    "text": "Solution\n\n\n\nPoint transect analysis exercise"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#truncation-of-20m",
    "href": "05-points/R-prac/Prac5_solution.html#truncation-of-20m",
    "title": "Point transect sampling solution",
    "section": "Truncation of 20m",
    "text": "Truncation of 20m\n\nPTExercise.hn.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hn\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.hr.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"hr\", truncation=20,\n                    convert_units=conversion.factor)\nPTExercise.uf.cos.t20m &lt;- ds(data=PTExercise, transect=\"point\", key=\"unif\", \n                        adjustment=\"cos\", truncation=20,convert_units=conversion.factor)\n\n\n\n\nResults from simulated point transect data.\n\n\n\n\n\n\n\n\n\n\n\n\nDetectionFunction\nAdjustments\nTruncation\nAIC\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\nNone\n34.2\n919.140\n79.630\n0.126\n62.121\n102.074\n\n\nHalf-normal\nNone\n20.0\n764.305\n70.826\n0.157\n51.978\n96.507\n\n\nHazard rate\nNone\n20.0\n767.211\n62.364\n0.187\n43.207\n90.015\n\n\nUniform\nCosine\n20.0\n765.503\n75.044\n0.144\n56.515\n99.648"
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "href": "05-points/R-prac/Prac5_solution.html#plots-of-probability-density-functions-to-inspect-fit",
    "title": "Point transect sampling solution",
    "section": "Plots of probability density functions to inspect fit",
    "text": "Plots of probability density functions to inspect fit\n\nplot(PTExercise.hn, main=\"Half normal, no truncation\", pdf=TRUE)\nplot(PTExercise.hn.t20m, main=\"Half normal, truncation 20m\", pdf=TRUE)\nplot(PTExercise.hr.t20m, main=\"Hazard rate, truncation 20m\", pdf=TRUE)\nplot(PTExercise.uf.cos.t20m, main=\"Uniform with cosine, truncation 20m\", pdf=TRUE)\n\n\n\n\n\n\nHalf normal without truncation\n\n\n\n\n\n\n\nHalf normal 20m truncation\n\n\n\n\n\n\n\n\n\nHazard rate 20m truncation\n\n\n\n\n\n\n\nUniform cosine 20m truncation\n\n\n\n\n\n\nWe see a fair degree of variability between analyses - reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates."
  },
  {
    "objectID": "05-points/R-prac/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "href": "05-points/R-prac/Prac5_solution.html#probability-density-functions-for-bucklands-winter-wren-point-transects",
    "title": "Point transect sampling solution",
    "section": "Probability density functions for Buckland’s winter wren point transects",
    "text": "Probability density functions for Buckland’s winter wren point transects\n\nplot(wren5min.uf.cos.t110, main=\"5 minute count\")\nplot(wrensnap.hr.cos.t110, main=\"Snapshot moment\")\n\n\n\n\n\n\nDetection function 5 minute count\n\n\n\n\n\n\n\nDetection function snapshot\n\n\n\n\n\n\nAs the detection distance histograms indicate, winter wren showed evidence of observer avoidance, more than other species in the Montrave study. We show the detection function graph rather than the PDF to emphasise the evasive movement aspect of the data. If you conduct the goodness of fit test, using gof_ds(), you will find that the models still suitably fit the data."
  },
  {
    "objectID": "04-precision/precisionlanding.html",
    "href": "04-precision/precisionlanding.html",
    "title": "Precision of density estimates",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set."
  },
  {
    "objectID": "04-precision/precisionlanding.html#precision-of-density-estimates",
    "href": "04-precision/precisionlanding.html#precision-of-density-estimates",
    "title": "Precision of density estimates",
    "section": "",
    "text": "This exercise examines the ability to precisely estimate density from line transect data.\nThe data are again simulated and there are two data sets. The first data set you will analyse has transects of varying lengths; increasing from west to east in the study area. In addition to this change in transect lengths, there is an accompanying gradient in animal density, decreasing from west to east. In combination, this leads to transects with very high encounter rates in the west and transects with very low encounter rates in the east.\nYou are to analyse these data in several ways, focusing your attention on the precision associated with each estimation method.\nThere is a second data set that shares the same survey design (short transects in the west, long transects in the east). But in this second data set, there in no accompanying animal density gradient. Perform similar analyses with this data set."
  },
  {
    "objectID": "04-precision/precisionlanding.html#lecture-materials",
    "href": "04-precision/precisionlanding.html#lecture-materials",
    "title": "Precision of density estimates",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "04-precision/precisionlanding.html#exercise-materials",
    "href": "04-precision/precisionlanding.html#exercise-materials",
    "title": "Precision of density estimates",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "04-precision/precisionlanding.html#supplemental-materials",
    "href": "04-precision/precisionlanding.html#supplemental-materials",
    "title": "Precision of density estimates",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nCould use left-truncation\n\nThere is no supplemental material"
  },
  {
    "objectID": "03-criticism/DistWin-prac/temp.html",
    "href": "03-criticism/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "02-detfns/DistWin-prac/temp.html",
    "href": "02-detfns/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "04-precision/DistWin-prac/temp.html",
    "href": "04-precision/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "05-points/DistWin-prac/temp.html",
    "href": "05-points/DistWin-prac/temp.html",
    "title": "Placeholder",
    "section": "",
    "text": "blank file holder"
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html",
    "href": "05-points/R-prac/Pr5-instructions.html",
    "title": "Point transect sampling",
    "section": "",
    "text": "The purpose of this exercise is to analyse point transect survey data: it can sometimes be more difficult than line transect data. In the first problem, the data are simulated and so the true density is known. In the second problem, two different data collection methods were used to survey song birds."
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html#probability-density-function",
    "href": "05-points/R-prac/Pr5-instructions.html#probability-density-function",
    "title": "Point transect sampling",
    "section": "Probability density function",
    "text": "Probability density function\nTo plot the more informative probability density function (pdf), an additional argument is required in the plot() function:\n\nplot(ptdat.hn, pdf=TRUE)"
  },
  {
    "objectID": "05-points/R-prac/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "href": "05-points/R-prac/Pr5-instructions.html#analyse-both-conventional-and-snapshot-data-sets",
    "title": "Point transect sampling",
    "section": "Analyse both conventional and snapshot data sets",
    "text": "Analyse both conventional and snapshot data sets\nWhat to do:\n\nSelect a simple model for exploratory data analysis. Experiment with different truncation distances, \\(w\\), and select a suitable value for each method. Are there any potential problems with any of the data sets?\nTry other models and model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density and corresponding confidence interval for each method."
  },
  {
    "objectID": "02-detfns/R-prac/Pr2-instructions.html",
    "href": "02-detfns/R-prac/Pr2-instructions.html",
    "title": "Line transect estimation using R",
    "section": "",
    "text": "In this exercise, we use R (R Core Team, 2019) and the Distance package (Miller et al., 2019) to fit different detection function models to the duck nest data (introduced in Exercise 1) and estimate duck nest density and abundance.\n\nObjectives\nThe aims of this exercise are to:\n\nLoad the Distance library\nImport a data file\nFit a basic detection function using the ds function\nPlot and examine a detection function\nAssess goodness of fit of the detection function\nFit different detection function forms.\n\n\n\nSurvey data\nAs a reminder of the survey, 20 line transects, each of length 128.75 km, were searched out to a distance of 2.4 metres (Anderson & Pospahala, 1970). Perpendicular distances to detected nests have been provided in a data set named ducknest. The columns in the file ducknest are:\n\nStudy.Area - this is the name of the study, Monte Vista NWR\nRegion.Label - identifier of regions: in this case there is only one region and it is set to ‘Default’\nArea - size of the study region (km\\(^2\\)): here the area is set to zero. The area of the refuge is 47.7 km\\(^2\\) - this is needed to obtain abundance: for the purposes of this exercise, we are interested in fitting detection functions and density rather than abundance.\nSample.Label - line transect identifier (numbered 1-20)\nEffort - length of the line transects (km)\nobject - unique identifier for each duck nest identified\ndistance - perpendicular distance (metres) to each duck nest.\n\nThe distances allow different key functions/adjustments to be fitted in the detection function model and, by including the transect lengths and area of the region, density and abundance can be estimated.\n\n\nUsing the Distance package\nThe Distance package has been installed in RStudio/Cloud. When you work on your own machine, you will need to install it from CRAN:\n\ninstall.packages(Distance)\n\n\n\nAccessing the data\nThe duck nest data are part of the Distance package, so if you have the package installed, the data set can be accessed simply by using the data() function\n\nlibrary(Distance)\ndata(ducknest)\n\nTo look at the first few rows of ducknest type the following command.\n\nhead(ducknest)\n\nThe object ducknest is a dataframe object made up of rows and columns. There is one row for each detected nest: use the function nrow to remind yourself how many detections there are:\n\nnrow(ducknest)\n\n\n\nSummarising the perpendicular distances\nCreate a numerical summary of the distances:\n\nsummary(ducknest$distance)\n\nSimilarly to plot a histogram of distances, the command is:\n\nhist(ducknest$distance, xlab=\"Distance (m)\")\n\n\n\nFitting a simple detection function model with ds\nDetection functions are fitted using the ds function and this function requires a data frame to have a column called distance. We have this in our ducknest data, therefore, we can simply supply the name of the data frame to the function as follows.\n\n\n\n\n\n\nTake care\n\n\n\nA guaranteed way to produce incorrect results from your analysis is to misspecify the units distances are measured. The ds function has an argument convert_units where the user provides a value to report density in proper units. Providing an incorrect value will result in estimates that are out by orders of magnitude.\n\n\nBefore fitting a model, the units of measure within the survey need to be reconciled. We can choose the units in which duck nest density is to be reported, we choose square kilometres. How to import this information to the ds function?\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n\n\n# Fit half-normal detection function, no adjustment terms\nnest.hn &lt;- ds(data=ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\n\nDetails about the arguments for this function:\n\nkey=\"hn\"\n\nfit a half-normal key detection function\n\nadjustment=NULL\n\ndo not include adjustment terms\n\nconvert_units=conversion.factor\n\nrequired because, for this example, the perpendicular distances are in metres and the line transect lengths are in km - this argument converts the perpendicular distance measurements from metres to km.\n\n\nAs we have seen, on executing the ds command some information is provided to the screen reminding the user what model has been fitted and the associated AIC value. More information is supplied if we ask for a summary of the model as follows:\n\n# Summarise model object\nsummary(nest.hn)\n\nCan you match the information with the values you used in Exercise 1 - was your density estimate similar to the one obtained here?\nTo look at the fitted detection function, simply use the plot function:\n\nplot(nest.hn)\n\nThe number of bins in the histogram can be changed by specifying the nc argument, for example, to plot the histogram having 8 bins (as in Exercise 1) we can specify:\n\nplot(nest.hn, nc=8)\n\nThe histogram should look like the one you drew in Exercise 1.\n\n\nGoodness of fit\nPrior to making inference based upon a detection function model, it is prudent to assess the fit of the model. The usual tools for checking goodness of fit are available: the function gof_ds performs goodness of fits tests and plots a QQ-plot. In this command, 8 bins will be used for the chi-square goodness of fit test.\n\ngof_ds(nest.hn)\n\n\n\nSpecifying different detection functions\nDifferent detection function forms and shapes, are specified by changing the key and adjustment arguments.\nThe different options available for key detection functions are:\n\nhalf normal (key=\"hn\") - this is the default\nhazard rate (key=\"hr\")\nuniform (key=\"unif\")\n\nThe different options available for adjustment terms are:\n\nno adjustment terms (adjustment=NULL)\ncosine (adjustment=\"cos\") - default\nHermite polynomial (adjustment=\"herm\")\nSimple polynomial (adjustment=\"poly\")\n\nFor each model specified below, note down the AIC, density and 95% confidence interval and compare it to the model already fitted (i.e. half-normal with no adjustments). Which detection function model would you choose?\nTo fit a uniform key function with cosine adjustment terms, use the command:\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\n\nBy default, AIC selection will be used to fit adjustment terms of up to order 5. Have any adjustment terms been selected?\nTo fit a hazard rate key function with Hermite polynomial adjustment terms, then use the command:\n\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501\n\n\nMiller, D. L., Rexstad, E., Thomas, L., Marshall, L., & Laake, J. L. (2019). Distance Sampling in R. Journal of Statistical Software, 89(1), 1–28. https://doi.org/10.18637/jss.v089.i01\n\n\nR Core Team. (2019). R: A language and environment for statistical computing. Retrieved from https://www.R-project.org"
  },
  {
    "objectID": "02-detfns/R-prac/Prac2_solution.html",
    "href": "02-detfns/R-prac/Prac2_solution.html",
    "title": "Line transect estimation using R solution",
    "section": "",
    "text": "Solution\n\n\n\nLine transect estimation\n\n\n\nInspect duck nest data\nImport and check the data.\n\nlibrary(Distance)\ndata(ducknest)\nhead(ducknest, n=3)\n\n  Region.Label Area Sample.Label Effort object distance      Study.Area\n1      Default    0            1 128.75      1     0.06 Monte Vista NWR\n2      Default    0            1 128.75      2     0.07 Monte Vista NWR\n3      Default    0            1 128.75      3     0.04 Monte Vista NWR\n\nnrow(ducknest)\n\n[1] 534\n\nbrks &lt;- seq(from=0, to=2.4, by=0.3)\nhist(ducknest$distance, breaks=brks, xlab=\"Distance (m)\",\n     main=\"Perpendicular distances duck nests\")\n\n\n\n\n\n\nFit detection functions\nFit the three models using proper units of distance measure.\nThe answer is another function convert_units. Arguments to this function are\n\ndistance_units\n\nunits of measure for perpendicular/radial distances\n\neffort_units\n\nunits of measure for effort (NULL for point transects)\n\narea_units\n\nunits of measure for the study area.\n\n\n\nconversion.factor &lt;- convert_units(\"Meter\", \"Kilometer\", \"Square Kilometer\")\n# Half-normal with no adjustments\nnest.hn &lt;- ds(ducknest, key=\"hn\", adjustment=NULL,\n              convert_units=conversion.factor)\nsummary(nest.hn)\n\n\nSummary for distance analysis \nNumber of observations :  534 \nDistance range         :  0  -  2.4 \n\nModel       : Half-normal key function \nAIC         :  928.1338 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n             estimate        se\n(Intercept) 0.9328967 0.1703933\n\n                       Estimate          SE         CV\nAverage p             0.8693482  0.03902051 0.04488479\nN in covered region 614.2533225 29.19681554 0.04753221\n\nSummary statistics:\n   Region  Area CoveredArea Effort   n  k        ER       se.ER      cv.ER\n1 Default 12.36       12.36   2575 534 20 0.2073786 0.007970756 0.03843576\n\nDensity:\n  Label Estimate       se         cv     lcl      ucl       df\n1 Total 49.69687 2.936724 0.05909274 44.2033 55.87318 99.55677\n\n\nIn addition to the half normal key function, fit uniform and hazard rate models with possible adjustment terms.\n\nnest.uf.cos &lt;- ds(ducknest, key=\"unif\", adjustment=\"cos\",\n                  convert_units=conversion.factor)\nnest.hr.herm &lt;- ds(ducknest, key=\"hr\", adjustment=\"herm\", \n                  convert_units=conversion.factor)\n\n\n\nAssess absolute model fit\nThe goodness of fit for the basic model is shown below.\n\ngof_ds(nest.hn, plot=FALSE)\n\n\nGoodness of fit results for ddf object\n\nDistance sampling Cramer-von Mises test (unweighted)\nTest statistic = 0.0353634 p-value = 0.955416\n\n\n\n\nContrast competing models\nA function useful for contrasting models is summarize_ds_models. A summary table of goodness of fit statistics for all models is created below.\n\n# Summarise gof statistics\nknitr::kable(summarize_ds_models(nest.hn, nest.uf.cos, nest.hr.herm, output=\"plain\"), \n               caption=\"Model results for ducknest data set.\", digits=3)\n\n\nModel results for ducknest data set.\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\nnest.hn\nHalf-normal\n~1\n0.955\n0.869\n0.039\n0.000\n\n\nnest.uf.cos\nUniform with cosine adjustment term of order 1\nNA\n0.821\n0.846\n0.044\n0.346\n\n\nnest.hr.herm\nHazard-rate\n~1\n0.981\n0.889\n0.050\n1.660\n\n\n\n\n\n\n\nDensity estimates from the competing models\nThe density results from all models are summarized below.\n\n\n\nDensity estimates and confidence intervals for three fitted models.\n\n\nModel\nDetectionFunction\nDensity\nLowerCI\nUpperCI\n\n\n\n\n1\nHalf-normal, no adjustments\n49.70\n44.20\n55.87\n\n\n2\nUniform, cosine adjustments\n51.04\n44.92\n58.00\n\n\n3\nHazard rate, no adjustments\n48.59\n42.52\n55.54\n\n\n\n\n\n\n\nVisualise shape of key functions with duck nest data\nThe detection function plots are shown below.\n\nplot(nest.hn, nc=8, main=\"Half normal, no adjustments\")\nplot(nest.uf.cos, nc=8, main=\"Uniform, cosine adjustments\")\nplot(nest.hr.herm, nc=8, main=\"Hazard rate, no adjustments\")\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nUniform with cosine\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\nThe half-normal detection function with no adjustments has the smallest AIC which provides support for this model. The \\(\\Delta\\)AIC values for all three models is small. In general, you should get similar density estimates using different detection function models, provided those models fit the data well, as in this example."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html",
    "href": "03-criticism/R-prac/Pr3-instructions.html",
    "title": "Assessing line transect detection functions",
    "section": "",
    "text": "In Exercise 2, we fitted different detection functions and compared them in terms of goodness of fit and AIC. Here, we continue to fit and assess different models and look at additional arguments in the ds package."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#accessing-the-data",
    "href": "03-criticism/R-prac/Pr3-instructions.html#accessing-the-data",
    "title": "Assessing line transect detection functions",
    "section": "Accessing the data",
    "text": "Accessing the data\nLoad the Distance package to access both the data and analytical functions.\n\n# Load library (if not already loaded)\nlibrary(Distance)\n# Access data\ndata(\"LTExercise\")\n# Check that it has been imported correctly\nhead(LTExercise, n=3)\n\n\nData format: no detections on a transect\nBefore fitting models, it is worth investigating the data a bit further: let’s start by summarising the perpendicular distances:\n\n# Summary of perpendicular distances\nsummary(LTExercise$distance)\n\nThe summary indicates that the minimum distance is min(hndat$distance) and the maximum is max(hndat$distance) metres and there is one missing value (indicated by the NA). If we print a few rows of the data, we can see that this missing value occurred on transect ‘Line 11’.\n\n# Print out a few lines of data\nLTExercise[100:102, ]\n\nThe NA indicates that there were no detections on this transect, but the transect information needs to be included otherwise the number of transects and the total line length will be incorrect."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#truncation",
    "href": "03-criticism/R-prac/Pr3-instructions.html#truncation",
    "title": "Assessing line transect detection functions",
    "section": "Truncation",
    "text": "Truncation\nLet’s start by fitting a basic model, i.e. no adjustment terms (by default a half normal model is fitted).\nFor this project, perpendicular distances are in metres and the transect lines are recorded in kilometres.\n\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"square kilometer\")\n# Fit half normal, no adjustments\nlt.hn &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL,\n            convert_units=conversion.factor)\n\nLooking at a summary of the model object, how many objects are there in total? What is the maximum observed perpendicular distance?\n\n# Print a summary of the fitted detection function\nsummary(lt.hn)\n\nPlot the detection function and specify many histogram bins:\n\nplot(lt.hn, nc=30)\n\nThe histogram indicates that there is a large gap in detections therefore, to avoid a long right hand tail in the detection function, truncation is necessary. There are several ways to truncate: excluding distances beyond some specified distance or excluding a specified percentage of the largest distances. Note that here we only consider excluding large perpendicular distances, which is frequently referred to as right truncation.\n\nTruncation at a fixed distance\nThe following command truncates the perpendicular distances at 20 metres i.e. objects detected beyond 20m are excluded.\n\n# Truncate at 20metres\nlt.hn.t20m &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=20,\n                 convert_units=conversion.factor)\n\nGenerate a summary and plot of the detection function to see what effect this truncation has had on the number of objects.\n\n\nTruncating a percentage of distances\nAn alternative way to truncate distances is to specify a percentage of detected objects that should be excluded. In the command below, 10% of the largest distances are excluded.\n\n# Truncate largest 10% of distances\nlt.hn.t10per &lt;- ds(data=LTExercise, key=\"hn\", adjustment=NULL, truncation=\"10%\",\n                   convert_units=conversion.factor)\n\nGenerate a summary and plot to see what effect truncation has had.\n\nsummary(lt.hn.t10per)\nplot(lt.hn.t10per)"
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#exploring-different-models",
    "href": "03-criticism/R-prac/Pr3-instructions.html#exploring-different-models",
    "title": "Assessing line transect detection functions",
    "section": "Exploring different models",
    "text": "Exploring different models\nDecide on a suitable truncation distance (but don’t spend too long on this) and then fit different key detection functions and adjustment terms to assess whether these data can be satisfactorily analysed with the ‘wrong’ model. By default, the ds function fits a half normal function and cosine adjustment terms (adjustment=\"cos\") of up to order 5: AIC is used to determine how many, if any, adjustment terms are required. This model is specified in the command below:\n\n# Half normal detection, cosine adjustments, no truncation\nlt.hn.cos &lt;- ds(data=LTExercise, key=\"hn\", adjustment=\"cos\",\n                convert_units=conversion.factor)\n\nChange the key and adjustment terms: possible options are listed in Exercise 2 or use the help(ds) for options. See how the bias and precision compare between the models: the true density was 79.8 animals per km\\(^2\\)."
  },
  {
    "objectID": "03-criticism/R-prac/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "href": "03-criticism/R-prac/Pr3-instructions.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nSometimes we wish to convert exact distances to binned distances, if for example, there is evidence of rounding to favoured values. To do this in ds we need to specify the cutpoints of the bins, including zero and the maximum distance. In the example below, cutpoints at 0, 10, 20, …, 80 are specified.\n\ndata(capercaillie)\n# Specify cutpoint for bins\nbins &lt;- seq(from=0, to=80, by=10)\nconversion.factor &lt;- convert_units(\"meter\", \"kilometer\", \"hectare\")\n# Fit model with binned distances\ncaper.bin &lt;- ds(data=capercaillie, key=\"hn\", cutpoints=bins, \n                convert_units=conversion.factor)\nplot(caper.bin)"
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html",
    "href": "03-criticism/R-prac/Prac3_solution.html",
    "title": "Assessing line transect detection functions solution",
    "section": "",
    "text": "Solution\n\n\n\nAssessing line transect detection functions"
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "href": "03-criticism/R-prac/Prac3_solution.html#fitting-multiple-models-to-exact-distance-data",
    "title": "Assessing line transect detection functions solution",
    "section": "Fitting multiple models to exact distance data",
    "text": "Fitting multiple models to exact distance data\n\n# Half normal model \ncaper.hn.cos &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Hazard rate model  \ncaper.hr.cos &lt;- ds(data=capercaillie, key=\"hr\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n# Uniform model  \ncaper.uf.cos &lt;- ds(data=capercaillie, key=\"unif\", adjustment=\"cos\",\n                   convert_units=conversion.factor)\n\nThe detection functions and QQ plots are shown below:\n\nplot(caper.hn.cos, main=\"Half normal\")\nx &lt;- gof_ds(caper.hn.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.hr.cos, main=\"Hazard rate\")\nx &lt;- gof_ds(caper.hr.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\nplot(caper.uf.cos, main=\"Uniform\")\nx &lt;- gof_ds(caper.uf.cos)\ntext(.5, .1, paste(\"P-value=\", round(x$dsgof$CvM$p,3)))\n\n\n\n\n\n\nHalf normal\n\n\n\n\n\n\n\nQQ plot half normal\n\n\n\n\n\n\n\n\n\nHazard rate\n\n\n\n\n\n\n\nQQ plot hazard rate\n\n\n\n\n\n\n\n\n\nUniform with adjustment\n\n\n\n\n\n\n\nQQ plot uniform adj\n\n\n\n\n\n\nSummarise the goodness of fit statistics (in a pretty format). This table indicates that the hazard rate detection function had the lowest AIC but the difference in AIC between all three models was small.\n\nknitr::kable(summarize_ds_models(caper.hn.cos, caper.hr.cos, caper.uf.cos, output=\"plain\"),\n               caption=\"Summary of results of Capercaillie analysis.\", digits = 3)\n\n\nSummary of results of Capercaillie analysis.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nKey function\nFormula\nC-vM \\(p\\)-value\nAverage detectability\nse(Average detectability)\nDelta AIC\n\n\n\n\n2\ncaper.hr.cos\nHazard-rate\n~1\n0.663\n0.703\n0.052\n0.000\n\n\n1\ncaper.hn.cos\nHalf-normal\n~1\n0.332\n0.613\n0.053\n0.031\n\n\n3\ncaper.uf.cos\nUniform with cosine adjustment terms of order 1,2\nNA\n0.613\n0.682\n0.098\n0.280\n\n\n\n\n\nThe results for the three different models are shown below: density is in birds per ha.\n\n\n\nCapercaillie point estimates of density and associated measures of precision.\n\n\nDetectionFunction\nAIC\nPa\nDensity\nD.CV\nLower.CI\nUpper.CI\n\n\n\n\nHalf-normal\n957.905\n0.613\n0.048\n0.148\n0.027\n0.083\n\n\nHazard rate\n957.874\n0.703\n0.042\n0.148\n0.020\n0.084\n\n\nUniform\n958.154\n0.682\n0.043\n0.191\n0.026\n0.069\n\n\n\n\n\nThese capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results."
  },
  {
    "objectID": "03-criticism/R-prac/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "href": "03-criticism/R-prac/Prac3_solution.html#converting-exact-distances-to-binned-distances",
    "title": "Assessing line transect detection functions solution",
    "section": "Converting exact distances to binned distances",
    "text": "Converting exact distances to binned distances\nTo deal with rounding in the distance data, the exact distances can be converted into binned distances. The cutpoints need to be chosen with care so that the distance bins are sufficiently wide enough to ensure that the ‘correct’ perpendicular distance is in the band containing the rounded recorded value. The bin widths do not have to be equal, as shown in example here: the cutpoints are 0, 7.5, 17.5, 27.5, …, 67.5, 80.0 m. Note, that any distances beyond the largest bin will be excluded.\n\n# Specify (uneven) cutpoint for bins\nbins &lt;- c(0, seq(from=7.5, to=67.5, by=10), 80)\nprint(bins)\n\n[1]  0.0  7.5 17.5 27.5 37.5 47.5 57.5 67.5 80.0\n\ncaper.hn.bin &lt;- ds(data=capercaillie, key=\"hn\", adjustment=\"cos\", cutpoints=bins,\n                   convert_units=conversion.factor)\nplot(caper.hn.bin, main=\"Capercaillie, binned distances\")\n\n\n\n# See a portion of the results\nknitr::kable(caper.hn.bin$dht$individuals$summary, row.names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegion\nArea\nCoveredArea\nEffort\nn\nk\nER\nse.ER\ncv.ER\nmean.size\nse.mean\n\n\n\n\nMonaughty Forest\n1472\n3840\n240\n112\n1\n0.4666667\n0\n0\n1\n0\n\n\n\n\nknitr::kable(caper.hn.bin$dht$individuals$D[1:6], row.names = FALSE, digits=3)\n\n\n\n\nLabel\nEstimate\nse\ncv\nlcl\nucl\n\n\n\n\nTotal\n0.045\n0.007\n0.152\n0.026\n0.079\n\n\n\n\n\nNote that the binning of the data results in virtually identical estimates of density (0.045 birds per ha) and essentially no change in the precision of the density estimate compared with the estimates with analysis of exact distance data."
  },
  {
    "objectID": "04-precision/R-prac/Pr4-instructions.html",
    "href": "04-precision/R-prac/Pr4-instructions.html",
    "title": "Variance estimation for systematic survey designs",
    "section": "",
    "text": "In the lecture describing measures of precision, we explained that systematic survey designs usually have the best variance properties, but obtaining good estimates of the variance is a difficult problem for statisticians. In this exercise, we give an example of a situation where the systematic design gives a density estimate with much better precision than a random design. This means that the usual variance estimators used in the ds function, which are based on random transect placement, are far too high. The true variance is low, but the estimated variance is high.\nWe will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance. In Section 6, we examine another case to see that the unstratified variance estimates provided by ds are usually fine for a systematic design: things only go wrong when there are strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g. the highest densities always occur on the shortest lines, or vice versa).\nWe begin with a population and survey shown below. The data used for this exercise were simulated on a computer: they are not real data. Note the characteristics for the data in Figure 1: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design has covered a fairly large proportion of the survey area (the covered region is shaded). These are danger signals that the usual ds variance estimators might not work well and a post-stratification scheme should be considered.\n\n\n\n\n\nAn example of survey data where there is a strong trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\n\nObjectives\nThe aims of this exercise are to illustrate:\n\nDefault variance estimation,\nVariance estimation with bootstrapping,\nPost-stratification to improve variance estimation,\nWhen post-stratification is not needed (optional).\n\n\n\nGetting started\nDon’t forget to load the Distance package for your session.\n\nlibrary(Distance)\n\n\n\nBasic (default) variance estimation\nIn the code below, the necessary data file is imported and a simple model is fitted and a summary produced. Make a note of the CV of the density estimate - this is obtained using the default (analytical) estimator in the ds function and is based on the assumption that the lines were placed at random. This CV can then be compared with the CV estimates obtained from alternative methods.\n\n# Import data\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\n# Fit a simple model\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=NULL,\n                 convert_units=conversion.factor)\n# Summary\nprint(sysvar2.hn$dht$individuals$D)\nprint(sysvar2.hn$dht$individuals$N)\n\nThe true density and abundance are known (because the data were simulated): the true abundance in the survey region was \\(N=1000\\) and \\(D=2000 \\textrm{ animals per km}^2\\) (i.e. 1000 animals in an area of size \\(A=0.5 \\textrm{km}^2\\)). How do the point estimates compare with truth? What do you think about the precision of the estimates?\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\n# Bootstrap estimate of uncertainty\n# Run the bootstrap (this can take a while if nboot is large!) \nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize,\n                    convert_units=conversion.factor, nboot=199)\n\nThe arguments for this command are:\n\nmodel - fitted detection function model object\nflatfile - data frame of the survey data\nsummary_fun - function used to obtain the summary statistics from each bootstrap\nconvert_units - conversion units for abundance estimation\nnboot - number of bootstrap samples to generate. Note, it can take a long time to produce a large number of bootstraps and so perhaps try a small number at first.\n\n\n# See the results\nsummary(est.boot)\n\nThe summary includes:\n\nEstimate - the median value of the bootstrap estimates\nse is the standard deviation of the bootstrap estimates\nlcl and ucl are the limits for a 95% confidence interval.\ncv is the coefficient of variation (\\(CV = SE/Estimate\\))\n\nAre the bootstrapped confidence intervals for abundance and density similar to the analytical confidence intervals produced previously?\nRecall that we have a particular situation in which we have systematically placed transects which are unequal in length. Furthermore, there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study region with the highest density. In the next section, we examine a process by which we can use post-stratification to produce a better estimate of the variance in estimated abundance.\n\n\nPost-stratification to improve variance estimation\nThe estimation of encounter rate variance in the previous section used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic and, in some circumstances, taking this in account can substantially reduce the encounter rate variance. The data we are working with is an example of this, where there are very high densities on the very shortest lines. In samples of lines, collected using a completely random design, the sample, by chance, might not contain any very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance. Although there is no way of getting a variance estimate that is exactly unbiased for a systematic sample because it is effectively a sample of size 1- only the first line position was randomly chosen and the rest followed on deterministically from there. We can greatly improve on the random-based estimate by using a post-stratification scheme.\nThe post-stratification scheme works by grouping together pairs of adjacent lines from the systematic sample and each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample. This encounter rate estimator is called ‘O2’ (Fewster et al., 2009) and is implemented in the dht2 function.\n\n# Post-stratification - stratified variance estimation by grouping adjacent transects\n\n# Ensure that Sample.Labels are numeric, this is required for O2 ordering\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\n\n# Use the Fewster et al 2009, \"O2\" estimator \nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, strat_formula=~1, \n               convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nNote that this estimator assumes that the numbering of the transects (in this example Sample.Label takes values 1 to 20) has some geographical meaning (i.e. transect 1 is next to 2 and 2 is next to 3 etc.). If this is not the case, then the user can manually define some sensible grouping of transects and create a column called grouping in the data object.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThe simulated population shown in Figure 2 does not exhibit strong trends across the survey region, otherwise, the strip dimensions and systematic design are the same as for the previous example. These data are stored in the data set Systematic_variance_1.\n\n\n\n\n\nAn example of survey data that does not exhibit a trend in density. The systematically placed search strips are shaded. Axis units are in kilometres.\n\n\n\n\nIn the code below, these data are imported into R and a simple detection function model is fitted. The default estimate of variance is then compared to that obtained using the ‘O2’ estimator (Fewster et al., 2009).\n\n# When post-stratification is not needed\n# Import the data\ndata(\"Systematic_variance_1\")\n# Ensure that Sample.Labels are numeric, for O2 ordering\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\n# First fit a simple model\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\n# Obtain default estimates for comparison\nprint(sysvar1.hn$dht$individuals$D)\nprint(sysvar1.hn$dht$individuals$N)\n# Now use Fewster et al 2009, \"O2\" estimator \nest1.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1, \n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est1.O2, report=\"both\")\n\nDid you see a difference in the CV and 95% confidence interval between the two estimators?\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "04-precision/R-prac/Prac4_solution.html",
    "href": "04-precision/R-prac/Prac4_solution.html",
    "title": "Variance estimation for systematic survey designs solution",
    "section": "",
    "text": "Solution\n\n\n\nVariance estimation for systematic designs\n\n\n\nBasic (default) variance estimation\nRecall the data for this example, in which we have a strong gradient in animal density across our study region and at the same time we have a difference in the lengths of the transects, such that short transects are in regions of high animal density and long transects are in regions of low animal density.\n\nlibrary(Distance)\ndata(\"Systematic_variance_2\")\nconversion.factor &lt;- convert_units(\"metre\", \"kilometre\", \"square kilometre\")\nsysvar2.hn &lt;- ds(data=Systematic_variance_2, key=\"hn\", adjustment=\"cos\",\n                 convert_units=conversion.factor)\nprint(sysvar2.hn$dht$individuals$D)\n\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 2044.592 566.3958 0.2770214 1161.012 3600.614 20.74468\n\nprint(sysvar2.hn$dht$individuals$N)\n\n  Label Estimate       se        cv     lcl      ucl       df\n1 Total 1022.296 283.1979 0.2770214 580.506 1800.307 20.74468\n\n\nThe point estimates are good (\\(\\hat D = 2,044\\) animals per unit area and \\(\\hat N=1,022\\) - note the size of the area) but the precision obtained with the default estimator is poor: estimated abundance ranges from about 580 to 1,800 - a three-fold difference over which we are uncertain. Given that our survey covered 40% of the triangular region and had a good sample size (254 animals on 20 transects), this would be a disappointing result in practice.\n\n\nVariance estimation with bootstrapping\nThe function bootdht_Nhat_summarize pulls out the estimates of abundance \\((\\hat{N_i})\\) for all bootstrap replicates \\(i = 1, \\cdots, N_{boot}\\) where \\(N_{boot}\\) is the number of replicates requested.\nThe following command performs the bootstrap.\n\nest.boot &lt;- bootdht(model=sysvar2.hn, flatfile=Systematic_variance_2,\n                    summary_fun=bootdht_Nhat_summarize, \n                    convert_units=conversion.factor, nboot=100)\n\n\nsummary(est.boot)\n\nBootstrap results\n\nBoostraps          : 100 \nSuccesses          : 100 \nFailures           : 0 \n\n     median   mean     se    lcl    ucl   cv\nNhat 985.58 981.06 254.52 550.31 1508.1 0.26\n\n\nThe bootstrap results are very similar to the analytical results, as we would expect, because again this process assumed the transects were placed at random.\n\n\nPost-stratification to improve variance estimation\n\nSystematic_variance_2$Sample.Label &lt;- as.numeric(Systematic_variance_2$Sample.Label)\nest.O2 &lt;- dht2(sysvar2.hn, flatfile=Systematic_variance_2, \n               strat_formula=~1, convert_units=conversion.factor, er_est=\"O2\")\nprint(est.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k     ER se.ER cv.ER\n  Total  0.5      0.1922   9.61 254 20 26.431 1.459 0.055\n\nDensity estimates:\n .Label Estimate      se   cv      LCI      UCI     df\n  Total 2044.592 162.914 0.08 1744.988 2395.636 75.871\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     52.03 47.97\n\n\nThe precision of the estimated abundance has greatly improved in the post-stratified analysis (Fewster et al., 2009).\nIt must be remembered that we have not made any change to our data by the post-stratification; we are using getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be as extreme as they are in this example and post-stratification will not make a great difference. Such an situation is illustrated in the next problem.\n\n\nSystematic designs where post-stratification is not needed (optional)\nThese data did not exhibit strong trends across the survey region and, hence, there are no great differences between the CVs and 95% confidence intervals using the two methods.\n\n# Access the data\ndata(\"Systematic_variance_1\")\nSystematic_variance_1$Sample.Label &lt;- as.numeric(Systematic_variance_1$Sample.Label)\nsysvar1.hn &lt;- ds(Systematic_variance_1, key=\"hn\", adjustment=NULL, \n                 convert_units=conversion.factor)\nprint(sysvar1.hn$dht$individuals$D)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 1954.016 160.5554 0.08216688 1657.276 2303.888 50.59541\n\nprint(sysvar1.hn$dht$individuals$N)\n\n  Label Estimate       se         cv      lcl      ucl       df\n1 Total 977.0078 80.27768 0.08216688 828.6378 1151.944 50.59541\n\nest2.O2 &lt;- dht2(sysvar1.hn, flatfile=Systematic_variance_1, strat_formula=~1,\n                convert_units=conversion.factor, er_est=\"O2\")\nprint(est2.O2, report=\"density\")\n\nDensity estimates from distance sampling\nStratification : geographical \nVariance       : O2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n .Label Area CoveredArea Effort   n  k    ER se.ER cv.ER\n  Total  0.5      0.2058  10.29 252 20 24.49 1.594 0.065\n\nDensity estimates:\n .Label Estimate      se    cv      LCI      UCI     df\n  Total 1954.015 162.491 0.083 1653.804 2308.723 49.172\n\nComponent percentages of variance:\n .Label Detection    ER\n  Total     38.76 61.24\n\n\n\n\n\n\n\n\n\n\nReferences\n\nFewster, R. M., Buckland, S. T., Burnham, K. P., Borchers, D. L., Jupp, P. E., Laake, J. L., & Thomas, L. (2009). Estimating the encounter rate variance in distance sampling. Biometrics, 65(1), 225–236. https://doi.org/10.1111/j.1541-0420.2008.01018.x"
  },
  {
    "objectID": "01-intro/Pr1-instructions.html",
    "href": "01-intro/Pr1-instructions.html",
    "title": "Line transect detection function fitting",
    "section": "",
    "text": "In this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA (Anderson & Pospahala, 1970). Twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nTable 1. Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nObjectives\nThe aim of this exercise is to plot a histogram of the perpendicular distances to the detected duck nests and estimate (by eye) a detection function and hence estimate density of duck nests, i.e. the number of nests per square metre or per square kilometre (be careful of units).\n\n\nAnswer these questions in sequence\nThese steps will produce an estimate of duck nest density.\n\nWith the graph paper PDF found at this link, print the PDF and plot a histogram of the data in Table 1 and fit a detection function by eye.\nEstimate the areas under the rectangle and the fitted detection function curve and hence estimate the proportion of nests that are detected in the covered region, i.e. the region within 2.4m of the transect centre line.\n\n\\[ Area_{rectangle} = \\] \\[ Area_{curve} = \\]\n\\[ \\hat{P}_a = \\frac{Area_{curve}}{Area_{rectangle}} = \\]\n\nObtain an estimate of the number of nests in the covered region (Note \\(n=534\\)):\n\n\\[ \\hat{N}_a = \\frac{n}{\\hat{P}_a} = \\]\n\nEstimate density (Note \\(L = 20 \\times 128.75 = 2575\\) km):\n\n\\[\\hat{D} = \\frac{\\hat{N}_a}{a} = \\frac{\\hat{N}_a}{2wL} = \\]\n\n\n\n\n\n\nReferences\n\nAnderson, D. R., & Pospahala, R. S. (1970). Correction of bias in belt transect studies of immotile objects. The Journal of Wildlife Management, 34(1), 141–146. https://doi.org/10.2307/3799501"
  },
  {
    "objectID": "01-intro/Prac1_solution.html",
    "href": "01-intro/Prac1_solution.html",
    "title": "Line transect detection function fitting solution",
    "section": "",
    "text": "Solution\n\n\n\nEstimation of duck nest density by hand\n\n\nIn this practical, we plot a histogram of line transect data and estimate a detection function. The data were collected during a line transect survey of duck nests in Monte Vista National Wildlife Refuge, Colorado, USA: twenty lines of 128.75 km were specified and a distance out to 2.4m was searched and the perpendicular distances of detected nests were recorded and summarised (Table 1).\n\n\n\n\nTable 1. Frequency of duck nests detected in perpendicular distance bands (metres)\n \n  \n    Distance.band \n    Frequency \n  \n \n\n  \n    0.0-0.3 \n    74 \n  \n  \n    0.3-0.6 \n    73 \n  \n  \n    0.6-0.9 \n    79 \n  \n  \n    0.9-1.2 \n    66 \n  \n  \n    1.2-1.5 \n    78 \n  \n  \n    1.5-1.8 \n    58 \n  \n  \n    1.8-2.1 \n    52 \n  \n  \n    2.1-2.4 \n    54 \n  \n\n\n\n\n\n\nHistogram of detected nests (black) overlaid with the estimated detection function (red) is shown below.\n\n\n\n\n\n\n\nTo estimate the area under the curve, I read off the heights of the mid points of my fitted curve (red) as follows: 75, 74, 72, 70, 66, 62, 58, 53. Therefore, my estimate of area under the curve is:\n\n\\[ Area_{curve} = (75+74+72+70+66+62+58+53) \\times 0.3 = 530 \\times 0.3 = 159 \\] There are lots of other ways to work out the area under a curve, e.g. counting the number of grid squares under the curve on your graph paper or using the trapezoidal rule.\n\\[Area_{rectangle} = height \\times width = 75 \\times 2.4 = 180\\]\nHence, my estimate of the proportion of nests detected in the covered region is:\n\\[\\hat P_a = \\frac{159}{180} = 0.883\\]\n\nHow many actual nests were there in the covered area? I saw 534 nests, and I estimate the proportion seen is 0.883, so my estimate of nests in the covered region is:\n\n\\[ \\hat N_a = \\frac{n}{\\hat P_a} =\\frac{534}{0.883} = 604.7 \\textrm{ nests in the covered area}\\] This estimate is for a covered area of \\(a = 2wL = 2 \\times (\\frac{2.4}{1000}) \\times 2575 = 12.36\\) km\\(^2\\).\n\nI therefore estimate nest density as:\n\n\\[\\hat D = \\frac{\\hat N_a}{2wL} = \\frac{604.7}{12.36} = 48.9 \\textrm{ nests per km}^2\\]"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "",
    "text": "Introduction to Distance Sampling with duck nest data"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#working-on-the-computer",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#working-on-the-computer",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Working on the computer",
    "text": "Working on the computer\n\nInstall the Distance software:\n\nDownload and install the Distance software from http://distancesampling.org/Distance/distance75download.html, if you have not already.\n\nAcquire the Distance project for this exercise:\n\nClick on this link to access data to be used in this exercise. You will be asked to save this file to your local computer. Remember the location where you saved the file, as you will be opening it in the next step.\n\nWe are going to work with a project called “Ducknest exercise”. It contains the same data you previously analysed by hand. Open the Distance software and select File followed by Open project. Under “Files of type” choose Zip archive files (*.zip).\n\n\n\n\n\n\n\n\nOther Distance projects\n\n\n\n\n\nThe Sample Projects folder created when you installed the Distance software, contains a number of other data sets, not related to this training workshop. You may want to look at those projects at a later date.\n\n\n\n\nDouble click on Ducknest exercise.zip. Click OK to unpack the project into the current directory and open it. Next time you open the project, you can open the file Ducknest exercise.dst directly."
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#examining-the-data",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#examining-the-data",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Examining the data",
    "text": "Examining the data\n\nClick on the Data tab of the Project Browser to show the Data Explorer. Look at the data structure, and in particular how the distance data have been entered. (You will need to click on Observation in the left hand pane of the Data Explorer to see this.)"
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#studying-the-first-analysis",
    "href": "02-detfns/DistWin-prac/Pr2-instructions-DistWin.html#studying-the-first-analysis",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "Studying the first analysis",
    "text": "Studying the first analysis\n\nClick on the Analysis tab of the Project Browser to show the Analysis Browser. You should see one analysis listed, called “Half-normal no adjustments.” Double-click on the grey status button for this analysis to open the Analysis Inputs tab for this analysis (you can do the same thing by clicking the 3rd button after “Analysis:” on the Analysis Browser menu bar, or by choosing Analyses then Analysis Details… from the menu bar at the top).\n\n\n\nA grey status icon indicates that this analysis has yet to be run. Click on the Run button in order to run the analysis. The Results tab should turn green.\nClick on the Results tab to see the results, and use the Next &gt; button to move through the pages of results, looking at each page and trying to relate the analysis given here to the one you did by hand. (Note: These are the analysis details (Inputs/Log/Results) for one analysis – you can resize this window so that you can view details from multiple analyses when you have more than one analysis to examine)."
  },
  {
    "objectID": "02-detfns/DistWin-prac/Pr2-solution-DistWin.html",
    "href": "02-detfns/DistWin-prac/Pr2-solution-DistWin.html",
    "title": "Line transect estimation using Distance for Windows software",
    "section": "",
    "text": "Introduction to Distance Sampling with duck nest data\n\n\n\n\n\nDistance for Windows Solution\n\n\n\n\nWatch the solutions video, where we work through the exercise step by step.\nWhen you have the Ducknest project open in Distance and click on the Data tab, your screen should look like this:\n\n\n\nOnce you’ve run the analysis, and open the Analysis Details window for that analysis, your screen should look something like this (see below). Distance gets a density estimate of 49.697 nests km-2, similar to the value we obtained by hand of 48.9 nests km-2."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html",
    "title": "Analysis of point transect survey data",
    "section": "",
    "text": "Analysis of point transect survey data"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html",
    "title": "Variance estimation for systematic designs",
    "section": "",
    "text": "Variance estimation for systematic designs\nRecall the situation in which we have a strong gradient in animal density across our study region, and at the same time we also have a difference in the lengths of our transects; such that short transects are in areas of high animal density, and long transects are in areas of low animal density."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "title": "Variance estimation for systematic designs",
    "section": "Basic variance estimation, with bootstrapping",
    "text": "Basic variance estimation, with bootstrapping\n\nThe precision is very poor: estimated density ranges from about 1000 to 3600: a three-and-a half-fold difference over which we are uncertain. Given that our survey covered 40% of the triangle region, and had a good sample size (254 on 20 transects), this would be a very disappointing result in practice.\nBootstrap output [your results may differ slightly as these are created from a random process]:\n\nEstimate %CV \\# df 95% Confidence Interval\n--------------------------------------------------------\nHalf-normal/Cosine\nD 2129.2 27.40 999 20.74 1216.2 3727.5\n                         1164.0 3427.2\n\nHalf-normal/Cosine\nN 1064.6 27.40 999 20.74 608.00 1864.0\n                         582.00 1714.0\n\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals.\nInterval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\nThe bootstrap results are very similar to the analytic results, as we would expect. In fact, this did not used to be the case in previous versions of Distance, as the old analytic variance estimator did not perform well when there were extreme trends in both density and line length. You can access the previous default estimator under the Advanced… tab on the Variance page of the Model Definition Properties (its estimator R3), and more details are given in (Fewster et al., 2009).\nThe component percentages of variance are as follows:\n\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 4.3\nEncounter rate :       95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#post-stratification-to-improve-variance-estimation",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#post-stratification-to-improve-variance-estimation",
    "title": "Variance estimation for systematic designs",
    "section": "Post-stratification to improve variance estimation",
    "text": "Post-stratification to improve variance estimation\n\nThe precision is now greatly improved:\n\nEstimate %CV df 95% Confidence Interval\n-----------------------------------------------------\nHalf-normal/Cosine\n\nD 2044.6 8.64 31.41 1715.0 2437.5\nN 1022.0 8.64 31.41 858.00 1219.0\nand a much smaller and more reasonable (considering the sample size and survey coverage) proportion of the variation comes from estimating encounter rate:\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 44.3\nEncounter rate :        55.7\n\nThe CV is now even smaller, although it could have gone either way because this is an estimator of the same quantity as the last question–just a more precise estimator.\n\nEstimate %CV df 95% Confidence Interval\n-----------------------------------------------------\n\nHalf-normal/Cosine\n\nD 2044.6 7.97 75.87 1745.0 2395.6\nN 1022.0 7.97 75.87 872.00 1198.0\nThe encounter rate degrees of freedom are now 19 (number of lines – 1) rather than 10 (number of post-strata) for the previous question–which is why this is a more precise estimator of the variance.\nRemember we have not made any change to our data by the post-stratification; we are just getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be extreme as they are in this example, and post-stratification will not make a great difference. Such an example is shown below."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "title": "Variance estimation for systematic designs",
    "section": "Systematic designs where post-stratification is not needed",
    "text": "Systematic designs where post-stratification is not needed\n\nThe following simulated population does not exhibit strong trends across the survey region. Otherwise, the strip dimensions and systematic design are the same as for the previous example.\n\nWithout post-stratification: analytic output\nEstimate %CV df 95% Confidence Interval\n------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1954.0 8.22 50.60 1657.3 2303.9\nN 977.00 8.22 50.60 829.00 1152.0\nNote: Your bootstrap results will differ slightly, as bootstrapping is a random procedure.\nEstimate %CV \\# df 95% Confidence Interval\n--------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1947.4 10.03 999 50.60 1592.8 2380.8\n                         1565.0 2350.3\n\nHalf-normal/Cosine\n\nN 973.69 10.03 999 50.60 796.00 1190.0\n                         782.00 1175.0\n\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals.\nInterval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\n\nWith post-stratification (non-overlapping): analytic output\nEstimate %CV df 95% Confidence Interval\n------------------------------------------------------\n\nHalf-normal/Cosine\n\nD 1954.0 8.38 25.80 1645.4 2320.6\nN 977.00 8.38 25.80 823.00 1160.0"
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html",
    "href": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "",
    "text": "More detection function modelling and model criticism\nResults of estimating density from simulated data in which true density was 79.8 per km2. Findings from some candidate models:\nNot surprisingly for these data (simulated from a half normal detection function with a broad shoulder), the negative exponential model gives a higher estimate than the others, although the confidence interval still includes the true density. The other models provide very similar estimates, though precision is slightly worse for the hazard-rate model (because more parameters fitted). Agreement between the estimate and the known true density is less good if you do not truncate the data, or do not truncate them sufficiently."
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html#additional-question",
    "href": "03-criticism/DistWin-prac/Pr3-solution-DistWin.html#additional-question",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "Additional question",
    "text": "Additional question\nThe capercaillie data are reasonably well-behaved and different models that fit the data well should give similar results. Note the rounding in the distance data. This means that interval cutpoints for histograms and goodness-of-fit testing, and for the analysis of grouped data if this is required, should be chosen with care (i.e., distance bands ought to be sufficiently broad such that the ‘correct’ perpendicular distance is in the bands containing the rounded recorded value. e.g. 0, 7.5, 17.5, 27.5, … )\n\n\n\n\n\n\n\n\n\n\nFitted model\n\\(\\hat{D}\\)\n\\(\\hat{D}\\) LCB\n\\(\\hat{D}\\) UCB\n\\(\\hat{D}\\) CV\n\n\nHalf normal\n4.76\n4.01\n5.65\n0.09\n\n\nUniform cosine\n4.28\n3.22\n5.68\n0.14\n\n\nHazard rate\n4.20\n3.6\n4.9\n0.08\n\n\nHalf normal with grouped data\n4.52\n3.81\n5.36\n0.09\n\n\n\n\n\n\n\n\n\nNote regarding estimates in previous table\n\n\n\n\n\nI have reported the density estimates in the table above as numbers km-2, rather than numbers ha-1 to make them easier to read. There are 100 hectares in a square kilometer."
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html",
    "href": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "",
    "text": "More detection function modelling and model criticism\n1a). The line transect data below were generated from a half-normal detection function model. You may either enter the data by hand (if you want practice doing this) using instructions in the next bullet point, or skip to step 1b) to open a project containing the data to begin analysis.\nPerpendicular distances in metres generated from a half-normal line transect detection function model.\n1b) The full data set is in project Exercise3-2023.zip Download the file from this link and save the file in a location on your computer. Open Distance for Windows. Choose Open project and select zip file type to open the file and begin analysis."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html",
    "title": "Variance estimation for systematic designs",
    "section": "",
    "text": "Variance estimation for systematic designs\nIn the lecture describing measures of precision we explained systematic survey designs usually have the best variance properties, but obtaining good estimates of the variance is a difficult problem for statisticians. In this exercise we give an example of a situation where the systematic design gives a density estimate with much better precision than a random design.\nThis means that the usual variance estimators used in Distance, which are based on a random design, give variance estimates that are far too high. The true variance is low, but the estimated variance is high. We will see how to implement a post-stratification scheme that enables us to get a better estimate of the variance.\nWe also look at another case to see that the unstratified variance estimates provided by Distance are usually fine for a systematic design: things only go wrong when there are very strong trends in animal density, especially when the strong trends are associated with changes in line length (e.g., the highest densities always occur on the shortest lines, or vice versa).\nWe begin with the population and survey shown below. All the populations in this exercise are simulated on a computer: they are not real data. Note the characteristics: extreme trends with very high density on short lines and very low density on long lines. Additionally, the systematic design (search strips are shaded) covers a fairly large portion of the survey area. These are the danger signals that the usual Distance variance estimators might not work well, and a post-stratification scheme should be considered.\nThe survey region is a triangle, with dimensions 1km by 1km. The systematically placed search strips are shaded."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#basic-variance-estimation-with-bootstrapping",
    "title": "Variance estimation for systematic designs",
    "section": "Basic variance estimation, with bootstrapping",
    "text": "Basic variance estimation, with bootstrapping\n\nDownload and open the Distance project Systematic_variance_2.zip.\nOn the Analysis tab, click New Analysis. Rename it Without post-stratification.\nUnder Model definition, click Properties. Rename the new model: No_adjustments_plus_bootstrap.\nClick the tab for Detection function, and click Adjustment terms. Select Manual selection so that no adjustment terms are fitted. Select the Constraints button, and click No constraints. These options will reduce the work that has to be done during bootstrapping.\nClick the tab for Variance, and check the box for Bootstrap variance estimate: Select non-parametric bootstrap. The box Resample samples should be checked (this means resample transect lines). Leave the other settings at default, noting that there will be 999 bootstrap resamples conducted.\nClick OK and then run the model. You can see the progress of the bootstrap in the bar at the top. Wait a few moments until the bootstrapping is completed.\nYour analytic output should resemble this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate\n%CV\ndf\nLower 95% Confidence bound\nUpper 95% Confidence bound\n\n\n\n\nD\n2044.6\n27.70\n20.74\n1161.0\n3600.6\n\n\nN\n1022.0\n27.70\n20.74\n581.0\n1800.0\n\n\n\n\nBecause we have simulated these data, we know the true values. The true number of animals in the survey region is N=1000, and the true density is D=2000 km-2 (1000 animals in an area of size A=0.5 km2). The point estimates are good, but what do you think about the precision in the output above?\nFind the bootstrapped confidence intervals for D and N, and check whether they are similar to the confidence intervals above.\nWhat percentage of the total density variance is attributed to encounter rate estimation and what percentage to the detection function estimation?\n\n\nVariance estimation for systematic designs using post-stratification\nRecall we have a particular situation in which we have systematically placed transects, unequal in length. Furthermore there exists an east-west gradient in animal density juxtaposed such that the shortest lines are those that pass through the portion of the study area with the highest density. We examine a means by which we can use post-stratification to produce a better estimate of the variance in estimated density."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#post-stratification-to-improve-variance-estimation",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#post-stratification-to-improve-variance-estimation",
    "title": "Variance estimation for systematic designs",
    "section": "Post-stratification to improve variance estimation",
    "text": "Post-stratification to improve variance estimation\nThe estimation of encounter rate variance above used estimators that assumed the transect lines were randomly placed throughout the triangular region. In our case, the transects were not random, but systematic. In some circumstances, this can reduce the encounter rate variance a great deal.\nThe data we are working with is an example of this. There are very high densities on the very shortest lines. In samples of lines collected using a completely random design, the sample by chance might not contain any of these very short lines, or it might contain several. The variance is therefore very high, because the density estimates will be greatly affected by how many lines fall into the short-line / high-density region: we will get very low density estimates if there are no short lines, but very high density estimates if there are several short lines. By contrast, in a systematic sample, we cover the region methodically and we will always get nearly the same number of lines falling in the high density region. The systematic density variance is therefore much lower than the random placement density variance.\nAlthough there is no way of getting a variance estimate that is exactly unbiased for a systematic sample1, we can greatly improve on the random-based estimate by using a post-stratification scheme. This works by grouping together pairs of adjacent lines from the systematic sample. Each pair of adjacent lines is grouped into a stratum. The strata will improve variance estimation, because the systematic sample behaves more like a stratified sample than a random sample.\nFollow the steps below.\n\nOpen the Distance project we used in the previous section (Systematic_variance_2.dst; is has the “.dst” extension because you uncompressed it minutes ago).\nClick the Analyses tab, and click the “New analysis” button to create a new analysis. Double click the grey ball and the Analysis Details Window should come up. Name the new analysis something like With post stratification.\nUnder Model Definition, click New. Change the name at the bottom of the dialogue box to Poststratified_no_adjustments_no_bootstrap. (We don’t want to conduct a bootstrap for our poststratified data, because it would involve some extra confusion and is not necessary.) In the Variance tab, click Advanced..., and select the option “Post-stratify, grouping adjacent pairs of samplers”. Un-tick the option “Select non-parametric bootstrap”.\nClick OK and then Run to run the analysis. How does the variance and confidence limits compare with those you obtained in the previous section? What are the implications? Note what percentage of the overall variance now comes from encounter rate and from estimating the detection function, and compare this with the earlier percentages.\nNow try the overlapping post-stratification option. A simulation study in (Fewster et al., 2009) concluded that its performance was very similar to, but marginally better than the regular post-stratification. When the sample size of lines is small, it gives more post-strata and so is to be preferred for that reason. Create a new analysis, called say With overlapping post stratification, and then a new Model Definition for that analysis, in which you choose the Advanced variance option “Post-stratify, with overlapping strata made up of adjacent samplers”. How does the variance compare with those you previous obtained? How do the degrees of freedom in the Estimation Summary – Encounter Rate page of output compare with that from the previous question?\n(Optional) If you wish, you can try manual post-stratification. This is good practice if you need to do post-stratification for point transect studies. In this case you will have to add a new field to the sample layer, and then set up a new model definition in which you tell Distance to use post-stratification. Here goes:\n\nClick the Data tab. Click the padlock button on the toolbar to unlock the data sheet for modification.\nOn the left-hand outline, click Line transect. The data sheet expands to 20 rows, each row corresponding to one line transect. This is the best format for the data sheet to be in when entering a new stratum number for each transect.\nClick on the cell corresponding to Line transect Label 1. Several buttons on the tool-bar should become live. Click on the button corresponding to Append field after current. (The button has an arrow pointing sideways then downwards.)\nYou are prompted for Field name: enter VarGroup to indicate that you are grouping lines together for the purpose of variance estimation. Click Field type: Integer, and click OK.\nYou can now enter the line groupings for post-stratified variance estimation. Enter label 1 for lines 1 and 2; label 2 for lines 3 and 4; label 3 for lines 5 and 6; and so on, to finish with label 10 for lines 19 and 20. You have now defined 10 strata, each containing two adjacent transect lines from the systematic sample of lines.\nAfter entering the column of VarGroup labels, click the padlock button again to lock the data sheet.\nNow we will analyse the post-stratified data. Click the Analyses tab. Create a new analysis with a suitable name - .e.g, Manual post stratification\nCreate a new Model definition, with a suitable name. In the Estimate tab, click the button for Poststratify. Select Layer type: sample, and Field name: VarGroup. This means that we want to poststratify at the sample (transect) level, using our newly defined groupings VarGroup to delimit the strata.\nFor the levels of resolution, select the following:\n\n\n\nDensity: Global and Stratum\nEncounter Rate: Stratum only\nDetection function: Global only\nCluster size (not required): Global only\n\n\nThese settings ensure that it is only encounter rate variance that is affected by the post-stratification scheme; the detection function is still pooled over all observations as before.\n\n\nIn the next field, enter Global density estimate is Mean of stratum estimates, and in the next field select Weighted by Total effort in stratum. Do not tick the box saying Strata are Replicates.\nClick OK and run the new model. The point estimates should be the same as the previous non-overlapping post stratification run.\n\n\nNote: The precision of D and N are greatly improved in the post-stratified analyses. Note that we are not getting something for nothing: the second analysis is giving us an answer much closer to the true answer, while the first analysis was simply giving us the wrong answer. We have not changed the true variance by our post-stratification scheme: we are just getting a better estimate of the true variance. Because the data above were generated by simulation, we can use repeated simulated surveys to check that the second answer is indeed close to the true density variance over the repeats."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#systematic-designs-where-post-stratification-is-not-needed",
    "title": "Variance estimation for systematic designs",
    "section": "Systematic designs where post-stratification is not needed",
    "text": "Systematic designs where post-stratification is not needed\nThe following simulated population does not exhibit strong trends across the survey region. Otherwise, the strip dimensions and systematic design are the same as for the previous example.\n\nDownload and open the project Systematic_variance_1.zip. Add the new data column VarGroup before conducting any analyses this time. With the augmented data, repeat the analyses you performed on the Systematic_variance_2.zip project. Find the relevant outputs. Has the post-stratification scheme been necessary in this case?"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#footnotes",
    "href": "04-precision/DistWin-prac/Pr4-instructions-DistWin.html#footnotes",
    "title": "Variance estimation for systematic designs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nbecause it is effectively a sample of size 1 – only the first line position was randomly chosen, and the rest followed on deterministically from there.↩︎"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html",
    "title": "Analysis of point transect survey data",
    "section": "",
    "text": "Analysis of point transect survey data"
  },
  {
    "objectID": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html#additional-question-with-another-line-transect-data-set",
    "href": "03-criticism/DistWin-prac/Pr3-instructions-DistWin.html#additional-question-with-another-line-transect-data-set",
    "title": "Additional detection function modelling using Distance for Windows software",
    "section": "Additional question with another line transect data set",
    "text": "Additional question with another line transect data set\n\n2. Below are perpendicular distance data (m) from line transect surveys of capercaillie (a large grouse) in Scotland. Total line length was 240km. The data are also in a text file capercaillie.txt you may download from this link and save on your computer. Description of a line transect survey of this species is presented in (Catt et al., 1998)\nIn the text file, column 1 is the transect number, column 2 is the transect length and column 3 is perpendicular distance as shown.\n\n\n  V1  V2 V3\n1  1 240 28\n2  1 240 17\n3  1 240 15\n4  1 240 14\n5  1 240 18\n6  1 240  0\n\n\nColumns are separated by tab characters. Create a new Distance project and either enter the data by hand or use the Data Import Wizard (Tools &gt; Import Data Wizard) to import the data from the text file. Then decide on a suitable model for the detection function and estimate bird density.\nCapercaillie, Monaughty Forest 112 detections\n28.0 17.0 15.0 14.0 18.0 0.0 38.0 6.0 50.0 65.0\n75.0 1.0 70.0 28.0 40.0 40.0 40.0 15.0 40.0 30.0\n5.0 55.0 60.0 40.0 24.0 30.0 0.0 50.0 55.0 10.0\n40.0 10.0 30.0 34.0 24.0 30.0 15.0 20.0 14.0 48.0\n0.0 30.0 2.0 52.0 11.0 48.0 28.0 38.0 25.0 35.0\n45.0 0.0 16.0 12.0 2.0 14.0 12.0 24.0 70.0 50.0\n49.0 40.0 80.0 18.0 27.0 30.0 30.0 60.0 58.0 14.0\n0.0 56.0 40.0 19.0 21.0 0.0 38.0 20.0 28.0 30.0\n20.0 16.0 0.0 69.0 40.0 46.0 50.0 40.0 70.0 67.0\n28.0 12.0 12.0 22.0 40.0 48.0 48.0 15.0 12.0 0.0\n15.0 20.0 17.0 30.0 30.0 32.0 48.0 20.0 10.0 20.0\n42.0 30.0"
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard",
    "title": "Variance estimation for systematic designs",
    "section": "Component Percentages of Var(D)",
    "text": "Component Percentages of Var(D)\nDetection probability : 4.3 Encounter rate : 95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard-1",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#component-percentages-of-vard-1",
    "title": "Variance estimation for systematic designs",
    "section": "Component Percentages of Var(D)",
    "text": "Component Percentages of Var(D)\nDetection probability : 44.3 Encounter rate : 55.7\n\nThe CV is now even smaller, although it could have gone either way because this is an estimator of the same quantity as the last question–just a more precise estimator.\n\n\nEstimate %CV df 95% Confidence Interval\n\nHalf-normal/Cosine\nD 2044.6 7.97 75.87 1745.0 2395.6\nN 1022.0 7.97 75.87 872.00 1198.0\nThe encounter rate degrees of freedom are now 19 (number of lines – 1) rather than 10 (number of post-strata) for the previous question–which is why this is a more precise estimator of the variance.\n\nRemember we have not made any change to our data by the post-stratification; we are just getting a better estimate of the variance. In this case, the increase in precision could make a fundamental difference to the utility of the survey: it might make the difference between being able to make a management decision or not. Usually, trends will not be extreme as they are in this example, and post-stratification will not make a great difference. Such an example is shown below."
  },
  {
    "objectID": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#estimate-cv-df-95-confidence-interval",
    "href": "04-precision/DistWin-prac/Pr4-solution-DistWin.html#estimate-cv-df-95-confidence-interval",
    "title": "Variance estimation for systematic designs",
    "section": "Estimate %CV # df 95% Confidence Interval",
    "text": "Estimate %CV # df 95% Confidence Interval\nHalf-normal/Cosine D 2129.2 27.40 999 20.74 1216.2 3727.5 1164.0 3427.2\nHalf-normal/Cosine N 1064.6 27.40 999 20.74 608.00 1864.0 582.00 1714.0\nNote: Confidence interval 1 uses bootstrap SE and log-normal 95% intervals. Interval 2 is the 2.5% and 97.5% quantiles of the bootstrap estimates.\n\nThe bootstrap results are very similar to the analytic results, as we would expect. In fact, this did not used to be the case in previous versions of Distance, as the old analytic variance estimator did not perform well when there were extreme trends in both density and line length. You can access the previous default estimator under the Advanced… tab on the Variance page of the Model Definition Properties (its estimator R3), and more details are given in (Fewster et al., 2009).\nThe component percentages of variance are as follows:\n\nComponent Percentages of Var(D)\n-------------------------------\nDetection probability : 4.3\nEncounter rate :       95.7\nIt should ring an alarm bell to see such a high contribution from Encounter rate. Generally we might expect encounter rate to be in the region of 70% to 80% for line transect surveys."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#simulated-point-transect-data",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#simulated-point-transect-data",
    "title": "Analysis of point transect survey data",
    "section": "Simulated point transect data",
    "text": "Simulated point transect data\n\nSimulated point transect data from 30 points are given in project PTExercise1.zip. These data were generated from a half-normal detection function, and true density was 79.6 animals ha-1.\n\nDownload and analyse this data set. Experiment with keys other than the half-normal (uniform, hazard-rate and negative exponential), to assess whether these data can be satisfactorily analysed using the wrong model. For each key, determine a suitable truncation point, and decide on whether, and which, adjustments are needed. (Truncation points come under the data filter.) How do bias and precision compare between models?"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#winter-wrens-in-scotland",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#winter-wrens-in-scotland",
    "title": "Analysis of point transect survey data",
    "section": "Winter wrens in Scotland",
    "text": "Winter wrens in Scotland\n\nThe following projects are available for download and analysis:\n\n\nWren1.zip 5min count,\nWren2.zip snapshot,\nWren3.zip cue count and\nWren4.zip line transect.\n\nThe projects contain winter wren data, collected at Montrave, Scotland in 2004, as described in (Buckland, 2006). Each project corresponds to a different method of data collection. Thirty-two points were defined through 33.2 ha of parkland (Fig. 1), and detection distances were measured in meters with the aid of a laser rangefinder. Three types of point transect data were collected:\n\nstandard five-minute counts\nthe ‘snapshot’ method and\na cue count method.\n\nIn addition, line transect data were collected (method 4), and territory mapping was conducted, which gave an estimate of 43 wren territories (1.30 territories ha-1).\n\nSelect a single model for exploratory data analysis. Experiment with different truncation distances w, and select a suitable value for each method. Do you see potential problems with any of the data sets?\nTry other models and other model options. Use plots, AIC values and goodness-of-fit test statistics to determine an adequate model.\nRecord your estimates of density for each method. Record also the corresponding confidence intervals."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#savannah-sparrows-in-colorado-usa",
    "href": "05-points/DistWin-prac/Pr5-instructions-DistWin.html#savannah-sparrows-in-colorado-usa",
    "title": "Analysis of point transect survey data",
    "section": "Savannah sparrows in Colorado USA",
    "text": "Savannah sparrows in Colorado USA\n\nTwo additional point transect projects are available for download and analysis:\n\n\nSavannah Sparrow 1980.zip and\nSavannah Sparrow 1981.zip.\n\nThese were part of a large data set collected in Arapaho National Wildlife Refuge, Colorado. For both data sets, consider an appropriate truncation distance, decide on a suitable model for the detection function, and estimate density, both for each stratum individually and for the whole study area. You should include in your analysis an assessment of whether the detection function can be estimated from data pooled across strata, or whether separate estimates are needed per stratum. (This will be covered in the lecture discussing stratification if you don’t already know how to do it.)"
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#simulated-point-transect-data",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#simulated-point-transect-data",
    "title": "Analysis of point transect survey data",
    "section": "Simulated point transect data",
    "text": "Simulated point transect data\n\nResults from selected model options; remember these are simulated data with a half normal detection function and true density 79.6:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKey\nAdjustment\nterms\nw(m)\n\\(\\hat{D}\\)\n%CV\nLCB\nUCB\n\n\n\n\nHalf-normal\nNone\n0\n34.2\n79.6\n12.6\n62.1\n102.1\n\n\nHalf-normal\nNone\n0\n20\n70.8\n15.7\n52\n96.5\n\n\nUniform\nCosine\n1\n20\n75\n14.4\n56.5\n99.6\n\n\nHazard-rate\nNone\n0\n20\n62.4\n18.7\n43.2\n90\n\n\nNeg Exp\nSimple poly\n1\n20\n73.1\n29.2\n41.5\n128.6\n\n\n\nWe see a fair degree of variability between analyses–reliable analysis of point transect data is more difficult than for line transect data. We see greater loss in precision from truncating data relative to line transect sampling, but if we do not truncate data, different models can give widely differing estimates. For these data, the hazard-rate model appears to have downward bias, and precision is very poor for the negative exponential model."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#winter-wren-data-analysis-with-bonus-species-included",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#winter-wren-data-analysis-with-bonus-species-included",
    "title": "Analysis of point transect survey data",
    "section": "Winter wren data analysis (with bonus species included)",
    "text": "Winter wren data analysis (with bonus species included)\n\nOur analyses of the winter wren data produced the following estimated densities (territories ha-1). Three other species were surveyed in the field. Estimates of density for those species are in the table below, although the Distance for Windows projects are not included. Method 5 is territory mapping (which does not use distance sampling, and as you note has no measure of precision associated because it is akin to a census method).\n\n\n\n\nMethod\nCommon Chaffinch\nGreat Tit\nEuropean Robin\nWinter Wren\n\n\n\n\n1\n1.03\n0.58\n0.52\n1.29\n\n\nCI\n(0.74,1.43)\n(0.36,0.94)\n(0.26,1.06)\n(0.80,2.11)\n\n\n2\n0.90\n0.22\n0.60\n1.02\n\n\nCI\n(0.62,1.29)\n(0.13,0.39)\n(0.38,0.94)\n(0.80,1.32)\n\n\n3\n0.71\n0.26\n0.82\n1.21\n\n\nCI\n(0.45,1.23)\n(0.09,0.76)\n(0.52,1.31)\n(0.82,1.79)\n\n\n4\n0.64\n0.26\n0.69\n1.07\n\n\nCI\n(0.46,0.90)\n(0.16,0.42)\n(0.47,1.00)\n(0.87,1.31)\n\n\n5\n0.75\n0.21\n0.84\n1.30\n\n\n\nTo obtain the above estimates, I used a truncation distance of 110m for methods 1 and 2, 92.5m for method 3, and 95m for method 4. For the wren data, I used the uniform key with two cosine adjustments for method 1, the hazard-rate model for methods 2 and 3, and the half-normal key with two Hermite polynomial adjustments for method 4.\nPoints to note about the wren data: the wren more than any of the other species showed evidence of observer avoidance. This didn’t cause too many difficulties, except that the model favoured by AIC for line transect sampling was the hazard-rate model, which had a very flat shoulder out to around 75m. It was implausible that detection was certain out to this distance, so that I selected a model with a slightly inferior AIC value, but with a more plausible fitted detection function. Analyses of the cue count data are necessarily rather subjective, as the data show substantial over-dispersion (a single bird may give many songbursts all from the same location during a five-minute count). In this circumstance, goodness-of-fit tests are very misleading, and care must be taken not to overfit the data."
  },
  {
    "objectID": "05-points/DistWin-prac/Pr5-solution-DistWin.html#savannah-sparrow-analysis",
    "href": "05-points/DistWin-prac/Pr5-solution-DistWin.html#savannah-sparrow-analysis",
    "title": "Analysis of point transect survey data",
    "section": "Savannah sparrow analysis",
    "text": "Savannah sparrow analysis\n\nGood fits to the 1980 Savannah sparrow data were obtained by truncating at 55m. The half-normal detection function without adjustments fitted well, as did the uniform with cosine adjustments. The hazard-rate model performed less well. There was a marginal preference for fitting the detection function separately in each stratum as judged by AIC, but pooling distance data across strata might offer rather more robust estimation. The estimates of density in the table correspond to a half-normal detection function, fitted separately in each stratum, with a truncation distance of 55m.\n\nFor 1981, w=55m was again satisfactory. There was now a clear preference for estimating the detection function separately by stratum, but little to choose between the half-normal model and the uniform model with cosine adjustments. For comparability with 1980, I chose the half-normal model, although AIC showed a very marginal preference for uniform + cosine. (Again, the hazard-rate model provided less good fits overall.)\nEstimated densities \\(\\hat{D}\\) (birds/ha) of Savannah sparrows\n\n\n\nYear\nPasture\n\\(\\hat{D}\\)\nLCB\nUCB\n\n\n\n\n1980\n1\n1.43\n0.94\n2.18\n\n\n\n2\n4.12\n3.15\n5.38\n\n\n\n3\n2.35\n1.72\n3.2\n\n\n\nAll\n2.63\n2.19\n3.16\n\n\n\n\n\n\n\n\n\n1981\n0\n1.39\n0.82\n2.36\n\n\n\n1\n0.52\n0.27\n1.03\n\n\n\n2\n1.7\n1.07\n2.71\n\n\n\n3\n1.35\n0.81\n2.26\n\n\n\nAll\n1.24\n0.95\n1.62"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html",
    "href": "06-design/R-prac/Pr6-instructions.html",
    "title": "Distance sampling survey design",
    "section": "",
    "text": "We provide two exercises in survey design so you can choose the one you feel is most useful to you.\n\nThe first example involves designing a line transect survey to estimate the abundance of porpoise, common dolphins and seals in and around St Andrews Bay.\n\nIt considers how you choose your design based on effort limitations.\nIt also compares an aerial survey based on systematic parallel lines with a boat based survey using zigzags.\n\nThe second example involves designing a point transect bird survey in Tentsmuir Forest.\n\nThis looks at how to project your study area from latitude and longitude on to a flat plane using R.\nIt also involves defining a design for multiple strata with different coverage in each strata."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#study-region",
    "href": "06-design/R-prac/Pr6-instructions.html#study-region",
    "title": "Distance sampling survey design",
    "section": "Study Region",
    "text": "Study Region\nSet up the study region and plot it. The shapefile for this study area is contained within the dssd R library. This shapefile has already been projected from latitude and longitude on to a flat plane and its units are in metres. The first line of code below returns the path for the shapefile within the R library and may vary on different computers. You will then pass this shapefile pathway to the make.region function to set up the survey region. As this shapefile does not have a projection (.prj) file associated with it we should tell dssd the units (m) when we create the survey region.\n\n#Find the pathway to the file\nshapefile.name &lt;- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\n#Create the region using this shapefile\nregion &lt;- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\n#Plot the region\nplot(region)\n\n\n\n\nStudy Region"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#coverage",
    "href": "06-design/R-prac/Pr6-instructions.html#coverage",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nThe next step is to set up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover &lt;- make.coverage(region, n.grid.points = 500)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#systematic-parallel-design",
    "href": "06-design/R-prac/Pr6-instructions.html#systematic-parallel-design",
    "title": "Distance sampling survey design",
    "section": "Systematic Parallel Design",
    "text": "Systematic Parallel Design\nThe small survey plane available can complete a total flight time of around 250km (excluding the flight time to and from the landing strip at Fife Ness). Generally, systematic parallel line designs are preferable for aerial surveys as they allow some rest time for observers as the plane travels between transects and avoids the sharp turns associated with zigzag designs.\nFirstly, we will consider the design angle. Often animal density is affected by distance to coast so it is probably wise for this survey to orientate lines approximately perpendicular to the coast. To do this we can select a design angle of 90 degrees. We can therefore expect to spend a little more than 40 km (the height of the survey region) on off-effort transit time and might hope to be able to complete around 200km of transects. dssd lets us specify the desired line length as a design parameter and will then choose an appropriate value for transect spacing. We will choose a minus sampling strategy and set the truncation distance to 2km. Note that as our survey region coordinates are in metres we also need to supply the design parameters in metres.\n\n# Define the design\ndesign.LL200 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      line.length = 200000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nNow we have defined the design we should check it visually by creating a survey (a single set of transects).\n\n# Create a single survey from the design\nsurvey.LL200 &lt;- generate.transects(design.LL200)\n# Plot the region and the survey\nplot(region, survey.LL200)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nThe survey consists of parallel systematically spaced transects running horizontally across the survey region roughly perpendicular to the coast as we wanted. We can also view the details of the survey which will tell us what spacing dssd used to try and achieve a line length of 200 km.\n\n# Display the survey details\nsurvey.LL200\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced parallel transects\nSpacing:  4937.5\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nDesign angle:  90\nEdge protocol:  minus\nCovered area:  779476845\nStrata coverage: 78.93%\nStrata area:  987500079\n\n   Study Area Totals:\n   _________________\nLine length: 199960.8\nTrackline length: 248012.3\nCyclic trackline length: 284758.4\nNumber of samplers:  8\nCovered area:  779476845\nAverage coverage: 78.93%\n\n\nThe spacing used by dssd was 4938m which gives us 8 samplers and a coverage of just under 80%. In addition, this example survey has a line length of just under 200km and a trackline length of just over 248 km. However, given the random nature of the design and the fact that the width of the study region is not constant everybody should get slightly different values. Although the trackline length was just under 250km it is not sufficient to only look at one survey, we need to know that all surveys under this design will have a trackline length of &lt; 250km.\nTo assess the design statistics across many surveys we will now run a coverage simulation. This simulation will randomly generate many surveys from our design and record coverage as well as various statistics including line length and trackline length. As we know that coverage for a parallel line design is largely uniform (apart from edge effects due to minus sampling) we do not need to run too many repetitions, 100 should be sufficient to give us an indication of the range of line lengths and trackline lengths for this design.\n\n# Run the coverage simulation\ndesign.LL200 &lt;- run.coverage(design.LL200, reps = 100)\ndesign.LL200\n\nExamine the design statistics. The mean line length should be around 200km (200,000 m). Now look at the maximum trackline length, we need this value to be less than 250km (250,000 m).\nUse the results of this simulation to create some new designs based on various spacings to find the maximum line length that can be achieved without risking exceeding the maximum trackline length of 250km (remember to generate a line length of 200km dssd selected a spacing of 4938m). Try transect spacings of 5000m or 5500m.\n\n# Define the design\ndesign.space500 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat spacing would you select for this design?\nWhat is the maximum trackline length for the design you have selected?\nWhat on-effort line length are we likely to achieve?"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#zigzag-design",
    "href": "06-design/R-prac/Pr6-instructions.html#zigzag-design",
    "title": "Distance sampling survey design",
    "section": "Zigzag Design",
    "text": "Zigzag Design\nZigzag designs are often more efficient in their use of effort having less off-effort transit time between transects. For this survey another option would be to complete a boat-based survey. The boat survey will have the same total effort available allowing us a trackline length of 250km.\nDefine a zigzag design for the same region. For zigzag designs the design angle has a different definition, it describes the angle across which the zigzags are constructed. Set the vertical design angle to 0. Zigzag designs also require an additional argument as zigzags can only be created inside convex shapes. Specify the bounding shape, choose a convex hull as it is more efficient than a minimum bounding rectangle. A convex hull works as if we were stretching an elastic band around the survey region. The code below shows you how to create the zigzag design, you should then create a single realisation of this design and plot it to check it looks acceptable.\n\n# Define the zigzag design\ndesign.zz.4500 &lt;- make.design(region = region,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover)\n\nsurvey.zz &lt;- generate.transects(design.zz.4500)\nplot(region, survey.zz)\n\n\n\n\nSingle survey generated from the equalspaced zigzag design\n\n\n\n\nRun a coverage simulation to verify that we have stayed within the restraints of our survey effort; a total trackline length of &lt; 250km. Run the coverage simulation with more repetitions to also assess the coverage.\nExamine the design statistics.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDoes this design meet our survey effort constraint?\nWhat is the maximum total trackline length for this design?\nWhat line length are we likely to achieve with this design?\nIs this higher or lower than the systematic parallel design?\n\n\n\n\n# Run the coverage simulation\ndesign.zz.4500 &lt;- run.coverage(design.zz.4500, reps = 500)\n# Display the design statistics\ndesign.zz.4500\n\nExamine the coverage. Sometimes with zigzag surveys generated inside convex hulls produce areas of higher coverage in narrower parts of the survey region at either end of the design axis. One of the easiest ways to assess coverage is visually by plotting the coverage grid.\n\n# Plot the coverage grid\nplot(design.zz.4500)\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDo you think the coverage scores look uniform across the study region?\nGiven the lack of uniformity, where are they higher/lower?\nWhy do you think this is?\n\nNote, revisit one of the parallel line designs and plot the coverage scores to compare (although there are fewer repetitions you can still get an idea of coverage)."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#projecting-your-study-region",
    "href": "06-design/R-prac/Pr6-instructions.html#projecting-your-study-region",
    "title": "Distance sampling survey design",
    "section": "Projecting your Study Region",
    "text": "Projecting your Study Region\nThis exercise demonstrates how to deal with unprojected shapefiles. Study areas should always be projected onto a flat plane before you use them to design your survey. This is because in most parts of the world one degree latitude is not the same in distance as one degree longitude. If we didn’t project, our study region and any surveys generated in it, would be distorted possibly leading to non-uniform coverage.\nLoad the study region and project it onto a flat plane using an Albers Equal Area Conical projection. As we have to project the shapefile we load the shape object separately instead of directly into a region object.\n\n#Load the unprojected shapefile\nshapefile.name &lt;- system.file(\"extdata\", \"TentsmuirUnproj.shp\", package = \"dssd\")\nsf.shape &lt;- read_sf(shapefile.name)\n# Check current coordinate reference system\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n# Define a European Albers Equal Area projection\nproj4string &lt;- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\n# Project the study area on to a flat plane\nprojected.shape &lt;- st_transform(sf.shape, crs = proj4string)\n\nCreate the region object for dssd using the projected shape and plot it to check what it looks like.\n\n# Create the survey region in dssd\nregion.tm &lt;- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n# Plot the survey region\nplot(region.tm)\n\n\n\n\nTentsmuir Forest: showing the main stratumand the Morton Loch stratum."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#coverage-1",
    "href": "06-design/R-prac/Pr6-instructions.html#coverage-1",
    "title": "Distance sampling survey design",
    "section": "Coverage",
    "text": "Coverage\nSet up a coverage grid. A coverage grid is a grid of regularly spaced points and is used to assess whether each point in the survey region is equally likely to sampled. The coverage grid needs to be created before the coverage simulation is run.\n\n# Set up coverage grid\ncover.tm &lt;- make.coverage(region.tm, n.grid.points = 1000)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#design",
    "href": "06-design/R-prac/Pr6-instructions.html#design",
    "title": "Distance sampling survey design",
    "section": "Design",
    "text": "Design\nSet up a systematic point transect design. We will assume that we have sufficient resources to survey 40 point transects. As the Morton Lochs stratum is of special interest we will give it higher coverage. We will therefore explicitly allocate 25 samplers to the main stratum and 15 to the Morton Lochs stratum (note that the area of the Morton Lochs stratum is much small than the main stratum). If we wanted to allocate the same effort to both stratum we could provide the samplers argument with the single value of 40 and it would divide the effort equally between the strata. We will leave the design angle as 0 and set the truncation distance to 100 m. We will use a minus sampling approach at the edges.\n\n\n\n\n\n\nQuestion:\n\n\n\n\nWhat are the analysis implications of a design with unequal coverage?\n\n\n\n\n# Set up a multi strata systematic point transect design\ndesign.tm &lt;- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#generate-a-survey",
    "href": "06-design/R-prac/Pr6-instructions.html#generate-a-survey",
    "title": "Distance sampling survey design",
    "section": "Generate a Survey",
    "text": "Generate a Survey\nYou will now generate a single survey from this design and plot it inside the survey region to check what it looks like. If you want to check whether the covered areas of the samplers in the Morton Lochs stratum overlap add the argument covered.area = TRUE to the plot function.\n\n# Create a single survey from the design\nsurvey.tm &lt;- generate.transects(design.tm)\n# Plot the region and the survey\nplot(region.tm, survey.tm, covered.area = TRUE)\n\n\n\n\nSingle survey generated from the systematicparallel line design\n\n\n\n\nExamine the survey information.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers?\nDid your survey achieve exactly the number of samplers you requested?\nHow much does coverage differ between the two strata for this realisation?\n\n\n\n\n# Display survey information\nsurvey.tm\n\n\nSave coordinates to a file\nIf this survey is to be conducted in the field, you will want the coordinates that you can load into a handheld GPS. The function write.transects() can write waypoints of the survey (in this case the point transect stations) to text, comma-separated value or GPX files.\n\nwrite.transects(survey.tm,\n                dsn = \"tentsmuir-points.gpx\",\n                layer = \"points\",\n                dataset.options = \"GPX_USE_EXTENSIONS=yes\",\n                proj4string=sf::st_crs(sf.shape))\n\nThe GPX file can be transferred to a GPS, or viewed using Google Earth.\n\n\n\n\n\nRealised survey with locations written to GPX file and imported into Google Earth."
  },
  {
    "objectID": "06-design/R-prac/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "href": "06-design/R-prac/Pr6-instructions.html#assessing-coverage-and-design-statistics",
    "title": "Distance sampling survey design",
    "section": "Assessing Coverage and Design Statistics",
    "text": "Assessing Coverage and Design Statistics\nWe will now run a coverage simulation to assess how much the number of samplers and average coverage varies between surveys. We will also be able to assess how coverage varies spatially to see if edge effects are of concern.\nView the design statistics.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is the minimum number of samplers you will achieve in each strata?\nIs this sufficient to complete separate analyses in each stratum?\n\n\n\nPlot the coverage scores.\n\n\n\n\n\n\nQuestions:\n\n\n\n\nDoes it appear that there is even coverage within each strata?\n\nAs there is such a difference in the range of coverage scores between strata you may need to plot each strata individually.\n\n\n\n\n\n# View the design statistics\nprint(design.tm)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  100\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum      22.0         13.0  36.0\nMean         25.1         15.1  40.2\nMedian       25.0         15.0  40.0\nMaximum      27.0         18.0  44.0\nsd            0.9          1.1   1.3\n\n    Covered area:\n    \n        Main Area Morton Lochs      Total\nMinimum 683662.63    363258.99 1076099.58\nMean    768186.34    420283.45 1188469.79\nMedian  772479.34    417730.22 1188447.46\nMaximum 816669.52    468461.70 1264365.86\nsd       25071.81     25394.87   32875.01\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.85        50.79  7.26\nMean         5.44        58.76  8.02\nMedian       5.48        58.40  8.02\nMaximum      5.79        65.49  8.53\nsd           0.18         3.55  0.22\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00000000    0.2200000 0.00000000\nMean    0.05483087    0.5750000 0.08094378\nMedian  0.05000000    0.6450000 0.06000000\nMaximum 0.15000000    0.7800000 0.78000000\nsd      0.02997747    0.1456197 0.12170445\n\n# Plot the coverage scores\nplot(design.tm)\n\n\n\n# coverage scores for individual strata could be plotted separately\n# plot(design.tm, strata.id = 1)\n# plot(design.tm, strata.id = 2)"
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html",
    "href": "06-design/R-prac/Pr6-solution.html",
    "title": "Distance sampling survey design solution",
    "section": "",
    "text": "Solution\n\n\n\nDistance sampling survey design\nlibrary(dssd)\nlibrary(sf)\n\nLinking to GEOS 3.11.2, GDAL 3.7.2, PROJ 9.3.0; sf_use_s2() is TRUE"
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html#systematic-parallel-line-design",
    "href": "06-design/R-prac/Pr6-solution.html#systematic-parallel-line-design",
    "title": "Distance sampling survey design solution",
    "section": "Systematic Parallel Line Design",
    "text": "Systematic Parallel Line Design\n\n\n\n\n\n\nAnswers\n\n\n\nWhat spacing would you select for this design? What is the maximum trackline length for the design you have selected? What on-effort line length are we likely to achieve?\n\nThe spacing chosen by dssd of 4937.5m to generate a line length of 200km resulted in a maximum trackline length of around 261km (each exact answer will vary due to the random generate of surveys). If we choose this design then it is possible that when we randomly generate our survey we may not be able to complete it with the effort we have available.\n\n\nWe should therefore increase the spacing between the transects and re-run the coverage simulations. A spacing of 5000m gave a maximum trackline length of around 249km (see summary table of Trackline length in the output below) so we can be fairly confident that we will be able to complete any survey which we randomly generate from this design. This spacing should allow us to achieve an on-effort line length of 199km (see Line length section of design summary below). The minimum line length we would expect to achieve is 184km and the maximum is 206km. [Note your values might differ to those below]\n\n\n\n\nshapefile.name &lt;- system.file(\"extdata\", \"StAndrew.shp\", package = \"dssd\")\nregion.sab &lt;- make.region(region.name = \"St Andrews Bay\",\n                      units = \"m\",\n                      shape = shapefile.name)\ncover.sabay &lt;- make.coverage(region.sab, n.grid.points = 5000)\ndesign.spacing5km &lt;- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"systematic\",\n                      spacing = 5000,\n                      design.angle = 90,\n                      edge.protocol = \"minus\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.spacing5km &lt;- run.coverage(design.spacing5km, reps = 250, quiet=TRUE)\nplot(design.spacing5km)\n\n\n\n\nCoverage grid plot for parallel design of St Andrews Bay.\n\n\n\n\n\nprint(design.spacing5km)\n\n\n   Strata St Andrews Bay:\n   _______________________\nDesign:  systematically spaced transects\nSpacing:  5000\nNumber of samplers:  NA\nLine length: NA\nDesign angle:  90\nEdge protocol:  minus\n\nStrata areas:  987500079\nRegion and effort units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        St Andrews Bay Total\nMinimum            7.0   7.0\nMean               7.9   7.9\nMedian             8.0   8.0\nMaximum            8.0   8.0\nsd                 0.3   0.3\n\n    Covered area:\n    \n        St Andrews Bay     Total\nMinimum      725915118 725915118\nMean         762017542 762017542\nMedian       766577949 766577949\nMaximum      778652968 778652968\nsd            15037946  15037946\n\n    % of region covered:\n    \n        St Andrews Bay Total\nMinimum          73.51 73.51\nMean             77.17 77.17\nMedian           77.63 77.63\nMaximum          78.85 78.85\nsd                1.52  1.52\n\n    Line length:\n    \n        St Andrews Bay     Total\nMinimum      184437.76 184437.76\nMean         197642.73 197642.73\nMedian       198918.17 198918.17\nMaximum      205709.28 205709.28\nsd             5491.92   5491.92\n\n    Trackline length:\n    \n        St Andrews Bay     Total\nMinimum      220530.40 220530.40\nMean         242879.10 242879.10\nMedian       246282.46 246282.46\nMaximum      248801.29 248801.29\nsd             7574.99   7574.99\n\n    Cyclic trackline length:\n    \n        St Andrews Bay     Total\nMinimum      251942.93 251942.93\nMean         279724.52 279724.52\nMedian       283685.94 283685.94\nMaximum      285987.68 285987.68\nsd             8855.44   8855.44\n\n    Coverage Score Summary:\n    \n        St Andrews Bay     Total\nMinimum      0.3520000 0.3520000\nMean         0.7716703 0.7716703\nMedian       0.7920000 0.7920000\nMaximum      0.8440000 0.8440000\nsd           0.0850723 0.0850723"
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html#equal-spaced-zigzag-design",
    "href": "06-design/R-prac/Pr6-solution.html#equal-spaced-zigzag-design",
    "title": "Distance sampling survey design solution",
    "section": "Equal Spaced Zigzag Design",
    "text": "Equal Spaced Zigzag Design\n\n\n\n\n\n\nAnswers\n\n\n\nDoes this design meet our survey effort constraint? What is the maximum total trackline length for this design? What line length are we likely to achieve with this design? Is this higher or lower than the systematic parallel design?\n\nYou were asked to then run a coverage simulation and check if the trackline length was within our effort constraints. I found the maximum trackline length to be 242km (see Trackline length summary table in the output below) so within our constraint of 250km. I then got a mean line length of 221km and minimum and maximum line lengths of 212km and 227km, respectively (see Line length summary table in the output below). We can therefore expect to achieve just over 20km more on-effort survey line length with the zigzag design than the systematic parallel line design - 10% gain. [Note your values may differ]\n\n\n\n\ndesign.zz.4500 &lt;- make.design(region = region.sab,\n                      transect.type = \"line\",\n                      design = \"eszigzag\",\n                      spacing = 4500,\n                      design.angle = 0,\n                      edge.protocol = \"minus\",\n                      bounding.shape = \"convex.hull\",\n                      truncation = 2000,\n                      coverage.grid = cover.sabay)\n\n\ndesign.zz.4500 &lt;- run.coverage(design.zz.4500, reps = 250, quiet=TRUE)\n# Plot coverage\nplot(design.zz.4500)\n\n\n\n\nCoverage grid plot for zigzag design of St Andrews Bay.\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nDo you think the coverage scores look uniform across the study region? Where are they higher/lower? Why do you think this is?\n\nYou were finally asked to look at the coverage scores across the survey region to see if this design has even coverage. There are some points with lower coverage around the survey region boundary. This is actually down to the fact we are using a minus sampling strategy. If we plotted coverage scores from a systematic parallel design we would see a similar pattern. Usually edge effects from minus sampling are minor unless we have a very long survey region boundary containing a small study area. If the fact that we are using a zigzag design was causing us issues with coverage we would expect to see higher coverage at the very top or very bottom of the survey region (as our design angle is 0). We do not see this. The survey region boundaries at the top and bottom are both quite wide and perpendicular to the design angle, in this situation zigzag designs perform well with regard to even coverage."
  },
  {
    "objectID": "06-design/R-prac/Pr6-solution.html#coverage",
    "href": "06-design/R-prac/Pr6-solution.html#coverage",
    "title": "Distance sampling survey design solution",
    "section": "Coverage",
    "text": "Coverage\nOrganise the study area shape file.\n\nshapefile.name &lt;- system.file(\"extdata\", \"TentsmuirUnproj.shp\", \n                              package = \"dssd\")\nsf.shape &lt;- read_sf(shapefile.name)\nst_crs(sf.shape)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nproj4string &lt;- \"+proj=aea +lat_1=56 +lat_2=62 +lat_0=50 +lon_0=-3 +x_0=0 \n                +y_0=0 +ellps=intl +units=m\"\nprojected.shape &lt;- st_transform(sf.shape, crs = proj4string)\nregion.tm &lt;- make.region(region.name = \"Tentsmuir\",\n                         strata.name = c(\"Main Area\", \"Morton Lochs\"),\n                         shape = projected.shape)\n\nCreate the coverage grid.\n\ncover.tm &lt;- make.coverage(region.tm, n.grid.points = 5000)\ndesign.tm &lt;- make.design(region = region.tm,\n                         transect.type = \"point\",\n                         design = \"systematic\",\n                         samplers = c(25,15),\n                         design.angle = 0,\n                         edge.protocol = \"minus\",\n                         truncation = 100,\n                         coverage.grid = cover.tm)\nsurvey.tentsmuir &lt;- generate.transects(design.tm)\n\n\nprint(survey.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  751.2295\nNumber of samplers:  26\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  804498.2\nStrata coverage: 5.7%\nStrata area:  14108643\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  218.3674\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\nCovered area:  423713.7\nStrata coverage: 59.24%\nStrata area:  715264.9\n\n   Study Area Totals:\n   _________________\nNumber of samplers:  41\nCovered area:  1228212\nAverage coverage: 8.29%\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nWhat spacing was used in each strata to try and achieve the desired number of samplers? Did your survey achieve exactly the number of samplers you requested? How much does coverage differ between the two strata for this realisation?\n\nA spacing of 751m was used in the main stratum and 218m in the Morton Lochs stratum - these values are calculated based on the stratum areas and should not vary between surveys generated from the same design. You may or may not have achieved the number of transects you requested, this will depend on the random start point calculated for your particular survey. There will also be some variability in coverage, my survey achieved a coverage of 5.7% in the main strata and 64.8% in the Morton Loch strata.\n\n\n\n\ncoverage.tentsmuir &lt;- run.coverage(design.tm, reps=250, quiet=TRUE)\nprint(coverage.tentsmuir)\n\n\n   Strata Main Area:\n   __________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  25\nDesign angle:  0\nEdge protocol:  minus\n\n   Strata Morton Lochs:\n   _____________________\nDesign:  systematically spaced transects\nSpacing:  NA\nNumber of samplers:  15\nDesign angle:  0\nEdge protocol:  minus\n\nStrata areas:  14108643, 715265\nRegion units:  m\nCoverage Simulation repetitions:  250\n\n    Number of samplers:\n    \n        Main Area Morton Lochs Total\nMinimum        22         12.0  36.0\nMean           25         15.1  40.1\nMedian         25         15.0  40.0\nMaximum        27         18.0  44.0\nsd              1          1.1   1.5\n\n    Covered area:\n    \n        Main Area Morton Lochs     Total\nMinimum  674440.9    350770.83 1078239.1\nMean     764377.3    417147.84 1181525.1\nMedian   768948.5    416203.62 1181678.9\nMaximum  818276.8    468878.38 1265309.1\nsd        28590.5     23968.03   35866.9\n\n    % of region covered:\n    \n        Main Area Morton Lochs Total\nMinimum      4.78        49.04  7.27\nMean         5.42        58.32  7.97\nMedian       5.45        58.19  7.97\nMaximum      5.80        65.55  8.54\nsd           0.20         3.35  0.24\n\n    Coverage Score Summary:\n    \n         Main Area Morton Lochs      Total\nMinimum 0.00400000    0.2280000 0.00400000\nMean    0.05420429    0.5826500 0.07957983\nMedian  0.05200000    0.6360000 0.05600000\nMaximum 0.11200000    0.6960000 0.69600000\nsd      0.01571483    0.1086818 0.11648291\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nView the design statistics. What is the minimum number of samplers you will achieve in each strata? Is this sufficient to complete separate analyses in each stratum?\n\nMy design statistics indicated I should achieve between 22 and 27 transects in the main stratum and between 12 and 18 in the Morton Lochs stratum. I might be a bit concerned about the possibility of only achieving 12 transects in the Morton Lochs stratum (remember I cannot just discard a survey due to the number of transects and generate another as it will affect my coverage properties) but whether this is sufficient will depend on a number of things… what are the objectives of the study? how many detections are you likely to get from each transect? etc. Information from a pilot study would be useful to help decide how many transects are required as a minimum.\n\n\n\n\nplot(coverage.tentsmuir, strata=1)\nplot(coverage.tentsmuir, strata=2)\n\n\n\n\n\n\nCoverage scores main stratum Tentsmuir Forest.\n\n\n\n\n\n\n\nCoverage scores Morton Lochs stratum Tentsmuir Forest.\n\n\n\n\n\n\n\n\n\n\n\n\nAnswers\n\n\n\nDoes it appear that you that there is even coverage within strata?\n\nThe main strata looks to have fairly uniform coverage. The values appear to have such small levels of variability that the variability that is seen will be down to stochasticity as it is seen across the entire strata. The Morton Lochs strata we can see has areas of lower coverage around the edge of the study region. This grid is a bit too coarse to allow us to properly judge how much of an issue edge effects will be in this strata. It may be wise to re-run the coverage simulation with a finer coverage grid and more repetitions too. Edge effects could potentially be problematic in such small areas."
  },
  {
    "objectID": "08-covariates/covariateslanding.html",
    "href": "08-covariates/covariateslanding.html",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "08-covariates/covariateslanding.html#analysis-of-point-transect-data",
    "href": "08-covariates/covariateslanding.html#analysis-of-point-transect-data",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "08-covariates/covariateslanding.html#lecture-materials",
    "href": "08-covariates/covariateslanding.html#lecture-materials",
    "title": "Analysis of point transect data",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "08-covariates/covariateslanding.html#exercise-materials",
    "href": "08-covariates/covariateslanding.html#exercise-materials",
    "title": "Analysis of point transect data",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "08-covariates/covariateslanding.html#supplemental-materials",
    "href": "08-covariates/covariateslanding.html#supplemental-materials",
    "title": "Analysis of point transect data",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "06-design/designlanding.html",
    "href": "06-design/designlanding.html",
    "title": "Analysis of data from stratified designs",
    "section": "",
    "text": "Fundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with our software, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add “incentives” to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area."
  },
  {
    "objectID": "06-design/designlanding.html#analysis-of-point-transect-data",
    "href": "06-design/designlanding.html#analysis-of-point-transect-data",
    "title": "Analysis of data from stratified designs",
    "section": "",
    "text": "Fundamental to safely making inference from transects you surveyed to the entire study area is a sound survey design that includes random allocation of survey effort.\nIt is actually difficult for humans to do things that are actually random. Computerised methods make the process reasonably straightforward. Employing a randomised sampling design derived with our software, means there is one less threat to the validity of your survey.\nYou have the opportunity to use shape files from two locations near St Andrews, one marine and one terrestrial, to design surveys. The marine study will use aircraft so considerations such as fuel load add “incentives” to produce the optimal survey design while allowing the aircraft to return safely to land. The terrestrial study deploys point transects into strata of a natural area."
  },
  {
    "objectID": "06-design/designlanding.html#lecture-materials",
    "href": "06-design/designlanding.html#lecture-materials",
    "title": "Analysis of data from stratified designs",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "06-design/designlanding.html#exercise-materials",
    "href": "06-design/designlanding.html#exercise-materials",
    "title": "Analysis of data from stratified designs",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "06-design/designlanding.html#supplemental-materials",
    "href": "06-design/designlanding.html#supplemental-materials",
    "title": "Analysis of data from stratified designs",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "07-stratify/stratifylanding.html",
    "href": "07-stratify/stratifylanding.html",
    "title": "Analysis of stratified surveys",
    "section": "",
    "text": "Carrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function."
  },
  {
    "objectID": "07-stratify/stratifylanding.html#analysis-of-point-transect-data",
    "href": "07-stratify/stratifylanding.html#analysis-of-point-transect-data",
    "title": "Analysis of point transect data",
    "section": "",
    "text": "Having mastered analysis of line transect data, we move on to analysis of point transect data. These data are more difficult to model because the region of the curve where we wish to be most precise is the region where data are most scarce.\nAssessing fit to point transect data also represents a challenge. Consequently, we use plots of the probability density function to gain perspective regarding the influence of regions of the probability density function upon lack of fit.\nTwo data sets are to be analysed: the first is a simulated data set in which both the true shape of the detection function as well as true animal density is known. Fit various detection functions to these data, also examining the effect of truncation (truncation for points is often more severe than for lines) upon estimates. Look for congruence in density estimates–as you did with a similar exercise performed on simulated line transect data in Practical 3.\nFinally, there are two optional data sets of winter wrens collected by Prof. Buckland. Same species and study area, but two different field protocols: one protocol used traditional 5-minute counts while the other protocol employed the “snapshot” method."
  },
  {
    "objectID": "07-stratify/stratifylanding.html#lecture-materials",
    "href": "07-stratify/stratifylanding.html#lecture-materials",
    "title": "Analysis of stratified surveys",
    "section": "Lecture materials",
    "text": "Lecture materials\n\nLecture discussion\n\nSlides\n\n\nVideo"
  },
  {
    "objectID": "07-stratify/stratifylanding.html#exercise-materials",
    "href": "07-stratify/stratifylanding.html#exercise-materials",
    "title": "Analysis of stratified surveys",
    "section": "Exercise materials",
    "text": "Exercise materials\n\n\n\nUsing R package\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary\n\n\n\n\nUsing Distance for Windows\n\nVideo introduction\n\n\nInstructions\n\n\nSolution and discussion\n\n\nVideo summary"
  },
  {
    "objectID": "07-stratify/stratifylanding.html#supplemental-materials",
    "href": "07-stratify/stratifylanding.html#supplemental-materials",
    "title": "Analysis of stratified surveys",
    "section": "Supplemental materials",
    "text": "Supplemental materials\n\nNo supplements\n\nNo supplemental material"
  },
  {
    "objectID": "07-stratify/R-prac/Prac7_solution.html",
    "href": "07-stratify/R-prac/Prac7_solution.html",
    "title": "Analysis of data from stratified surveys solution",
    "section": "",
    "text": "Solution\n\n\n\nAnalysis of data from stratified surveys\n\n\nReading in the data from the stratified survey in the Southern Ocean:\n\nlibrary(Distance)\nlibrary(kableExtra)\n# Load data\ndata(minke)\nhead(minke, n=3)\n\n  Region.Label  Area Sample.Label Effort distance object\n1        South 84734            1  86.75     0.10      1\n2        South 84734            1  86.75     0.22      2\n3        South 84734            1  86.75     0.16      3\n\n# Specify truncation distance\nminke.trunc &lt;- 1.5\n\n\nStrata treated distinctly\nFit detection function and encounter rate separately in each strata.\n\n## Fit to each region separately - full geographical stratification\n# Create data set for South\nminke.S &lt;- minke[minke$Region.Label==\"South\", ]\nminke.df.S.strat &lt;- ds(minke.S, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.S.strat)\n\n\nSummary for distance analysis \nNumber of observations :  39 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  8.617404 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.5102606 0.1921723\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.242147 0.3770239\n\n                      Estimate         SE       CV\nAverage p            0.4956459  0.0662961 0.133757\nN in covered region 78.6852058 13.8143714 0.175565\n\nSummary statistics:\n  Region  Area CoveredArea Effort  n  k         ER      se.ER     cv.ER\n1  South 84734     1453.23 484.41 39 13 0.08051031 0.01809954 0.2248102\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 4587.926 1200.166 0.2615924 2687.497 7832.219 21.14052\n\nDensity:\n  Label   Estimate         se        cv        lcl        ucl       df\n1 Total 0.05414505 0.01416393 0.2615924 0.03171687 0.09243301 21.14052\n\n# Combine selection and detection function fitting for North\nminke.df.N.strat &lt;- ds(minke[minke$Region.Label==\"North\", ],\n                       truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.N.strat)\n\n\nSummary for distance analysis \nNumber of observations :  49 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  37.27825 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.0108104 0.2203526\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 1.022021 0.6907906\n\n                      Estimate         SE        CV\nAverage p            0.7592309 0.09987673 0.1315499\nN in covered region 64.5389972 9.62021387 0.1490605\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n\nAbundance:\n  Label Estimate       se        cv      lcl      ucl       df\n1 Total 9986.683 3878.031 0.3883203 4469.865 22312.49 13.98197\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 Total 0.01583725 0.006149924 0.3883203 0.007088475 0.03538397 13.98197\n\n\n\n\nDetections combined across strata\nNext we fitted a pooled detection function.\n\nminke.df.all &lt;- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\n\nSummary for distance analysis \nNumber of observations :  88 \nDistance range         :  0  -  1.5 \n\nModel       : Hazard-rate key function \nAIC         :  48.63688 \nOptimisation:  mrds (nlminb) \n\nDetection function parameters\nScale coefficient(s):  \n              estimate        se\n(Intercept) -0.2967912 0.1765812\n\nShape coefficient(s):  \n            estimate        se\n(Intercept) 0.964833 0.3605009\n\n                       Estimate         SE        CV\nAverage p             0.6224396  0.0668011 0.1073214\nN in covered region 141.3791725 17.7757788 0.1257312\n\nSummary statistics:\n  Region   Area CoveredArea  Effort  n  k         ER      se.ER     cv.ER\n1  North 630582     4075.14 1358.38 49 12 0.03607238 0.01317937 0.3653591\n2  South  84734     1453.23  484.41 39 13 0.08051031 0.01809954 0.2248102\n3  Total 715316     5528.37 1842.79 88 25 0.04133635 0.01181436 0.2858103\n\nAbundance:\n  Label  Estimate        se        cv      lcl       ucl       df\n1 North 12181.419 4638.6284 0.3807954 5499.950 26979.692 12.96781\n2 South  3653.345  910.0975 0.2491135 2181.595  6117.971 17.96263\n3 Total 15834.764 4834.2848 0.3052957 8388.423 29891.167 15.25496\n\nDensity:\n  Label   Estimate          se        cv         lcl        ucl       df\n1 North 0.01931774 0.007356107 0.3807954 0.008722023 0.04278538 12.96781\n2 South 0.04311546 0.010740641 0.2491135 0.025746391 0.07220207 17.96263\n3 Total 0.02213674 0.006758251 0.3052957 0.011726878 0.04178736 15.25496\n\n\nCompute combined AIC for entire study area.\n\naic.all &lt;- summary(minke.df.all$ddf)$aic\naic.S &lt;- summary(minke.df.S.strat$ddf)$aic\naic.N &lt;- summary(minke.df.N.strat$ddf)$aic\naic.SN &lt;- aic.S + aic.N\n\n\n\nDetermining the correct stratification to use\nThe AIC value for the detection function in the South was 8.617 and the AIC for the North was 37.28. This gives a total AIC of 45.9. The AIC value for the pooled detection function was 48.64. Because 48.64 is greater than 45.9, estimation of separate detection functions in each stratum is preferable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDiffering abundance estimates from stratification decision\nIn the full geographical stratification, both encounter rate and detection function were estimated separately for each region (or strata). This resulted in the following abundances:\n\n\n\nAbundance estimates using full geographical stratification.\n\n\nLabel\nEstimate\n\n\n\n\nNorth\n9987\n\n\nSouth\n4588\n\n\nTotal\n14575\n\n\n\n\n\n\n\nNext, the distances were combined to fit a pooled detection function but encounter rate was obtained for each region. This resulted in the following abundances:\n\n\n\nAbundance estimates calculating encounter rate by strata and a pooled detection function.\n\n\nLabel\nEstimate\n\n\n\n\nNorth\n12181\n\n\nSouth\n3653\n\n\nTotal\n15835\n\n\n\n\n\n\n\n\n\nAnother approach to stratification (advanced)\nAn equivalent result for full geographic stratification could be produced using the dht2 function, which does not require the disaggregation of the data set into two data sets.\n\n# Geographical stratification with stratum-specific detection function \nstrat.specific.detfn &lt;- ds(data=minke, truncation=minke.trunc, key=\"hr\", \n                           adjustment=NULL, formula=~Region.Label)\nabund.by.strata &lt;- dht2(ddf=strat.specific.detfn, flatfile=minke, \n                        strat_formula=~Region.Label, stratification=\"geographical\")\nprint(abund.by.strata, report=\"abundance\")\n\nAbundance estimates from distance sampling\nStratification : geographical \nVariance       : R2, n/L \nMultipliers    : none \nSample fraction : 1 \n\n\nSummary statistics:\n Region.Label   Area CoveredArea  Effort  n  k    ER se.ER cv.ER\n        North 630582     4075.14 1358.38 49 12 0.036 0.013 0.365\n        South  84734     1453.23  484.41 39 13 0.081 0.018 0.225\n        Total 715316     5528.37 1842.79 88 25 0.048 0.011 0.237\n\nAbundance estimates:\n Region.Label Estimate       se    cv  LCI   UCI     df\n        North     9865 3761.296 0.381 4451 21860 13.035\n        South     4651 1224.818 0.263 2719  7955 22.163\n        Total    14515 3970.578 0.274 8215 25648 16.065\n\nComponent percentages of variance:\n Region.Label Detection    ER\n        North      8.18 91.82\n        South     27.13 72.87\n        Total     12.49 87.51\n\n\nI won’t say anything just now about the wrinkle I introduced with the formula argument in the call to ds(). Recognise there is an alternative (easier) way to perform the full geographic stratification analysis without tearing apart the data. The abundance estimates presented in the last output do not identically match the estimates shown earlier for full geographic stratification, but they are close. The added benefit of this latter analysis is that the uncertainty in the total population size is computed within dht2 rather than needing to be calculated manually using the delta method.\n\n\n\n\n\n\nAn aside\n\n\n\nIf geographic stratification were ignored, the abundance estimate of would be 18,293 minkes. This estimate is substantially larger than the estimates above. The reason is that the survey design was geographically stratified with a smaller proportion of the north stratum receiving sampling effort and a greater proportion of the southern stratum receiving survey effort. Ignoring this inequity in the unstratified analysis would lead us to believe that the more heavily sampled southern stratum is indicative of whale density throughout the study area."
  },
  {
    "objectID": "07-stratify/stratifylanding.html#analysis-of-stratified-surveys",
    "href": "07-stratify/stratifylanding.html#analysis-of-stratified-surveys",
    "title": "Analysis of stratified surveys",
    "section": "",
    "text": "Carrying on from the discussion of survey design where stratification was introduced, this exercise describes multiple analyses of a single data set.\nMultiple analyses arise because it is an open question whether a single detection function is appropriate for different strata that might sharply contrast in their vegetation that might influence detectability.\nEmploying an inappropriate detection function will invalidate population inferences; consequently it is important to have a decision framework that can be employed to determine the role stratification should play in the detection function."
  },
  {
    "objectID": "07-stratify/R-prac/Pr7-instructions.html",
    "href": "07-stratify/R-prac/Pr7-instructions.html",
    "title": "Analysis of stratified survey data",
    "section": "",
    "text": "For this exercise, we use data from a survey of Antarctic minke whales. The study region was divided into two strata (North and South) the two strata were surveyed by different vessels at the same time. The minkes tend to be found in high densities against the ice edge, where they feed, and so the density in the southern stratum is typically higher than in the northern stratum (Figure 1). This is the primary reason for using a stratified survey design. It is also the reason for covering the southern stratum more intensely: in the southern stratum the transect length per unit area is more than 2.5 times that of the northern stratum.\n\nFigure 1. An example of the sort of survey design used and a typical minke density gradient. The irregular bottom border is the ice-edge. The ‘stepped’ black line defines the boundary between the strata; dotted lines are transects and dots are detections.\n\nObjectives\nThe objectives of this exercise are to:\n\nCreate subsets of the data\nDecide whether to fit separate detection functions or a pooled detection function\nSpecify different stratification options using the dht2 function.\n\n\n\nGetting started\nBegin by reading in the data. Distances are in kilometers and a truncation distance of 1.5km is specified and used in the following detection function fitting. Perpendicular distances, transect lengths and study area size are all measured in kilometers; hence convert_units argument to ds is 1 and has been omitted. To keep things simple, a hazard rate detection function with no adjustments is used for all detection functions.\n\nlibrary(Distance)\ndata(minke)\nhead(minke)\n# Specify truncation distance\nminke.trunc &lt;- 1.5\n\nYou will see that these data contain a column called ‘Region.Label’: this contains values ‘North’ or ‘South’.\n\n\nFull geographical stratification\nFirst, we want to fit encounter rate and detection function separately in each strata. This is easily performed by splitting the data by region and using ds on each subset. The commands below do this for the southern region (note, there are alternative ways to select a subset of data).\n\n# Create dataset for South \nminke.S &lt;- minke[minke$Region.Label==\"South\", ]\n# Fit df to south\nminke.df.S.strat &lt;- ds(minke.S, key=\"hr\", adjustment=NULL, truncation=minke.trunc)\nsummary(minke.df.S.strat)\n\nMake a note of the AIC. Perform a similar commands to obtain estimates for the northern region. What is the total AIC?\nAlso make a note of the abundance in each region. What is the total abundance in the study region?\n\n\nFitting a pooled detection function\nWe want to compare the total AIC found previously with the AIC from fitting a detection function to all data combined. This is easy to obtain:\n\nminke.df.all &lt;- ds(minke, truncation=minke.trunc, key=\"hr\", adjustment=NULL)\nsummary(minke.df.all)\n\nGiven the AIC value for the detection function from the pooled data, would you fit a separate detection function in each strata or not?\n\n\nStratification options using dht2\nThe command summary(minke.df.all) will provide the abundance estimates for each region and the total and for this simple example, this is sufficient. However, if we want to consider different stratification options, then the dht2 function is useful.\nAfter fitting a detection function, the dht2 function, allows abundance estimates to be computed over some specified regions. In the command below, the pooled detection function is used to obtain estimates in each strata and over all (like the summary function previously used).\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~Region.Label, stratification=\"geographical\")\n\nThe arguments are:\n\nddf\n\nthe detection function (fitted by ds)\n\nflatfile\n\nthe data object containing all the necessary information\n\nData is referred to as being in a flatfile format if it contains information on region, transects and observations. An alternative is to use a hierarchical structure and have region, transect and observation information in separate data files with links between them to ensure that transects are mapped to the relevant region and observations to the relevant transect. We’ve not used the hierarchical structure during this workshop.\n\n\nstrat_formula=~Region.Label\n\nformula (hence the ~) giving the stratification structure\n\nstratification=\"geographical\"\n\nin this example, we specify that each strata (specified in strat_formula) represents a geographical region.\n\nconvert_units\n\ngetting units conversion correct, same purpose as the convert_units argument in ds. For the minke data all measurements are in the same units, so the argument is not needed in this case.\n\n\nMake a note the total abundance in the study region.\n\n\n\n\n\n\nFailure to respect design during analysis\n\n\n\nWhat happens if we were to ignore the regions and treat the data as though it came from one large study region? This can (dangerously) be done by changing the stratification formula, as shown below.\n\ndht2(ddf=minke.df.all, flatfile=minke, strat_formula=~1, stratification=\"geographical\")\n\nHas this changed the abundance estimate? Of course it has; the question is why has this changed the abundance estimate; which estimate is proper?"
  }
]